{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Math for ML",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitmul/medical-ai-course-materials/blob/master/notebooks/Basic_Math_for_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4_DY-zYo0KPz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 機械学習に必要な数学の基礎\n",
        "\n",
        "この章では、ディープラーニングを含めた機械学習に必要な微分、線形代数、統計の3つについて、よく使う部分を完結に紹介します。\n",
        "\n",
        "\n",
        "\n",
        "## 微分\n",
        "\n",
        "微分は高校で習いますが、社会で使うことはほとんどなく、微分は何に使うのかと不明確なまま終わってしまっている人も少なくないだろう。機械学習においては**パラメータ更新**の際に大きなカギとなる考え方であり、その理由について最初に知ってから、数式を理解していこう。\n",
        "\n",
        "### 微分はどこで使われているか\n",
        "\n",
        "「微分は何に使えるか」をここでは考えることにしよう。微分が何に使えるのかの質問の前に、**微分は何が求まるのか**をまず考えよう。高校の時には微分では何が求まると習ったかというと、その答えは**接線の傾き**である。数値も交えて具体的に見ていこう。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/01.png\" height=\"200px\">\n",
        "\n",
        "この図の関数において、$a$ の点における接線の傾きというのは赤い直線の傾きを指し、例えば、傾きが+3のようになっている。+3という値は例ですが、右肩上がりな直線の傾きは正の値になります。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/02.png\" height=\"200px\">\n",
        "\n",
        "そして、この $b$ の点においては、接線の傾きは右肩下がりであるため、接戦の傾きは負の値であり、傾き-1のようになっている。微分では、この各点における「+3」や「-1」といった傾きを求めることができます。\n",
        "\n",
        "それでは、本題に戻りましょう。この各点の接戦の傾きが求まると何に使えるのでしょうか。結局、傾きが求まると聞いたところで、使い道が浮かばないのではないでしょうか。\n",
        "\n",
        "機械学習を使う場合、できるだけ真の値に近い良い予測値を得たいと思っています。学習の際には教師データ $t$ に対して、その予測値 $y$ がなるべく近い値であることが望まれます。そのために、その差分である $t-y$ が小さくなることが望ましいといえます。厳密には、$t-y$ を小さくしたいと考えれば、$y\\rightarrow\\infty$ とすれば $t-y$ は小さくなりますがそういう話ではなく、$t$ と $y$ の差を小さくしたいため、$|t-y|$ や $(t-y)^{2}$ が小さくなることが望ましいと考えられます。設定としては、$|t-y|$ でも良いのですが、$(t-y)^{2}$ の方が、数学的に取り扱いがしやすく、一般的にこちらが採用されます。この $(t-y)^2$ のことを**二乗誤差**と呼び、機械学習では、この二乗誤差を最も小さくできるように調整したいということがゴールとなります。\n",
        "\n",
        "そして、二乗誤差が最小となる点を求めたいと考え、その点での接線の傾きは正の値になるでしょうか。それとも、負の値になるでしょうか。答えはどちらでもなく、図の通り、誤差が最小となる点では、**接線の傾きが0**となります。つまり、接線の傾きを用いることで、**ある関数における最小（もしくは最大）となる点を求めることができる**と考えられるわけです。これが微分を学ぶ大きなモチベーションとなっています。コストを最小化したいや売り上げを最大化したいという要望は、ビジネスを含めたありとあらゆる現場で求められる課題設定であり、これらの最適な点を求めるために活躍するツールが微分である。そう考えると、微分を学ぶモチベーションが大きく湧いたのではないでしょうか。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/03.png\" height=\"200px\">\n",
        "\n",
        "### 2点間を通る直線の傾き\n",
        "\n",
        "それでは、微分の原理を理解していくために、まずは中学校で習った下図に示す2点間を通る直線の傾き $a$ を求めてみましょう。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/04.png\" height=\"200px\">\n",
        "$$\n",
        "傾きa = \\dfrac{f(x)の変化量}{xの変化量}\n",
        "$$\n",
        "中学校の時にならったこの考え方おり\n",
        "$$\n",
        "a = \\dfrac{f(x_{2}) - f(x_{1})}{x_{2}-x_{1}}\n",
        "$$\n",
        "と求まります。懐かしいなと思い出してもらえる程度で大丈夫です。これだけの話ですが、これが意外と微分の考え方の大半を占めています。\n",
        "\n",
        "### 1点での接線の傾き\n",
        "\n",
        "それでは、中学校で習った内容からさらに接線の傾きまで求められるように考えていきましょう。そのためには、**極限**を知っておく必要があるため紹介します。極限とは、$\\lim$ の下に書いた条件に値を近づけていく考え方です。例えば、\n",
        "$$\n",
        "\\displaystyle \\lim _{x\\rightarrow 0}3x=3\\times 0=0\n",
        "$$\n",
        "のような計算です。一見、$x=0$を代入しただけにしか見えないかも知れませんが、点の**動き**を表現したいときに有効な手段であり、こちらが微分でも登場します。\n",
        "\n",
        "それでは、次の問題として、下図のある点 $x$ における接線の傾き$a$を求めていきましょう。今回は2点ではない、ある1点での接線の傾きです。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/05.png\" height=\"200px\">\n",
        "\n",
        "さて、ここで問題が生じないでしょうか。接線の傾きを求めようにも先ほどまでの知識では1点だけでは傾きを求めることができません。そこで、さきほど考えた2点を通る直線と極限を組み合わせて、突破できないか考えてみましょう。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/06.png\" height=\"200px\">\n",
        "\n",
        "$x$ から $h$ だけ離れた点 $x+h$ を考え、この2点を通る直線の傾きを求めることとします。これでは先ほどまでの2点間を通る直線の傾きと変わらないのですが、$h \\rightarrow 0$ のようにすれば、理論上2点が1点となり、1点での接線として考えることができ、直線の傾きは\n",
        "$$\n",
        "\\begin{aligned}\n",
        "a &=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{\\left( x+h\\right) -x}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{h}\\\\\n",
        " &=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{h}\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "となります。この式が一般的に微分という言葉で認識されていることが多いですが、厳密には**導関数**と呼び、$f'(x)$ で表されます。導関数を求めることを**微分する**といいます。また、記号の使い方として、\n",
        "$$\n",
        "(\\cdot)' = \\dfrac{d}{dx}(\\cdot)\n",
        "$$\n",
        "のように、どちらも同じです。今後は主に右辺の書き方が多くなってきて複雑に見えるかもしれませんが、本日的には難しいものではありません。このように、中学校までに習った数学と極限の考え方を組み合わせるだけで微分の基本を理解することができました。\n",
        "\n",
        "### 微分の公式\n",
        "\n",
        "微分にはいくつか覚えておくと便利な公式があります。特に、機械学習を勉強し始める最初の段階では、まず以下の3つの公式だけである程度計算をスムーズに進めることができます。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left( 1\\right) ^{'}&=0\\\\\n",
        "\\left( x\\right) ^{'}&=1\\\\\n",
        "\\left( x^{2}\\right) ^{'}&=2x\n",
        "\\end{aligned}\n",
        "$$\n",
        "これらの微分の公式をこれからよく使うため、暗記しておきましょう。これらは公式として覚えてしまえばよいのですが、ここでは勉強もかねてこの公式を導いてみましょう。数式に対して、あまり余裕がない段階であれば公式だけ覚えて読み飛ばしていただいても構いません。\n",
        "\n",
        "まずは、$f(x)=1$のときを考えましょう。$f(x)=1$より、$f(x+h)=1$となります。この考え方に多少癖がありますが、考え方のヒントとしては $f(x)$のときに $x$ となっている箇所が $f(x+h)$ のときに $x+h$ へと変わります。今回は $f(x)=1$ より、$x$ が出てこなかったため、$f(x+h)$ も何も変わらず 1 と考えられるわけです。さて、導関数の計算を行うと、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f'(x)&=\\left( 1\\right)'\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {1-1}{h} \\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {0}{h}\\\\\n",
        "&=\\lim _{x\\rightarrow 0}0\\\\\n",
        "&=0\n",
        "\\end{aligned}\n",
        "$$\n",
        "が得られました。\n",
        "\n",
        "つぎに、$f(x)=x$ のときを考えてみましょう。$f(x+h) = x+h$ となり、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f'\\left( x\\right) &=\\left( x\\right)'\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {\\left( x+h\\right) -x}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {h}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}1\\\\\n",
        "&=1\\end\n",
        "{aligned}\n",
        "$$\n",
        "が得られました。\n",
        "\n",
        "最後に、$f(x) = x^{2}$ の場合、$f(x+h) = (x+h)^{2}$ となります。よくある間違いとして、$f(x+h) = x^{2} + h$ や $f(x+h) = x^{2} + h^{2}$ のように考えてしまう人を見かけますが、$f(x) = (x)^{2}$ と考えると、$x$ の部分が $x+h$ に代わるため、$f(x+h) = (x+h)^{2}$ です。このとき、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f\\left( x\\right) &=\\left( x^{2}\\right)'\\\\\n",
        "&=\\lim_{h\\rightarrow 0}\\dfrac {f{\\left( x+h\\right) }-f\\left( x\\right) }{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {\\left( x+h\\right) ^{2}-x^{2}}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {\\left( x^{2}+2xh+h^{2}\\right) -x^{2}}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {2xh+h^{2}}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {\\left( 2x+h\\right) h}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\ (2x + h)\\\\\n",
        "&=2x\n",
        "\\end{aligned}\n",
        "$$\n",
        "が得られ、3つの公式を導くことができました。\n",
        "\n",
        "それでは、次の２つの例題を考えながら、具体的な微分の計算に慣れていきましょう。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "( 3x^{2})'&=3\\times (x^{2})'\\\\\n",
        "&=3\\times 2x\\\\\n",
        "&=6x\n",
        "\\end{aligned}\n",
        "$$\n",
        "この $(3x^{2})' = 3 \\times (x^{2})'$ の部分に注目してください。微分では、定数項は微分の演算の外側に出すことができます。また、次の例題でも新しい特性があります。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left( 3x^{2}+4\\right)'&=\\left( 3x^{2}\\right)'+\\left( 4\\right)'\\\\\n",
        "&=3\\times \\left( x^{2}\\right)'+4\\times \\left( 1\\right)'\\\\\n",
        "&=3\\times 2x+4\\times 0\\\\\n",
        "&=6x\n",
        "\\end{aligned}\n",
        "$$\n",
        "この例題では、$\\left( 3x^{2}+4\\right)'=\\left( 3x^{2}\\right)'+\\left( 4\\right)'$ のように、全体の和をとった後に微分の演算を行っても、それぞれで微分の演算を行った後に和の計算をしても同じとなっています。これは微分の**線形性**と呼ばれる性質であり、気になった方は調べてみてください。ポイントとしては、定数項は演算の外に出すことができて、それぞれの項を微分した後に和をとっても良いということです。この性質は計算を楽にできるところに大きく貢献してくれます。\n",
        "\n",
        "### 合成関数の微分\n",
        "\n",
        "機械学習のアルゴリズムで登場する関数は複雑なものも多く、その結果、微分の計算も複雑になりがちです。具体的には、\n",
        "$$\n",
        "\\left\\{ (3x + 4)^{2} \\right\\}'\n",
        "$$\n",
        "のように、$3x+4$ の内側の部分と $(\\cdot)^{2}$ の外側の部分で構成されているような場合もあり、展開してからそれぞれ微分を計算してもよいのですが、これが3乗や4乗となってくると現実的ではなくなってこないでしょうか。この時に役に立つ考え方が**合成関数の微分**です。\n",
        "\n",
        "まず内側の関数を $u = (3x+4)$ とおくと、\n",
        "$$\n",
        "\\left\\{ (3x + 4)^{2} \\right\\}' = (u^{2})'\n",
        "$$\n",
        "とします。ここで、$(\\cdot)'$ をもう少し厳密に考える必要が出てきます。いまは、$x$ と $u$ の2つの変数が登場しており、$(\\cdot)'$ では、$x$ で微分しているのか $u$ で微分しているのかの区別がつきません。そこで、多少複雑に見えますが、微分する変数を厳密に記述すると、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left\\{ (3x + 4)^{2} \\right\\}' &= \\dfrac{d}{dx} \\left\\{ (3x + 4)^{2} \\right\\} \\\\\n",
        "&= \\dfrac{d}{dx} (u^2) \\\\\n",
        "&=  \\dfrac{d}{dx} f(u) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "となり、$u$ の関数 $f(u) = u^{2}$ に対して、$x$ で微分してしまっていることがわかります。そこで、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac{df(u)}{dx} = \\dfrac{du}{dx} \\dfrac{df(u)}{du}\n",
        "\\end{aligned}\n",
        "$$\n",
        "のように右辺と左辺は分数として約分してしまえば同じであるといった考え方を適用することができます。つまり、合成関数の計算は内側の微分と外側の微分をそれぞれ行い、その結果を掛け合わせれ良いわけです。それぞれの微分の計算は\n",
        "$$\n",
        "\\begin{aligned}\n",
        " \\dfrac{du}{dx} &= \\dfrac{d}{dx} (3x+4) = 3 \\\\\n",
        " \\dfrac{df(u)}{du} &= \\dfrac{d}{du} u^{2} = 2u \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "となります。これより、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac{df(u)}{dx} &= \\dfrac{du}{dx} \\dfrac{df(u)}{du} \\\\\n",
        " &= 3 \\times 2u \\\\\n",
        " &= 3 \\times 2(3x+4) \\\\\n",
        " &= 6(3x+4)\n",
        "\\end{aligned}\n",
        "$$\n",
        "が得られます。数式こそ多少複雑に見えますが、内側と外側をそれぞれ微分して掛け合わせるだけであるため、実際の計算は慣れると簡単に行えます。この合成関数の微分を使用する場面が何度も登場するため、この計算方法をしっかりと覚えておきましょう。\n",
        "\n",
        "### 偏微分\n",
        "\n",
        "微分最後のトピックとして、**偏微分**を紹介します。偏微分は多変数関数の微分です。機械学習では、1つの入力変数 $x$ から出力変数 $y$ を予測するケースは稀であり、基本的には、複数の入力変数 $x_{1}$, $x_{2}$, $\\ldots$, $x_{M}$ を用いて出力変数 $y$ を予測します。例えば、家賃を予測する場合、部屋の広さだけで予測するよりも、駅からの距離や犯罪発生率などを考慮した方が予測の性能は高まりそうだと考えられます。多変数 $x_{1}$, $x_{2}$, $\\ldots$, $x_{M}$ を考慮した多変数関数 $f(x_{1}, x_{2}, \\ldots, x_{M})$ では、各変数で微分することを偏微分と呼び、\n",
        "$$\n",
        "\\dfrac{\\partial}{\\partial x_{m}} f(x_{1}, x_{2}, \\ldots, x_{M})\n",
        "$$\n",
        "のように表します。大雑把には、$d$ が $\\partial$ に変わっただけです。しかも、計算方法は至って単純であり、$\\dfrac{\\partial}{\\partial x_{m}} $の場合は $x_{m}$ 以外は定数と考えて、$x_{m}$ のみ着目して微分を行います。\n",
        "\n",
        "難しい理論はさておき、例題で具体的な計算の流れを確認していきましょう。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}+4x_{2}\\right) &=\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}\\right) +\\dfrac {\\partial }{\\partial x_{1}}\\left( 4x_{2}\\right) \\\\\n",
        "&=3\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( x_{1}\\right) +4x_{2}\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( 1\\right) \\\\\n",
        "&=3\\times 1+4x_{2}\\times 0\\\\\n",
        "&= 3\n",
        "\\end{aligned}\n",
        "$$\n",
        "基本的には微分と同じ線形性の性質が適用できます。あとは、今回のケースでは、$x_{1}$ にだけ着目するため、$x_{2}$ ですら定数として扱うことを把握しておけば上記の計算の流れが理解できるはずです。これが偏微分であり、参考書にはここからさらに全微分の話に入っていくことが多いですが、ひとまずここまでの計算の方法を理解しておけば、この後の計算は理解することができるはずです。\n",
        "\n",
        "## 線形代数\n",
        "\n",
        "### 線形代数は何に役立つのか\n",
        "\n",
        "理系の大学に進んだ方にとっては最初の一般教養として受講する**線形代数**です。ベクトル、行列、ランク、逆行列などなど、授業で習いはするものの、どのような場面で役に立つのか見えないまま試験対策だけして過ぎ去った記憶があるのではないでしょうか。もちろん筆者もそのうちの一人です。\n",
        "\n",
        "さて、微分と同様に理論を学ぶ前に、これから学ぶ線形代数という学問がみなさんにとってどのようなメリットをもたらしてくれるかを考えましょう。$x_{1}$ や $x_{2}$ のように機械学習の中では、似たような複数の変数が登場してきます。また、これらすべてを細々と書いていくのは面倒ですし、間違えやすくもなります。そこで、同じような演算が適用されるものに関しては、まとめて扱いましょうという提案があることが至極当然だと思います。それが線形代数です。線形代数の演算を身に着けることによって、ひとつひとつ書かないといけなかった式を非常にシンプルに取り扱うことができます。人間が楽をするための学問であるため、ぜひ身に着けていきましょう。\n",
        "\n",
        "### スカラー、ベクトル、行列、テンソル\n",
        "\n",
        "線形代数を学ぶ上で一番最初に絶対に抑えておくべきことが**スカラー**、**ベクトル**、**行列**、**テンソル**の4つです。\n",
        "\n",
        "スカラーは、1つの値もしくは変数のことである。例えば、\n",
        "$$\n",
        "x, \\ y,\\  M,\\  N\n",
        "$$\n",
        "のようなものです。スカラーを単体で説明することは難しいため、次のベクトルと比較するとわかりやすいです。\n",
        "\n",
        "ベクトルは、複数のスカラーを縦方向（もしくは横方向）に集めたものであり、\n",
        "$$\n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2} \\\\\n",
        "x_{3}\n",
        "\\end{bmatrix}, \\\n",
        "\\boldsymbol{y}=\\begin{bmatrix}\n",
        "y_{1} \\\\\n",
        "y_{2} \\\\\n",
        "\\vdots \\\\\n",
        "y_{N}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "ように表します。ベクトルでは太文字としており、スカラーかベクトルかを一目で区別できるようにしています。ベクトルを縦方向に定義するか横方向に定義するかは業界によって異なっていますが、経験上、機械学習では縦方向で定義している論文や参考書が多いと感じるため、今回は**ベクトルは縦方向**で統一します。\n",
        "\n",
        "行列は複数のベクトルをまとめたものであり、\n",
        "$$\n",
        "\\boldsymbol{X}=\\begin{bmatrix}\n",
        "x_{11} & x_{12} \\\\\n",
        "x_{21} & x_{22} \\\\\n",
        "x_{31} & x_{32}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "のように表します。また、行列ではサイズを確認することが多く、この $\\boldsymbol{X}$ は3行2列であり、サイズが(3, 2)の行列と言います。また、行列の値が実数の場合がほとんどであり、よく $\\boldsymbol{X} \\in \\mathcal{R}^{3 \\times 2}$のようにサイズを簡潔に表現することがあるため、こちらも覚えておきましょう。\n",
        "\n",
        "最後に、テンソルは行列をさらにまとめたものであり、図のように行列を奥行き方向にさらに展開したものだと考えておきましょう。例えば、RGB (Red Green Blue) などの色空間で表現するカラー画像などがこのテンソルに対応します。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/07.png\" height=\"150px\">\n",
        "\n",
        "\n",
        "\n",
        "スカラー $\\subset$ ベクトル $\\subset$ 行列 $\\subset$ テンソルのような関係でありました。線形代数では $\\boldsymbol{y}$ や $\\boldsymbol{X}$ といった文字だけで式変形をしていくため、どのような形の数値が取り扱われているかわかりくいのですが、これはベクトルなどと常に意識しておくことでその形を見失わないように注意しましょう。\n",
        "\n",
        "|        | 小文字         | 大文字         |\n",
        "| ------ | -------------- | -------------- |\n",
        "| 細文字 | スカラーの変数 | スカラーの定数 |\n",
        "| 太文字 | ベクトル       | 行列、テンソル |\n",
        "\n",
        "### 足し算・引き算\n",
        "\n",
        "行列やベクトルの演算について覚えていきましょう。足し算や引き算は、\n",
        "$$\n",
        "\\begin{aligned}&\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "3\n",
        "\\end{bmatrix}+\\begin{bmatrix}\n",
        "4 \\\\\n",
        "5 \\\\\n",
        "6\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "1+4 \\\\\n",
        "2+5 \\\\\n",
        "3+6\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "7 \\\\\n",
        "8 \\\\\n",
        "9\n",
        "\\end{bmatrix}\\\\\n",
        "&\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6\n",
        "\\end{bmatrix}+\\begin{bmatrix}\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "8 & 10 & 12 \\\\\n",
        "14 & 16 & 18\n",
        "\\end{bmatrix}\\end{aligned}\n",
        "$$\n",
        "ように行列やベクトルの中の**要素**で対応する場所を足し合わせます。引き算も同様です。計算としては単純なものであり、特別なことはありません。ここでポイントとして、**同じサイズでないと計算が成立しない**ということをおぼえておきましょう。\n",
        "\n",
        "### かけ算（行列積）\n",
        "\n",
        "行列の掛け算は複数パターンあり、一般的に掛け算として認識されているものは**行列積**と呼ばれます。それ以外には外積や要素積（アダマール積）などがあります。行列積は計算の方法が少し変わっており、以下のように、「行」「列」と線を引きながら計算していきましょう。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/08.png\" height=\"100px\">\n",
        "\n",
        "このように、単純に足し算や引き算のように要素ごとの積を扱うわけではないため、最初は計算に慣れが必要です。そして、行列積では計算が成り立つためには、\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/09.png\" height=\"150px\">\n",
        "\n",
        "が条件となります。このように行列積は特殊な計算方法になっているが、この変わった計算方法が機械学習にとってはうまく作用します。また、行列ではかけ算はありますが割り算に相当する演算はありません。理由としては、$4 / 2 = 4 \\times \\dfrac{1}{2}$ のように割り算を掛け算で記述することができるためです。\n",
        "\n",
        "それでは、この計算条件の確認も踏まえて、下記の３つを練習問題として解いてください。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\left( 1\\right) \\begin{bmatrix} 1 & 2 \\end{bmatrix}\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\\\\ \n",
        "&\\left( 2\\right) \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix}\\\\ \n",
        "&\\left( 3\\right) \\begin{bmatrix} 1 & 2 \\end{bmatrix}\\begin{bmatrix} 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\\end{aligned}\n",
        "$$\n",
        "それでは、こちらが解答です。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\left( 1\\right) \\begin{bmatrix} 1 & 2 \\end{bmatrix}\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = 1\\times 3 + 2 \\times 4 = 11\\\\ \n",
        "&\\left( 2\\right) \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix}1\\times 5 + 2\\times 6 \\\\3 \\times 5 + 4 \\times 6\\end{bmatrix} = \\begin{bmatrix}17 \\\\ 39\\end{bmatrix}\\\\ \n",
        "&\\left( 3\\right) \n",
        "\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "3 & 4 \\\\\n",
        "5 & 6\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "3 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "3\\times 3+4\\times 1 \\\\\n",
        "5\\times 3 +6\\times 1\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "13 \\\\\n",
        "21\n",
        "\\end{bmatrix}\n",
        "=1 \\times 13+2\\times 21\n",
        "=55\n",
        "\\end{aligned}\n",
        "$$\n",
        "計算のイメージは沸いたでしょうか。実は、この３つの計算は機械学習においてよく出てくる形の計算です。押さえておくべきポイントとして、演算後に形が変わることを覚えておきましょう。\n",
        "\n",
        "### サイズ感\n",
        "\n",
        "この言葉は筆者が定義した言葉ですが、計算を行う上でこの**サイズ感**を覚えておくことが機械学習を円滑に理解するためのひとつのポイントではないかと思います。先ほどの３つの練習問題のサイズがどのように変化したかをまとめると、\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/10.png\" height=\"250px\">\n",
        "\n",
        "のようになります。もともとがベクトルや行列であっても、演算後にスカラーになるケースもあることがわかります。先ほど計算したこともあり、こちらは納得していただけると思います。さて、なぜこのサイズが変化する感覚をつかんでおくことが大事になのでしょうか。\n",
        "\n",
        "今回は数値で計算結果を追っていましたが、これからの計算はすべて数値ではなく文字で表現して扱います。線形代数を勉強し始めの人が $\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{y}$ を見て、スカラーであると瞬時に判断できるかというと、印象的にはより複雑な何かになるという印象があると思います。ここで、ベクトルは縦向きで定義するため、横向きのベクトルは**転置**（記号は上付きの $T$）を使うことによって表現しています。\n",
        "\n",
        "### 転置\n",
        "\n",
        "ベクトルは縦向きが基本としていましたが、先程の計算問題などでは、しばしば横向きのベクトルが出てきました。そこで登場するベクトルの縦と横を入れ替える演算のことを**転置**（Transpose）と呼び、\n",
        "$$\n",
        "\\begin{aligned}\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "3\n",
        "\\end{bmatrix}, \\ \n",
        "\\boldsymbol{x}^{T}=\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\\\\n",
        "\\boldsymbol{X}&=\\begin{bmatrix}\n",
        "1 & 4 \\\\\n",
        "2 & 5 \\\\\n",
        "3 & 6\n",
        "\\end{bmatrix}, \\\n",
        "\\boldsymbol{X}^{T}=\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6\n",
        "\\end{bmatrix}\\end{aligned}\n",
        "$$\n",
        "のようになります。このように転置自体の演算は簡単です。転置では下記３つの公式を覚えておくと、これからの計算が楽になります。\n",
        "$$\n",
        "\\begin{aligned}&\\left( 1\\right) \\ \\left( \\boldsymbol{A}^{T}\\right)^{T}=\\boldsymbol{A}\\\\\n",
        "&\\left( 2\\right) \\ \\left( \\boldsymbol{A}\\boldsymbol{B}\\right) ^{T}=\\boldsymbol{B}^{T}\\boldsymbol{A}^{T}\\\\\n",
        "&\\left( 3\\right) \\ \\left( \\boldsymbol{A}\\boldsymbol{B}\\boldsymbol{C}\\right) ^{T}=\\boldsymbol{C}^{T}\\boldsymbol{B}^{T}\\boldsymbol{A}^{T}\\end{aligned}\n",
        "$$\n",
        "\n",
        "### 単位行列\n",
        "\n",
        "単位行列とは、スカラーの１に対応した性質をもつ行列です。どのような性質かというと、10$\\times$1のように、乗じても変わらないという性質です。行列の演算において、これと同様の働きをする行列が**単位行列**になり、\n",
        "$$\n",
        "\\boldsymbol{I}=\\begin{bmatrix}\n",
        "1 & 0 & \\ldots  & 0 \\\\\n",
        "0 & 1 & \\ldots  & 0 \\\\\n",
        "\\vdots & \\vdots  & \\ddots  & \\vdots  \\\\\n",
        "0 & 0 & \\ldots  & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "のような形をしています。斜めの要素を**対角要素**と呼ぶのですが、対角要素が1で、それ以外の要素が0です。もう少し具体的に $\\boldsymbol{I} \\in \\mathcal{R}^{2\\times 2}$ の場合、\n",
        "$$\n",
        "\\boldsymbol{I} =\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "であり、$\\boldsymbol{I} \\in \\mathcal{R}^{3\\times 3}$ の場合、\n",
        "$$\n",
        "\\boldsymbol{I}=\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "となります。\n",
        "\n",
        "実際に計算して、元の行列と値が変わらないかを確認してみると、\n",
        "$$\n",
        "\\begin{aligned}\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "&=\\begin{bmatrix}\n",
        "1\\times 1+2\\times 6 & 1\\times 0+2\\times 1 \\\\\n",
        "3\\times 1+4\\times 0 & 3\\times 0+4\\times 1\n",
        "\\end{bmatrix}\\\\\n",
        "&=\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "のように、確かに元の値と同じであることがわかりました。単位行列 $\\boldsymbol{I}$ の性質は\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{A}\\boldsymbol{I}&=\\boldsymbol{A}\\\\\n",
        "\\boldsymbol{I}\\boldsymbol{A}&=\\boldsymbol{A}\n",
        "\\end{aligned}\n",
        "$$\n",
        "となります。\n",
        "\n",
        "### 逆行列\n",
        "\n",
        "**逆行列**とは\n",
        "$$\n",
        "2 \\times 2^{-1} = 1\n",
        "$$\n",
        "のようなスカラーの逆数に対応するような行列です。英語では **Inverse Matrix **です。\n",
        "\n",
        "逆行列の定義は\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{A}\\boldsymbol{A}^{-1}=\\boldsymbol{I}\\\\\n",
        "\\boldsymbol{A}^{-1}\\boldsymbol{A}=\\boldsymbol{I}\n",
        "\\end{aligned}\n",
        "$$\n",
        "であり、ここで、$\\boldsymbol{I}$ は単位行列です。この逆行列も機械学習の計算過程に頻出するため覚えておきましょう。大学で習う線形代数の授業では、$2 \\times 2$ や $3 \\times 3$ の逆行列の計算方法について習いますが、実際に扱うものは $10 \\times 10$ などさらに大きく、当然コンピュータで計算するものであるため、その計算方法については余裕が出てから知る程度でも良いと思っています。ただし、どのような行列においても逆行列を計算できるわけではないため、計算できるための条件は知っておきましょう。逆行列は**正方行列**と呼ばれる行と列のサイズが同じでないと計算することができず、これが最低限の条件となる。さらに詳細にはフルランクであることがあるが、こちらも計算に慣れて余裕が出てきてからさらに深めるので大丈夫です。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/11.png\" height=\"150px\">\n",
        "\n",
        "### 線形結合と二次形式\n",
        "\n",
        "これからの機械学習の数学でもよく出てくる形式として $\\boldsymbol{b}^{T}\\boldsymbol{x}$ と $\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}$ のような２つがあります。前者を**線形結合**もしくは**一次結合**、$\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}$ のような形式を**二次形式**と呼ばれています。中学校の数学で言うなら、一次式か二次式かといった話だと分かりやすいと思います。\n",
        "\n",
        "線形結合の計算の中身を見てみると、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{b}&=\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{bmatrix},\\ \n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\boldsymbol{b}^{T}\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}=x_{1}+2x_{2}\\end{aligned}\n",
        "$$\n",
        "のように $\\boldsymbol{x}$ の要素である $x_{1}$ もしくは $x_{2}$ に関して、1次式となっていることがわかります。\n",
        "\n",
        "また、二次形式も同様に計算の中身を確認してみると、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{A}&=\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix},\\ \n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}\n",
        "&=\\begin{bmatrix} x_{1} & x_{2}\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "&=\\begin{bmatrix}x_{1} & x_{2}\\end{bmatrix} \\begin{bmatrix}\n",
        "x_{1}+2x_{2} \\\\\n",
        "3x_{1}+4x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "&=x_{1}\\left( x_{1}+2x_{2}\\right) +x_{2}\\left( 3x_{1}+4x_{2}\\right) \\\\\n",
        "&=x^{2}_{1}+5x_{1}x_{2}+4x_{2}^{2}\\end{aligned}\n",
        "$$\n",
        "となり、各要素において二次式となっていることがわかります。\n",
        "\n",
        "そして、一般にこれらを足し合わせて、\n",
        "$$\n",
        "\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{b}^{T}\\boldsymbol{x} + c\n",
        "$$\n",
        "のように二次関数を表現します。ここで、$c$ はスカラーの定数項です。\n",
        "\n",
        "### ベクトルで微分\n",
        "\n",
        "機械学習において何度も出てくる重要な演算ですが、参考書では書かれていないことがよくあります。\n",
        "\n",
        "まず、ベクトルで微分の計算を紹介する前に、下記の例題を計算しましょう。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{b}&=\\begin{bmatrix}\n",
        "3 \\\\\n",
        "4\n",
        "\\end{bmatrix}, \\ \n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\boldsymbol{b}^{T}\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "3 & 4\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\n",
        "=3x_{1}+4x_{2}\\end{aligned}\n",
        "$$\n",
        "この計算は線形結合としてすでに紹介していますが、ここからが本題です。この $\\boldsymbol{b}^{T}\\boldsymbol{x}$ を ベクトル $\\boldsymbol{x}$ で微分したいというシチュエーションが機械学習の中では登場します。つまり、\n",
        "$$\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right)\n",
        "$$\n",
        "を求めたいというわけです。これを**ベクトルで微分**すると言います。果たして、このベクトルで微分は複雑な計算か、シンプルな計算なのかどちらでしょうか。実はその答えは非常にシンプルであり、具体的には\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right) &=\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( 3x_{1}+4x_{2}\\right) \\\\\n",
        "&=\\begin{bmatrix}\n",
        "\\dfrac {\\partial }{\\partial x_{1}} \\left( 3x_{1}+4x_{2}\\right)  \\\\\n",
        "\\dfrac {\\partial }{\\partial x_{2}} \\left( 3x_{1}+4x_{2}\\right) \n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "のように、ベクトルの要素（今回だと $x_1$と$x_{2}$）のそれぞれで偏微分した値をベクトルとして格納していくだけです。一見複雑そうに見えますが、シンプルな演算で構成されていることがわかります。実際に計算していくと、\n",
        "$$\n",
        "\\begin{aligned}\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}+4x_{2}\\right) &=\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}\\right) +\\dfrac {\\partial }{\\partial x_{1}}\\left( 4x_{2}\\right) \\\\\n",
        "&=3\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( x_{1}\\right) +4x_{2}\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( 1\\right) \\\\\n",
        "&=3\\times 1+4x_{2}\\times 0\\\\\n",
        "&=3\\end{aligned}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\\dfrac {\\partial }{\\partial x_{2}}\\left( 3x_{1}+4x_{2}\\right)&=\\dfrac {\\partial }{\\partial x_{2}}\\left( 3x_{1}\\right) +\\dfrac {\\partial }{\\partial x_{2}}\\left( 4x_{2}\\right) \\\\\n",
        "&=3x_{1}\\times \\dfrac {\\partial }{\\partial x_{2}}\\left( 1\\right) +4\\times \\dfrac {\\partial }{ax_{2}}\\left( x_{2}\\right) \\\\\n",
        "&=3x_{1} \\times 0 + 4 \\times 1 \\\\\n",
        "&= 4\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "となり、下記の計算結果が得られます。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right) \n",
        "&=\\begin{bmatrix}\n",
        "\\dfrac {\\partial }{\\partial x_{1}} \\left( 3x_{1}+4x_{2}\\right)  \\\\\n",
        "\\dfrac {\\partial }{\\partial x_{2}} \\left( 3x_{1}+4x_{2}\\right) \n",
        "\\end{bmatrix} =\\begin{bmatrix}\n",
        "3  \\\\\n",
        "4\n",
        "\\end{bmatrix} = \\boldsymbol{b}\n",
        "\\end{aligned}\n",
        "$$\n",
        "ここで、ベクトルで微分した結果が最初の $\\boldsymbol{b}$ と同じになっていることがわかります。これはたまたまではなく、一般的に成り立ちますが、この結果は最後に公式としてまとめましょう。\n",
        "\n",
        "もう一問、以下の例題を考えましょう。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( 1\\right) &=\\begin{bmatrix}\n",
        "\\dfrac {\\partial }{\\partial x_{1}}\\left( 1\\right)  \\\\\n",
        "\\dfrac {\\partial }{\\partial x_{2}}\\left( 1\\right) \n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "0 \\\\\n",
        "0\n",
        "\\end{bmatrix}=\\boldsymbol{0}\\end{aligned}\n",
        "$$\n",
        "もちろん、偏微分を行う対象の変数が含まれていない場合は 0 となります。要素が 0 のみで構成されたベクトルを**零（ゼロ）ベクトル**と言います。\n",
        "\n",
        "これらを踏まえて、公式としてまとめておきましょう。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\left( 1\\right) \\ \\dfrac {\\partial}{\\partial \\boldsymbol{x}}\\left( c\\right) = \\boldsymbol{0}\\\\\n",
        "&\\left( 2\\right) \\ \\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right) = \\boldsymbol{b}\\\\\n",
        "&\\left( 3\\right) \\ \\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}\\right) =\\left( \\boldsymbol{A}+\\boldsymbol{A}^{T}\\right) \\boldsymbol{x}\\end{aligned}\n",
        "$$\n",
        "(1) と (2) に関してはすでに計算したものであり、(3) に関しては導出が少し複雑なので省略しますが、数値を代入して確認してみてください。こちらの 3 つの公式は機械学習を学んでいく上で非常に重要な公式となりますので、必ず覚えておきましょう。\n",
        "\n",
        "こういった行列などにおける公式は他にもたくさんあり、論文などを読む際にはどういった公式があるのかを知っておくことも重要です。その際に、私がよく用いる便利な公式集がネット上にあり、[The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) が無料で公開されており、おすすめです。\n",
        "\n",
        "\n",
        "\n",
        "## 統計\n",
        "\n",
        "### 確率や統計は何に使えるのか\n",
        "\n",
        "機械学習といえば確率や統計といったイメージで勉強する人も多いと思いますが、微分と線形代数を理解しておくだけで簡単なアルゴリズムだと説明することができ、確率や統計が出てくることはありません。それでは、確率統計では何を行うことができるのでしょうか。\n",
        "\n",
        "大きく２つあると考えており、**データの分布の情報を定量評価できる**ことと、**データの分布を定式化できる**ことです。一見、同じように見えるかも知れませんが、目的がそれぞれ異なります。前者では、データの平均などといった情報を知ることで、アルゴリズムの前の**データの前処理**として使うことができます。後者では、分布の情報を定式化して**機械学習のモデル**に使用していきます。要するに使い道が前処理なのか、モデル化なのかです。前者の使い方の場合、学ぶべき数学が少なくて済むことに対して、後者の場合、**ベイズ統計**と呼ばれる生成モデルを取り扱うための数学が必要となり、多くの前提知識を必要とします。確率統計とひとまとめで呼ばれがちですが、前者が統計で、後者は確率として区別しています。\n",
        "\n",
        "ここでは、最初からベイズ統計向けに必要な数学をまとめると多くの人が挫折してしまうため、まずデータの前処理のための統計を学んでいきましょう。この統計を学ぶことによって、次の2つの問題に対して解決策を提示できるようになります。各入力変数の値が0～100であったり、0～1であったり、スケールがばらばらであり、アルゴリズム内部でデータを扱う際にそのスケールの差による悪影響を受ける場合があります。このスケールを揃える方法を考えます。また、データの**欠損値**は簡単に見つけることができるため取り除くのは難しくないのですが、データの**外れ値**は「どのようなデータが外れているのか」といった定義が必要であり、この外れ値のて定義の仕方を考えます。この**スケーリング**と**外れ値除去**は、データの前処理として実務ではほぼ必ず行うものであり、重要なパートといえます。\n",
        "\n",
        "### 統計量\n",
        "\n",
        "ここでは、よく使う統計量とその意味について紹介します。\n",
        "\n",
        "まずははじめに、最も有名な統計量であろう**平均**です。この計算方法はすでに知られていますが、たとえば、300円, 400円, 500円の平均は、\n",
        "$$\n",
        "\\dfrac{300 + 400 + 500}{3} = 400\n",
        "$$\n",
        "となり、すべてを足し合わせて候補の数で割ります。これを定式化すると、\n",
        "$$\n",
        "\\begin{aligned}\\overline {x}=\\dfrac {x_{1}+x_{2}+\\ldots +x_{N}}{N}\n",
        "=\\dfrac {1}{N}\\sum ^{N}_{n=1}x_{n}\\end{aligned}\n",
        "$$\n",
        "のようになります。$n$ は**サンプルの数**を表すときの文字として、これから多用していきます。平均は $\\bar{x}$ や $\\mu$ で表すことが多いです。データの分布ではその中心に相当する値です。\n",
        "\n",
        "次に、**分散**を紹介します。２番目にして、日常生活では登場する場面が少なくなるため、分散とは何を表す値か、どのような場面で使用するかを知っているでしょうか。まず分散の定義は\n",
        "$$\n",
        "\\begin{aligned}\\sigma ^{2}=\\dfrac {1}{N}\\sum ^{N}_{n=1}\\left( x_{n}-\\overline {x}\\right) ^{2}\\end{aligned}\n",
        "$$\n",
        "であり、高校の数学で習います。平均 $\\bar{x}$ からの差分 $x- \\bar{x}$ を計算し、それが二乗誤差の場合と同様、正と負の値を持ってしまうため、二乗してすべてを正にしてから**総和**を取って、平均の値を計算しています。つまり、平均からどの程度離れているか（の二乗）の平均値だといえます。これを何に使うことができるかの前に、分散にはもう一つ定義があり、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\sigma ^{2}=\\dfrac {1}{N-1}\\sum ^{N}_{n=1}\\left( x_{n}-\\overline {x}\\right) ^{2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "と表す場合もある。前者は**母分散**といい、後者は**不偏分散**という。なぜ $N$ と $N-1$ であるかは他の解説にゆずるとして、まず大事なことはどちらを使えば良いかです。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/12.png\" height=\"200px\">\n",
        "\n",
        "データ解析を行う際に、**母集団**に対する解析か**標本集団**に対する解析かを意識します。母集団とは解析を行いたい想定の範囲に対して、すべてのデータが揃っているいる場合であり、標本集団はそのうちの一部を抽出する場合です。例えば、全国の小学生の身長と体重を集計する場合に、全国の小学生を一人の抜け漏れもなく集められれば母集団であり、各都道府県で100人ずつ抜き出して集計する場合は標本集団です。母集団のデータを集めることは現実的に難しいことが多く、標本集団のデータから母集団の分布を推定することが一般的です。そうなると、基本的には標本集団向けである不偏分散を使用することになります。\n",
        "\n",
        "さて、分散についての定義がわかったところで、これは何に使えるのでしょうか。分散はデータのばらつきを定量評価することができます。実験をしたときに、このばらつきが多ければ、各実験で再現性を確保できていないことを見つけることができるように、何度か試行して平均に集まっていることが望ましい状況において、ざっくりと感覚で議論するのではなく、定量評価できることはとても助かります。また、0～1のスケールのデータでは、分散は小さな値になり、0～1000のスケールのデータでは、分散は大きな値になります。もちろん、そのデータのばらつき具合にもよりますが、分散を使えば**スケールの違い**も評価することができます。そのため、この情報はスケーリングに使えそうであることがわかります。\n",
        "\n",
        "最後に**標準偏差**です。分散では平均からの二乗で計算を行うため、元のスケールの二乗となってしまいます。これでは、データに基づいて議論する場合に、二乗のスケールを考慮しながら話す必要があり、混乱しやすいのではないでしょうか。そこで、一般的には、元のスケールで議論するため、\n",
        "$$\n",
        "\\sigma = \\sqrt{\\sigma^{2}}\n",
        "$$\n",
        "のように、分散の平方根を用い、これを標準偏差と呼びます。\n",
        "\n",
        "それでは、定義と使い道が見えてきたところで、練習問題で具体的な計算手順の確認を行いましょう。以下の①と②のデータに対して、平均、分散、標準偏差を求めてください。ただし、今回は母分散を使用することとします。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/13.png\" height=\"150px\">\n",
        "\n",
        "①の解答は以下の通りです。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\bar{x}&=\\dfrac {1}{5}\\left( -2-1+0+1+2\\right) =0\\\\\n",
        "\\sigma ^{2}&=\\dfrac {1}{5}\\left\\{ \\left( -2-0\\right) ^{2}+\\left( -1-0\\right) ^{2}+(0-0)^{2}+(1-0)^{2}+(2-0)^{2}\\right\\} \\\\\n",
        "&=\\dfrac {1}{5}\\times 10=2\\\\\n",
        "\\sigma &=\\sqrt {2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "また、②の解答は以下の通りです。\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\overline {x}&=\\dfrac {1}{5}\\left( -4-2+0+2+4\\right) =0\\\\\n",
        "\\sigma ^{2}&=\\dfrac {1}{5}\\left\\{ \\left( -4-0\\right) ^{2}+\\left( -2-0\\right) ^{2}+\\left( 0-0\\right) ^{2}+\\left( 2-0\\right) ^{2}+\\left( 4-0\\right) ^{2}\\right\\} \\\\\n",
        "&=\\dfrac {1}{5}\\times 40=8\\\\\n",
        "\\sigma &=\\sqrt {8}=2\\sqrt {2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "これより、②のケースの方が分散が大きく、データのばらつきが大きいことがわかります。\n",
        "\n",
        "### 正規分布\n",
        "\n",
        "確率では何度も登場する**正規分布**です。**ガウス分布**とも呼ばれています。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/14.png\" height=\"200px\">\n",
        "\n",
        "では、なぜこの正規分布がよく登場するのでしょうか。その理由として、\n",
        "\n",
        "- 物理現象でこの分布に従うことがよくある\n",
        "- 数式が扱いやすい\n",
        "\n",
        "の物理的な背景と数学的な背景の２つがあるのではないかと思います。\n",
        "\n",
        "そして、正規分布では平均 $\\mu$ と標準偏差 $\\sigma$ に対して、何%がその分布に入っているかといった議論を良く行います。例えば、$\\mu \\pm 3\\sigma$ の範囲内に、データの全体の99.7%が入るため、この $\\mu \\pm 3 \\sigma$ に入らない領域を外れ値として定義するところに使い道がありそうです。\n",
        "\n",
        "### スケーリング\n",
        "\n",
        "スケーリングはどのアルゴリズムでも前処理として重要になってきますが、ここでは簡単に２つの事例を紹介します。\n",
        "\n",
        "まず１つ目が距離の問題です。スケールが異なる変数 $x_{1}$ と $x_{2}$ があった場合に、下記の図のような状況になります。ここで、縦軸と横軸のスケールが大きく異なりますが、わざと同じようなスケールに見えるようにプロットしています。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/15.png\" height=\"250px\">\n",
        "\n",
        "\n",
        "\n",
        "この２点間の距離 $d$ を求めると、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "d&=\\sqrt {\\left( 100-1000\\right) ^{2}+\\left( 0.1-1\\right) ^{2}}\\\\\n",
        "&= \\sqrt {900^{2}+0.9^{2}}\\\\\n",
        "&= \\sqrt {810000+0.81} \\\\\n",
        "&= \\sqrt {810000.81}\n",
        "\\end{aligned}\n",
        "$$\n",
        "のようになります。ここで着目したい点として、$x_{1}$ と $x_{2}$のどちらが距離 $d$ に対して影響を与えているかですが、明らかに $x_{1}$ です。$x_{2}$ に関しては、スケールが小さいがゆえにほとんど影響を与えていません。これでは $x_{2}$ がデータの意味として重要な場合においても考慮できずに終わってしまいます。この問題を**スケーリング**で解決できます。そして、代表的なスケーリングの方法としては２つあります。\n",
        "\n",
        "１つ目が、**最小値0**、**最大値1**にスケーリングする方法です。これを**Min-Max スケーリング**と呼びます。この方法は至って単純で、各変数ごとに最小値 $x_{\\min}$ と最大値 $x_{\\max}$ を求めておき、すべてのデータに対して、\n",
        "$$\n",
        "\\widetilde{x} = \\dfrac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
        "$$\n",
        "の計算を行います。Min-Maxスケーリングには計算が単純というメリットの反面、$x_1$で外れ値が存在し、$x_{\\max}$ が外れ値であるこの一点に大きく引っ張られてしまい、大幅な外れ値がある場合に弱点となります。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/16.png\" height=\"200px\">\n",
        "\n",
        "もう１つのスケーリングの方法として、**平均0**、**標準偏差1** にスケーリングする方法があります。分散含め、標準偏差ではデータのばらつきを定量評価することができ、\n",
        "$$\n",
        "\\widetilde{x}  = \\dfrac{x - \\bar{x}}{\\sigma}\n",
        "$$\n",
        "のように、標準偏差で割ることで、スケールを統一することができます。分散を計算した例題の①に対して、適用してみると、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "x_{1}&=\\dfrac {-2-0}{\\sqrt {2}}=-\\dfrac {2}{\\sqrt {2}}\\\\\n",
        "x_{2}&=\\dfrac {-1-0}{\\sqrt {2}}=-\\dfrac {1}{\\sqrt {2}}\\\\\n",
        "x_{3}&=\\dfrac {0-0}{\\sqrt {2}}=0\\\\\n",
        "x_{4}&=\\dfrac {1-0}{\\sqrt {2}}=\\dfrac {1}{\\sqrt {2}}\\\\\n",
        "x_{5}&=\\dfrac {2-0}{\\sqrt {2}}=\\dfrac {2}{\\sqrt {2}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "のように、データが変換されます。この時の平均と標準偏差を求めてみると、\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\overline {x}&=\\dfrac {1}{5}\\left( -\\dfrac {2}{\\sqrt {2}}-\\dfrac {1}{\\sqrt {2}}+0+\\dfrac {1}{\\sqrt {2}}+\\dfrac {2}{\\sqrt {2}}\\right) =0\\\\\n",
        "\\sigma ^{2}&=\\dfrac {1}{5}\\left\\{ \\left( -\\dfrac {2}{\\sqrt {2}}-0\\right) ^{2}+\\left( -\\dfrac {1}{\\sqrt {2}}-0\\right) ^{2}+\\left( 0-0\\right) ^{2}\n",
        " +\\left( \\dfrac {1}{\\sqrt {2}}-0\\right) ^{2}+\\left( \\dfrac {2}{\\sqrt {2}}-0\\right) ^{2}\\right\\} =1\\\\\n",
        "\\sigma &=\\sqrt {\\sigma ^{2}}=1\n",
        "\\end{aligned}\n",
        "$$\n",
        "のように、平均0、標準偏差1にスケーリングできていることがわかります。この方法であれば、統計量を使用するため全体の傾向で議論することができ、１点だけの外れ値のようなケースには強い（これを**ロバスト**と表現する）と言えます。\n",
        "\n",
        "### 外れ値除去\n",
        "\n",
        "例以下の図のように時間によって変動するようなデータを扱うとしましょう。例えば、横軸が時刻、縦軸がCPUの負荷率(%)と考えるとわかりやすいと思います。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/17.png\" height=\"200px\">\n",
        "\n",
        "このデータに対して、CPUの負荷率が異常な場合（外れ値）を検出したいという場合、どのようにこの外れ値を定義して検出すれば良いでしょうか。その答えは、その値の**頻度**に着目します。\n",
        "\n",
        "<img src=\"https://github.com/mitmul/medical-ai-course-materials/raw/master/notebooks/images/1/18.png\" height=\"230px\">\n",
        "\n",
        "上図のように、平均に対して線を引き、それぞれの値において頻度を算出してヒストグラムを描いてみると正規分布が現れます。物理現象として正規分布に従うものが多いと説明していましたが、普段の生活で正規分布を目にすることがないので違和感を感じていた人もいると思いますが、このように中心付近の値の頻度は多く、離れるほど頻度が少なくなっていく現象にはよく遭遇しているはずであり、このような状況に正規分布が適しています。そして、外れ値を定義するために、データの平均 $\\mu$ と標準偏差 $\\sigma$ を計算し、$\\mu \\pm 3\\sigma$の値に線を引けば、外れ値除去を行うことができます。これを**3σ法**と呼び、理論がシンプルかつ、プログラムの実装的にも平均と標準偏差だけで行える簡単です。外れ値の回数が多い場合などは平均や標準偏差がその外れ値に引っ張られ、3σ法ではうまく対処できない場合があり、そのような場合には中央値をベースとした**Hampel判別法**を用います。"
      ]
    }
  ]
}