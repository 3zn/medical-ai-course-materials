

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ja" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ja" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>3. ニューラルネットワークの基礎 &mdash; メディカルAIコース オンライン講義資料&lt;Paste&gt;  ドキュメント</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="4. Deep Learningフレームワークの基礎" href="Introduction_to_Chainer.html" />
    <link rel="prev" title="2. 機械学習ライブラリの基礎" href="Introduction_to_ML_libs.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAIコース オンライン講義資料<Paste>
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Basic_Math_for_ML.html">1. 機械学習に必要な数学の基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_ML_libs.html">2. 機械学習ライブラリの基礎</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">3. ニューラルネットワークの基礎</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#ニューラルネットワークの構造">3.1. ニューラルネットワークの構造</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#非線形変換">3.1.1. 非線形変換</a></li>
<li class="toctree-l3"><a class="reference internal" href="#数値を見ながら計算の流れを確認">3.1.2. 数値を見ながら計算の流れを確認</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#目的関数">3.2. 目的関数</a></li>
<li class="toctree-l2"><a class="reference internal" href="#パラメータの最適化">3.3. パラメータの最適化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#パラメータ更新量の算出">3.3.1. パラメータ更新量の算出</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#誤差逆伝播法（バックプロパゲーション）">3.4. 誤差逆伝播法（バックプロパゲーション）</a></li>
<li class="toctree-l2"><a class="reference internal" href="#勾配消失">3.5. 勾配消失</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">4. Deep Learningフレームワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. 実践編: MRI画像のセグメンテーション</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">6. 実践編: 血液の顕微鏡画像からの細胞検出</a></li>
<li class="toctree-l1"><a class="reference internal" href="DNA_Sequence_Data_Analysis.html">7. 実践編：ディープラーニングを使った配列解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html">8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAIコース オンライン講義資料<Paste></a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>3. ニューラルネットワークの基礎</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Introduction_to_Neural_Network.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<p><a class="reference external" href="https://colab.research.google.com/github/japan-medical-ai/medical-ai-course-materials/blob/master/notebooks/Introduction_to_Neural_Network.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="ニューラルネットワークの基礎">
<h1>3. ニューラルネットワークの基礎<a class="headerlink" href="#ニューラルネットワークの基礎" title="このヘッドラインへのパーマリンク">¶</a></h1>
<p>ここでは，ニューラルネットワーク (Neural Network)
について簡単に紹介していきます．画像認識などに用いられる Convolutional
Neural Network (CNN) や，自然言語処理などに用いられる Recurrent Neural
Network (RNN) といった手法は，ニューラルネットワークの一種です．</p>
<p>ここではまず，最もシンプルなニューラルネットワークの構造について説明を行ったあと，複数の入力データと望ましい出力の組からなる学習用データセットを準備したとき，どうやってニューラルネットを学習させればよいのか（教師あり学習）について解説を行います．</p>
<p>ニューラルネットワークによって表現される複雑な関数を，現実的な時間で学習する手法についても紹介します．</p>
<p>まずはニューラルネットワークをブラックボックスとして扱ってしまうのではなく，一つ一つ内部で行われる計算を丁寧に調べます．そして，パラメータで特徴づけられた関数で表される線形変換とそれに続く非線形変換を組み合わせて，全体として微分可能な一つの関数を表していることを理解していきます．</p>
<div class="section" id="ニューラルネットワークの構造">
<h2>3.1. ニューラルネットワークの構造<a class="headerlink" href="#ニューラルネットワークの構造" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>まずはニューラルネットワークの構造を図式化して見てみましょう．</p>
<div class="figure" id="id10">
<img alt="01.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/01.png" />
<p class="caption"><span class="caption-text">01.png</span></p>
</div>
<p>この図のひとつひとつの丸い部分のことを<strong>ノード</strong>と呼び，そのノードの縦方向の集まりを<strong>層</strong>と呼びます．そして，一番初めの層を<strong>入力層</strong>，最後の層を<strong>出力層</strong>，そしてその間を<strong>中間層</strong>もしくは<strong>隠れ層</strong>と呼びます．このモデルは入力層，中間層，出力層の３層の構造となっていますが，中間層の数を増やすことでさらに多層のニューラルネットワークを定義することもできます．この例では各層間の全てのノードが結合されているため，<strong>全結合のニューラルネットワーク</strong>とも呼び，ニューラルネットワークの最も基礎的な構造です．</p>
<p>入力変数はこれまでと同様ですが，出力変数が連続値ではなく離散値となっています．例えば，上図では出力層の各ノードがそれぞれ白ワインと赤ワインに対応しています．このようにカテゴリの数だけ出力の変数があるということになります．なぜこのような構造となっているのでしょうか．</p>
<div class="figure" id="id11">
<img alt="02.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/02.png" />
<p class="caption"><span class="caption-text">02.png</span></p>
</div>
<p>まず，最終層にどのような値が入るのか，具体例を見てみましょう．例えば，年数が3年物でアルコール度数が14度，色合いが0.2，匂いが0.8で表されるワインがあるとします．内部の計算は後述するとして，このようなデータをニューラルネットワークに与えると，白ワイン
<span class="math notranslate nohighlight">\(y_{1} = 0.15\)</span>, 赤ワイン <span class="math notranslate nohighlight">\(y_{2}= 0.85\)</span>
という値が得られました．このとき，出力値の中で最も大きな値となっている変数に対応するクラス，すなわち「赤ワイン」をこの分類問題におけるこのニューラルネットワークによる予測結果とすることができます．</p>
<p>ここで出力ノードすべての値を合計してみると，1になっていることに気づきます．これは偶然ではなく，そうなるように出力層の値を計算しているためです．つまり，出力層のそれぞれのノードが持つ数値は，入力がそれぞれのクラスに属している確率を表していたのでした．そのため，カテゴリ数と同じ数だけ出力層にはノードが必要となります．</p>
<p>それでは，ここからニューラルネットワークの内部で行われる計算を詳しく見ていきましょう．ニューラルネットワークの各層は，前の層の値に線形変換と非線形変換を順番に施すことで計算されています．まずは，ここで言う線形変換とは何を表すのか，から見ていきましょう．</p>
<p>oko### 線形変換</p>
<div class="figure" id="id12">
<img alt="03.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/03.png" />
<p class="caption"><span class="caption-text">03.png</span></p>
</div>
<p>ここで言う線形変換とは，重み行列 (<span class="math notranslate nohighlight">\(w\)</span>) ×入力ベクトル (<span class="math notranslate nohighlight">\(h\)</span>) +
バイアスベクトル (<span class="math notranslate nohighlight">\(b\)</span>)
のような計算のことを指しています．ここでの掛け算は行列の掛け算であることに注意してください．また，これからは，<span class="math notranslate nohighlight">\(h\)</span>
が文字としてよく登場しますが，これは隠れ層 (hidden layer) の頭文字である
<span class="math notranslate nohighlight">\(h\)</span>
から来ています．ただし，表記の簡潔にするため入力層（上図における
<span class="math notranslate nohighlight">\(x_1, x_2, x_3, x_4\)</span>）も，<span class="math notranslate nohighlight">\(0\)</span>層目の隠れ層と考えることにして，以下では
<span class="math notranslate nohighlight">\(h_01, h_02, h_03, h_04\)</span>
と表記します．では上図で表される計算を数式で記述してみましょう．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
u_{11}&amp;=w_{11}h_{01}+w_{12}h_{02}+w_{13}h_{03}+w_{14}h_{04}+b_{1} \\
u_{12}&amp;=w_{21}h_{01}+w_{22}h_{02}+w_{23}h_{03}+w_{24}h_{04}+b_{2} \\
u_{13}&amp;=w_{31}h_{01}+w_{32}h_{02}+w_{33}h_{03}+w_{34}h_{04}+b_{2}
\end{aligned}\end{split}\]</div>
<p>これは，ベクトルと行列の計算として書き直すことができ，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\begin{bmatrix}
u_{11} \\
u_{12} \\
u_{13}
\end{bmatrix}&amp;=\begin{bmatrix}
w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14} \\
w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24} \\
w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34}
\end{bmatrix}\begin{bmatrix}
h_{01} \\
h_{02} \\
h_{03} \\
h_{04}
\end{bmatrix}+\begin{bmatrix}
b_{1} \\
b_{2} \\
b_{3}
\end{bmatrix}\\
{\bf u}_{1}&amp;={\bf W}{\bf h}_{0}+{\bf b}
\end{aligned}\end{split}\]</div>
<p>と同じことです．本来は <span class="math notranslate nohighlight">\({\bf W}\)</span> や <span class="math notranslate nohighlight">\({\bf b}\)</span>
にもどの層とどの層の間の計算に用いられるものなのかを表す添え字をつけるべき（たとえば<span class="math notranslate nohighlight">\({\bf W_{1 \to 2}}\)</span>のように）ですが，ここでは簡単のため省略しています．</p>
<div class="section" id="非線形変換">
<h3>3.1.1. 非線形変換<a class="headerlink" href="#非線形変換" title="このヘッドラインへのパーマリンク">¶</a></h3>
<div class="figure" id="id13">
<img alt="04.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/04.png" />
<p class="caption"><span class="caption-text">04.png</span></p>
</div>
<p>次に，非線形変換について説明します．線形変換のみでは，上図右のように入力と出力の間が非線形な関係である場合は，両者の間の関係を適切に表現することができません．そこで，ニューラルネットワークでは各層で線形変換に引き続いて非線形変換を施すことで，全体の関数が非線形性を持つようにしています．この非線形変換を行う関数を，ニューラルネットワークの文脈においては
<strong>活性化関数</strong> と呼びます．</p>
<div class="figure" id="id14">
<img alt="05.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/05.png" />
<p class="caption"><span class="caption-text">05.png</span></p>
</div>
<p>ニューラルネットワークでは上図に示す
<strong>ロジスティックシグモイド関数</strong>（以下シグモイド関数）</p>
<div class="math notranslate nohighlight">
\[h = f(u) = \dfrac{1}{1+e^{-u}}\]</div>
<p>が従来，よく用いられてきました．しかし，近年，層数が多いニューラルネットワークではシグモイド関数は活性化関数としてほとんど用いられていません．その理由の一つは，シグモイド関数を活性化関数に採用することで
<strong>勾配消失</strong>
という現象が起きやすくなるからです．これは後で詳述します．これを回避するために，<strong>Rectified
Linear Unit (ReLU)</strong>
という関数がよく用いられています．これは，以下のような形をした関数です．</p>
<p><img alt="06.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/06.png" /></p>
<div class="math notranslate nohighlight">
\[h = f(u) = \max(0, u)\]</div>
<p>これは，入力が負の値の場合には出力は0で一定であり，正の値の場合は入力をそのまま出力するという関数です．シグモイド関数では，入力が小さな，もしくは大きな値をとった際に，勾配がどんどん小さくなってしまうだろうことがプロットからも明らかに見て取れます．それに対し，ReLU関数は入力の値がいくら大きくなっても，一定の勾配が発生します．これがのちほど紹介する勾配消失という問題に有効に働くのです．</p>
</div>
<div class="section" id="数値を見ながら計算の流れを確認">
<h3>3.1.2. 数値を見ながら計算の流れを確認<a class="headerlink" href="#数値を見ながら計算の流れを確認" title="このヘッドラインへのパーマリンク">¶</a></h3>
<div class="figure" id="id15">
<img alt="07.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/07.png" />
<p class="caption"><span class="caption-text">07.png</span></p>
</div>
<p>ここで，上図に書き込まれた具体的な数値を使って，入力
<span class="math notranslate nohighlight">\(x_1, x_2, x_3\)</span> から出力 <span class="math notranslate nohighlight">\(y\)</span>
が計算される過程を確認してみましょう．今は計算を簡略化するためバイアス
<span class="math notranslate nohighlight">\({\bf b}\)</span>
の計算は省略します．数値例として，<span class="math notranslate nohighlight">\({\bf x}^T = \begin{bmatrix} 2 &amp; 3 &amp; 1 \end{bmatrix}\)</span>
が与えられた時の出力 <span class="math notranslate nohighlight">\(y\)</span>
の計算手順を一つ一つ追いかけてみましょう．</p>
<p>重回帰分析では，目的関数のパラメータについての導関数を0とおいて解析的に最適なパラメータを計算できましたが，ニューラルネットワークでは一般的に解析的にパラメータを解くことはできません．その代わり，この導関数の値（勾配）を利用した別の方法でパラメータを逐次的に最適化していきます．</p>
<p>このため，ニューラルネットワークの場合は，まずパラメータを乱数で初期化し，ひとまずデータを入力して目的関数の値を計算します．次にその関数の勾配を計算して，それを利用してパラメータを更新し，その更新後の新しいパラメータを使って再度入力を処理して目的関数の値を計算し…といったことを繰り返し行っていくことになります．今，上の図のグラフの枝に与えられているような数値でパラメータを初期化した状態で，入力層の値に線形変換を施すところまでを考えてみましょう．この計算は，以下のようになります．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
u_{11}&amp;=3\times 2+1\times 3+2\times 1=11\\
u_{12}&amp;=-2\times 2-3\times 3-1\times 1=-11
\end{aligned}\end{split}\]</div>
<p>次に非線形変換を行う活性化関数としてReLU関数を採用し，以下のように中間層の値を計算します．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
h_{11} &amp;= \max(0, 11) = 11 \\
h_{12} &amp;= \max(0, -11)  = 0
\end{aligned}\end{split}\]</div>
<p>同様に，出力層の <span class="math notranslate nohighlight">\(y\)</span> の値までを計算すると，</p>
<div class="math notranslate nohighlight">
\[y = 3 \times 11 + 2 \times 0 = 33\]</div>
<p>となります．</p>
<p>さて，次節からは，パラメータを，どうやって更新していくかを見てみましょう．</p>
</div>
</div>
<div class="section" id="目的関数">
<h2>3.2. 目的関数<a class="headerlink" href="#目的関数" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ニューラルネットワークでは，微分可能でさえあれば解きたいタスクに合わせて様々な目的関数を利用することができます．</p>
<p>例えば，出力層に<span class="math notranslate nohighlight">\(N\)</span>個の値を持つニューラルネットワークで回帰問題を解く場合を考えてみましょう．<span class="math notranslate nohighlight">\(N\)</span>個の出力それぞれ（<span class="math notranslate nohighlight">\(y_n (n=1, 2, \dots, N)\)</span>）に対して望ましい出力（<span class="math notranslate nohighlight">\(t_n (n=1, 2, \dots, N)\)</span>）が与えられたとき，目的関数をそれぞれの出力（<span class="math notranslate nohighlight">\(y_n\)</span>）と対応する正解（<span class="math notranslate nohighlight">\(t_n\)</span>）の間の
<strong>平均二乗誤差（mean squared error）</strong>
とすることで，回帰問題を解くことができます．</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \dfrac{1}{N} \sum_{n=1}^{N}(t_{n} - y_{n})^{2}\]</div>
<p>これを最小にするようなパラメータを求めればよいということになります．例えば，上図の例で正解として
<span class="math notranslate nohighlight">\(t = 20\)</span> が与えられたときの目的関数の値は，</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \dfrac{1}{1} (20 - 33)^2 = 169\]</div>
<p>となります．</p>
<p>一方，分類問題の場合は <strong>交差エントロピー（cross entropy）</strong>
が目的関数として多くの場合利用されます．<span class="math notranslate nohighlight">\(N\)</span>クラスの分類問題を考えるとき，<span class="math notranslate nohighlight">\(N\)</span>個の出力
<span class="math notranslate nohighlight">\(y_n (n=1, 2, \dots, N)\)</span>
が入力がそれぞれのクラスに属する確率（<span class="math notranslate nohighlight">\(y_i = p(y=i|x)\)</span>）を表しているとして，正解が1-hotベクトルで与えられる時，以下の計算で定義されるものがクロスエントロピーです．</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = - \sum_{n=1}^{N}t_{n}\log y_{n}\]</div>
<p>以下は交差エントロピーの定義について知りたい方だけ参考にしてください．情報理論などで交差エントロピーの定義を知っている方はこの値が交差エントロピーと違うようにみえます．これは以下のように説明できます．<span class="math notranslate nohighlight">\(q(y|x)\)</span>をニューラルネットワークのモデルが定義する条件付き確率であり，<span class="math notranslate nohighlight">\(p(y|x)\)</span>を実データの条件付き確率とします．<span class="math notranslate nohighlight">\(p(y|x)\)</span>は未知ですが，代わり学習データの経験分布
<span class="math notranslate nohighlight">\(p(y|x) = \frac{1}{n} \sum_i I(x =x_i, y=y_i)\)</span>であるとします．ただし<span class="math notranslate nohighlight">\(I\)</span>はディラック関数とよばれ，その等号が成立する時，値が<span class="math notranslate nohighlight">\(\infty\)</span>，それ以外では<span class="math notranslate nohighlight">\(0\)</span>であるような関数です．この時，確率分布p(y|x)<span class="math notranslate nohighlight">\(と\)</span>q(y|x)$間の交差エントロピーは</p>
<div class="math notranslate nohighlight">
\[\int_{x, y} p(y|x) \log \frac{q(y|x)}{p(y|x)}   dx dy\]</div>
<p>と定義されます．ここでディラック関数の定義および，<span class="math notranslate nohighlight">\(p\)</span>だけに依存する項を除くと，先程の交差エントロピーの目的関数が導出されます．</p>
</div>
<div class="section" id="パラメータの最適化">
<h2>3.3. パラメータの最適化<a class="headerlink" href="#パラメータの最適化" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>最適化の方法を考える前に，まず最適化の対象とはなんであったか，再度確認しましょう．ニューラルネットワークにおける“パラメータ”とは，ここまで紹介したシンプルな全結合型ニューラルネットワークの場合，各層の線形変換に用いられていた
<span class="math notranslate nohighlight">\({\bf W}\)</span> と <span class="math notranslate nohighlight">\({\bf b}\)</span> のことを指します．</p>
<p>ニューラルネットワークでは，目的関数の各パラメータについての勾配を0とおいて解析的に解くことは一般的には困難です．しかし，実際にデータをニューラルネットワークに入力してその入力の値における目的関数のパラメータについての勾配を数値的に求めることは可能です．この値が分かれば，パラメータをどのように変化させれば，その入力が再び与えられたときに出力される目的関数の値を小さく（または大きく）することができるのか，が分かります．すなわち，この勾配を使ってパラメータの最適化を行うことができます．この方法について説明を行います．</p>
<p>では，まず以下の図を御覧ください．図中の点線は，パラメータ <span class="math notranslate nohighlight">\(w\)</span>
に対する目的関数 <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
を表しています．この例では簡単のため二次関数になっていますが，実際にはもっと複雑な関数であることがほとんどでしょう．この目的関数が最小値を与えるような
<span class="math notranslate nohighlight">\(w\)</span> は，どのように発見することができるでしょうか．</p>
<div class="figure" id="id16">
<img alt="13.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/13.png" />
<p class="caption"><span class="caption-text">13.png</span></p>
</div>
<p>前節で説明したように，ニューラルネットワークのパラメータはまず乱数で初期化されます．ここでは，例として
<span class="math notranslate nohighlight">\(w=3\)</span>
となったと考えてみましょう．そうすると，<span class="math notranslate nohighlight">\(w=3\)</span>における<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>の勾配
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial w}\)</span>
が求まります．ここでは，仮に <span class="math notranslate nohighlight">\(w=3\)</span> における
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial w}\)</span> が <span class="math notranslate nohighlight">\(3\)</span>
であったとしましょう．すると，以下の図のように，この <span class="math notranslate nohighlight">\(3\)</span>
という値は <span class="math notranslate nohighlight">\(w=3\)</span> における <span class="math notranslate nohighlight">\(\mathcal{L}(w)\)</span>
という関数の接戦の傾きを表し，これは
<strong>:math:`w`を微小量増加させた時の:math:`mathcal{L}`の割合変化量</strong>
という意味です‥</p>
<div class="figure" id="id17">
<img alt="11.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/11.png" />
<p class="caption"><span class="caption-text">11.png</span></p>
</div>
<p>もし，<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>の値を小さくしたいのであれば，勾配とは逆の方向に
<span class="math notranslate nohighlight">\(w\)</span> を変化させる，すなわち <strong>:math:`w` から
:math:`frac{partial mathcal{L}}{partial w}` を引いてやればよい</strong>
でしょう．これがニューラルネットワークのパラメータを目的関数の勾配を用いて更新していく際の基本的な考え方です．このときの
<span class="math notranslate nohighlight">\(w\)</span> のステップサイズ（更新量）のスケールを調整するために，勾配に
<strong>学習率</strong> (learning rate) と呼ばれる値を乗じるのが一般的です．</p>
<p>例えば，今学習率を <span class="math notranslate nohighlight">\(0.5\)</span>
に設定してみます．そうすると，<span class="math notranslate nohighlight">\(w\)</span>の更新量は <span class="math notranslate nohighlight">\(-\)</span> 学習率
<span class="math notranslate nohighlight">\(\times\)</span> 勾配で決まるので，<span class="math notranslate nohighlight">\(-0.5 \times 3 = -1.5\)</span>
となります．現在 <span class="math notranslate nohighlight">\(w=3\)</span> なので，<span class="math notranslate nohighlight">\(w \leftarrow w - 1.5\)</span>
と更新した後は， <span class="math notranslate nohighlight">\(w=1.5\)</span>
となります．ここでさらに，この点においても，勾配を求めてみます．その結果が
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial w} |_{w=1.5} = -1\)</span>
であったとしましょう．すると，次の更新量は，<span class="math notranslate nohighlight">\(- 0.5 \times -(-1) = 0.5\)</span>
となります．以上の通りに2回更新したあとは，以下の図のような位置に
<span class="math notranslate nohighlight">\(w\)</span> はあるでしょう．</p>
<div class="figure" id="id18">
<img alt="12.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/12.png" />
<p class="caption"><span class="caption-text">12.png</span></p>
</div>
<p>「-1 <span class="math notranslate nohighlight">\(\times\)</span> 学習率 <span class="math notranslate nohighlight">\(\times\)</span>
勾配」を更新量としてパラメータを変化させていくと，<span class="math notranslate nohighlight">\(w\)</span> は求めたい
<span class="math notranslate nohighlight">\(\mathcal{L}\)</span> の最小値を与える <span class="math notranslate nohighlight">\(w\)</span>
に徐々に近づいていきます．このように，勾配を使って目的関数の最小化（もしくは最大化）を行う手法を
<strong>勾配降下法</strong> と呼びます．ニューラルネットワークは，基本的に
<strong>微分可能な関数のみを層間をつなぐ関数として用いて</strong>
設計されるため，登場する線形変換と非線形変換はすべて微分可能であり，学習データセットを用いて勾配降下法によってパラメータを最適化する方法が適用可能です．</p>
<p>ニューラルネットワークを勾配降下法で最適化する場合は，学習データセット内に存在するすべてのサンプルに対して目的関数の値を計算し，その総和を最小にするようにパラメータを更新する<strong>バッチ最適化</strong>という方法<strong>ではなく</strong>，「<strong>ミニバッチ学習</strong>」と呼ばれる方法をとるのが一般的です．これは，学習データセットからランダムに<span class="math notranslate nohighlight">\(k (&gt;0)\)</span>個のデータを抽出し，その<span class="math notranslate nohighlight">\(k\)</span>個のデータに対する目的関数の平均を最小化するという方法です．このときの<span class="math notranslate nohighlight">\(k\)</span>をバッチサイズもしくはミニバッチサイズと呼び，このような方法は，<strong>確率的勾配降下法</strong>
(SGD: Stocastic Gradient Descent)
と呼ばれます．現在ほとんどすべてのニューラルネットワークのための最適化手法はこのSGDをベースとした手法となっています．SGDを用いると，全体の計算時間が劇的に少なくできるだけでなく，下図のように目的関数が凸関数でなかったとしても，“ほとんど確実に”局所最適解に収束することが知られています．</p>
<div class="figure" id="id19">
<img alt="14.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/14.png" />
<p class="caption"><span class="caption-text">14.png</span></p>
</div>
<div class="section" id="パラメータ更新量の算出">
<h3>3.3.1. パラメータ更新量の算出<a class="headerlink" href="#パラメータ更新量の算出" title="このヘッドラインへのパーマリンク">¶</a></h3>
<div class="figure" id="id20">
<img alt="08.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/08.png" />
<p class="caption"><span class="caption-text">08.png</span></p>
</div>
<p>それでは今，上図のような3層の全結合型ニューラルネットワークを考え，1層目と2層目の間の線形変換が
<span class="math notranslate nohighlight">\({\bf w}_1, {\bf b}_1\)</span>
というパラメータによって表され，2層目と3層目の間の線形変換が
<span class="math notranslate nohighlight">\({\bf w}_2, {\bf b}_2\)</span>
というパラメータによって表されるとします（図ではバイアス
<span class="math notranslate nohighlight">\({\bf b}_1, {\bf b}_2\)</span> は省略されています）．これらをまとめて
<span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span> と表すことにします．</p>
<p>入力ベクトルは <span class="math notranslate nohighlight">\({\bf x}\)</span>，ニューラルネットワークの出力は
<span class="math notranslate nohighlight">\({\bf y} \in \mathbb{R}^N\)</span>とし，入力 <span class="math notranslate nohighlight">\({\bf x}\)</span>
に対応した“望ましい出力”である教師ベクトルを <span class="math notranslate nohighlight">\({\bf t}\)</span>
とします．ここで，目的関数には前述の平均二乗誤差関数を用いることとします．さて，パラメータをそれぞれ適当な乱数で初期化したあと，入力
<span class="math notranslate nohighlight">\({\bf x}\)</span>
が与えられたときの目的関数の各パラメータについての勾配を計算して，それぞれのパラメータについて更新量を算出してみましょう．</p>
<p>まず，目的関数を改めてベクトル表記を用いて書き下すと，以下のようになります．</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}({\bf y}, {\bf t}) = \frac{1}{N} || {\bf t} - {\bf y} ||_2\]</div>
<p>ここで，ニューラルネットワーク全体を <span class="math notranslate nohighlight">\(f\)</span> と書くことにすると，出力
<span class="math notranslate nohighlight">\({\bf y}\)</span> は</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
{\bf y} &amp;= f({\bf x}; \boldsymbol{\Theta}) \\
&amp;= a_2 ( {\bf w}_2 a_1({\bf w}_1 {\bf x} + {\bf b}_1) + {\bf b}_2 )
\end{aligned}\end{split}\]</div>
<p>と書くことができます．ここで，<span class="math notranslate nohighlight">\(a_1, a_2\)</span>
はそれぞれ，1層目と2層目の，および2層目と3層目の間で線形変換のあとに施される非線形変換（活性化関数）です．以下，簡単のために，各層間で行われた線形変換の結果を
<span class="math notranslate nohighlight">\({\bf u}_1, {\bf u}_2\)</span>とし，中間層の値，すなわち
<span class="math notranslate nohighlight">\({\bf u}_1\)</span> に活性化関数を適用した結果を <span class="math notranslate nohighlight">\({\bf h}_1\)</span>
と書きます．<span class="math notranslate nohighlight">\({\bf u}_2\)</span> に活性化関数を適用した結果は
<span class="math notranslate nohighlight">\({\bf y}\)</span>
です．すると，これらの関係は以下のように整理することができます．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
{\bf y} &amp;= a_2({\bf u}_2) \\
{\bf u}_2 &amp;= {\bf w}_2 {\bf h}_1 + {\bf b}_2 \\
{\bf h}_1 &amp;= a_1({\bf u}_1) \\
{\bf u}_1 &amp;= {\bf w}_1 {\bf x} + {\bf b}_1
\end{aligned}\end{split}\]</div>
<p>それではまず，出力層に近い方のパラメータ，<span class="math notranslate nohighlight">\({\bf w}_2\)</span> についての
<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
の勾配を求めてみましょう．これは，合成関数の偏微分なので，連鎖率を用いて以下のように展開できます．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial {\bf w}_2}
&amp;= \frac{\partial \mathcal{L}}{\partial {\bf y}} \frac{\partial {\bf y}}{\partial {\bf w}_2} \\
&amp;= \frac{\partial \mathcal{L}}{\partial {\bf y}} \frac{\partial {\bf y}}{\partial {\bf u}_2} \frac{\partial {\bf u}_2}{\partial {\bf w}_2}
\end{aligned}\end{split}\]</div>
<p>この3つの偏微分はそれぞれ，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial {\bf y}}
&amp;= -\frac{2}{N} ({\bf t} - {\bf y}) \\
\frac{\partial {\bf y}}{\partial {\bf u}_2}
&amp;= \frac{\partial a_2}{\partial {\bf u}_2} \\
\frac{\partial {\bf u}_2}{\partial {\bf w}_2}
&amp;= {\bf h}_1
\end{aligned}\end{split}\]</div>
<p>と求まります．ここで，活性化関数の入力に関する出力の勾配（<span class="math notranslate nohighlight">\(\frac{\partial a_2}{\partial {\bf u}_2}\)</span>）が登場しました．これは，例えば活性化関数にシグモイド関数を用いる場合は，</p>
<div class="math notranslate nohighlight">
\[a_2({\bf u}_2) = \frac{1}{1 + \exp(-{\bf u}_2)}\]</div>
<p>を微分すればよく，すなわち</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial a_2({\bf u}_2)}{\partial {\bf u}_2}
&amp;= -\frac{-(\exp(-{\bf u}_2))}{(1 + \exp(-{\bf u}_2))^2} \\
&amp;= \frac{1}{1 + \exp(-{\bf u}_2)} \cdot \frac{\exp(-{\bf u}_2)}{1 + \exp(-{\bf u}_2)} \\
&amp;= \frac{1}{1 + \exp(-{\bf u}_2)} \cdot \frac{1 + \exp(-{\bf u}_2) - 1}{1 + \exp(-{\bf u}_2)} \\
&amp;= \frac{1}{1 + \exp(-{\bf u}_2)} (1 - \frac{1}{1 + \exp(-{\bf u}_2)}) \\
&amp;= a_2({\bf u}_2)(1 - a_2({\bf u}_2))
\end{aligned}\end{split}\]</div>
<p>となります．シグモイド関数の勾配は，このようにシグモイド関数の出力値を使って簡単に計算することができます．</p>
<p>これで <span class="math notranslate nohighlight">\({\bf w}_2\)</span>
の勾配を計算するのに必要な値は全部計算できそうです．では実際にNumPyを使ってこの勾配を計算してみましょう．ここでは簡単のために，バイアスベクトルはすべて0で初期化されているとします．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import numpy as np

# 入力
x = np.array([2, 3, 1])

# 正解
t = np.array([20])
</pre></div>
</div>
</div>
<p>まず，NumPyモジュールを読み込んでから，入力の配列を定義します．ここでは，上図と同じになるように
<code class="docutils literal notranslate"><span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">1</span></code>
の3つの値を持つ3次元ベクトルを定義しています．また，正解として仮に
<code class="docutils literal notranslate"><span class="pre">20</span></code> を与えることにしました．次に，パラメータを定義します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># 1-2層間のパラメータ
w1 = np.array([[3, 1, 2], [-2, -3, -1]])
b1 = np.array([0, 0])

# 2-3層間のパラメータ
w2 = np.array([[3, 2]])
b2 = np.array([0])
</pre></div>
</div>
</div>
<p>ここでは，以下の4つのパラメータを定義しました．</p>
<p><strong>1層目と2層目の間の線形変換のパラメータ</strong></p>
<p><span class="math notranslate nohighlight">\({\bf w}_1 \in \mathbb{R}^{2 \times 3}\)</span> :
3次元ベクトルを2次元ベクトルに変換する行列</p>
<p><span class="math notranslate nohighlight">\({\bf b}_1 \in \mathbb{R}^2\)</span> : 2次元バイアスベクトル</p>
<p><strong>2層目と3層目の間の線形変換のパラメータ</strong></p>
<p><span class="math notranslate nohighlight">\({\bf w}_2 \in \mathbb{R}^{1 \times 2}\)</span> :
2次元ベクトルを1次元ベクトルに変換する行列</p>
<p><span class="math notranslate nohighlight">\({\bf b}_2 \in \mathbb{R}^1\)</span> : 1次元バイアスベクトル</p>
<p>それでは，各層の計算を実際に実行してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># 中間層の計算
u1 = w1.dot(x) + b1
h1 = 1. / (1 + np.exp(-u1))

# 出力の計算
u2 = w2.dot(h1) + b2
y = 1. / (1 + np.exp(-u2))

print(y)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0.95257194]
</pre></div></div>
</div>
<p>出力は <span class="math notranslate nohighlight">\(0.95257194\)</span>
と求まりました．つまり，<span class="math notranslate nohighlight">\(f([2, 3, 1]^T) = 0.95257194\)</span>
ということになります．次に，上で求めた</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}}{\partial {\bf w}_2}
= \frac{\partial \mathcal{L}}{\partial {\bf y}} \frac{\partial {\bf y}}{\partial {\bf u}_2} \frac{\partial {\bf u}_2}{\partial {\bf w}_2}\]</div>
<p>の右辺の3つの偏微分をそれぞれ計算してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># dL / dy
g_Ly = -2 / 1 * (t - y)

# dy / du_2
g_yu2 = y * (1 - y)

# du_2 / dw_2
g_u2w2 = h1
</pre></div>
</div>
</div>
<p>これらを掛け合わせれば，求めたかったパラメータ <span class="math notranslate nohighlight">\({\bf w}_2\)</span>
についての勾配を得ることができます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># dL / dw_2: 求めたい勾配
g_Lw2 = g_Ly * g_yu2 * g_u2w2

print(g_Lw_2)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[-1.72104507e+00 -1.43112111e-06]
</pre></div></div>
</div>
<p>勾配が求まりました．これが
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {\bf w}_2}\)</span>
の値です．これを学習率でスケールさせたものを使えば，パラメータ
<span class="math notranslate nohighlight">\({\bf w}_2\)</span>
を更新することができます．更新式は，具体的には以下のようになります．</p>
<div class="math notranslate nohighlight">
\[{\bf w}_2 \leftarrow {\bf w}_2 - \eta \frac{\partial \mathcal{L}}{\partial {\bf w}_2}\]</div>
<p><span class="math notranslate nohighlight">\(\eta\)</span>
が学習率と呼ばれるもので，これが大きすぎると，繰り返しパラメータ更新を行っていく中で目的関数の値が振動したり，発散したりしてしまいます．小さすぎると，収束に時間がかかってしまいます．そのため，この学習率を適切に決定することがニューラルネットワークの学習においては非常に重要となります．</p>
<p>次に，<span class="math notranslate nohighlight">\({\bf w}_1\)</span>
についての勾配も求めてみましょう．ここで，これは，以下のように計算できるはずです．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial {\bf w}_1}
&amp;= \frac{\partial \mathcal{L}}{\partial {\bf y}} \frac{\partial {\bf y}}{\partial {\bf w}_1} \\
&amp;=
\frac{\partial \mathcal{L}}{\partial {\bf y}}
\frac{\partial {\bf y}}{\partial {\bf u}_2}
\frac{\partial {\bf u}_2}{\partial {\bf w}_1} \\
&amp;=
\frac{\partial \mathcal{L}}{\partial {\bf y}}
\frac{\partial {\bf y}}{\partial {\bf u}_2}
\frac{\partial {\bf u}_2}{\partial {\bf h}_1}
\frac{\partial {\bf h}_1}{\partial {\bf w}_1} \\
&amp;=
\frac{\partial \mathcal{L}}{\partial {\bf y}}
\frac{\partial {\bf y}}{\partial {\bf u}_2}
\frac{\partial {\bf u}_2}{\partial {\bf h}_1}
\frac{\partial {\bf h}_1}{\partial {\bf u}_1}
\frac{\partial {\bf u}_1}{\partial {\bf w}_1}
\end{aligned}\end{split}\]</div>
<p>この5つの偏微分のうち初めの1つはすでに求めました．残りの4つは，それぞれ，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{\partial {\bf y}}{\partial {\bf u}_2}
&amp;= {\bf y}(1 - {\bf y}) \\
\frac{\partial {\bf u}_2}{\partial {\bf h}_1}
&amp;= {\bf w}_2 \\
\frac{\partial {\bf h}_1}{\partial {\bf u}_1}
&amp;= {\bf h}_1(1 - {\bf h}_1) \\
\frac{\partial {\bf u}_1}{\partial {\bf w}_1}
&amp;= {\bf x}
\end{aligned}\end{split}\]</div>
<p>と計算することができます．では，さっそく実際にNumPyを用いて計算を実行してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>g_yu2 = y * (1 - y)
g_u2h1 = w2
g_h1u1 = h1 * (1 - h1)
g_u1w1 = x

# 上から du1 / dw1 の直前までを一旦計算
g_Lu1 = g_Ly * g_yu_2 * g_u_2h_1 * g_h_1u_1

# g_u1w1は (3,) というshapeなので，g_u1w1[None]として(1, 3)に変形
g_u1w1 = g_u1w1[None]

# dL / dw_1: 求めたい勾配
g_Lw1 = g_Lu1.T.dot(g_u1w1)

print(g_Lw1)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[-1.72463398e-04 -2.58695098e-04 -8.62316992e-05]
 [-5.72447970e-06 -8.58671954e-06 -2.86223985e-06]]
</pre></div></div>
</div>
<p>計算ができました．これが
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {\bf w}_1}\)</span>
の値です．これを用いて，<span class="math notranslate nohighlight">\({\bf w}_2\)</span>
と同様に以下のような更新式でパラメータ <span class="math notranslate nohighlight">\({\bf w}_1\)</span>
の更新をすることができます．</p>
<div class="math notranslate nohighlight">
\[{\bf w}_1 \leftarrow {\bf w}_1 - \eta \frac{\partial \mathcal{L}}{\partial {\bf w}_1}\]</div>
</div>
</div>
<div class="section" id="誤差逆伝播法（バックプロパゲーション）">
<h2>3.4. 誤差逆伝播法（バックプロパゲーション）<a class="headerlink" href="#誤差逆伝播法（バックプロパゲーション）" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ここまでで，勾配を手計算により導出して実際に数値計算を行うということを体験しました．しかし，これは層数が増えてくれば増えてくるほど，一つ一つのパラメータに関する勾配を手計算で求めることは大変になっていきます．</p>
<p>しかし，ここまでで何度か行ってきた合成関数の偏微分を連鎖率によって複数の偏微分の積の形に変形する行程は自動的に計算できそうです．</p>
<p>そこで，ここまでの説明で用いていた3層全結合型ニューラルネットワークをもう一度見直して，更新量がどのように計算されていたか動画で確かめてみましょう．</p>
<div class="figure" id="id21">
<img alt="backpropagation" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/backpropagation.gif" />
<p class="caption"><span class="caption-text">backpropagation</span></p>
</div>
<p>ここでは，目的関数の出力を <span class="math notranslate nohighlight">\(l = \mathcal{L}({\bf y}, {\bf t})\)</span>
としています．この図の丸いノードは変数を表し，四角いノードは関数を表しています．上で説明したように，このニューラルネットワーク全体を
<span class="math notranslate nohighlight">\(f\)</span> と表す場合，各層間の線形変換を <span class="math notranslate nohighlight">\(f_1\)</span>, <span class="math notranslate nohighlight">\(f_2\)</span>
と表すと，<span class="math notranslate nohighlight">\(f\)</span>
は以下のような関数合成で書くことができることになります．</p>
<div class="math notranslate nohighlight">
\[f = a_2 \circ f_2 \circ a_1 \circ f_1\]</div>
<p>ニューラルネットワークの場合，<strong>この全体を構成する一つ一つの関数が全て，その入力について微分可能である必要があります</strong>．そして，事前にそれらの導関数を計算しておけば，新しい入力が渡されたとき，それを導関数に代入するだけですぐに入力に関する勾配を計算することができます．例えば，図中の<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {\bf y}}\)</span>
や，<span class="math notranslate nohighlight">\(\frac{\partial a_2}{\partial {\bf u}_2}\)</span>
などは，<span class="math notranslate nohighlight">\(\mathcal{L}\)</span> や <span class="math notranslate nohighlight">\(a_2\)</span>
に入力が与えられた時点で計算することができます．</p>
<p>また，こうしてみるとニューラルネットワークは<strong>微分可能な関数によって構成される計算グラフ</strong>であるということがよく分かります．この計算グラフ上を入力データ
<span class="math notranslate nohighlight">\({\bf x}\)</span>
が伝播していくこと（<strong>上の図で青色の矢印で表された左から右方向への計算</strong>）を，<strong>順伝播（forward
computation）</strong> といいます．</p>
<p>順伝播が進行していくと，すでに通過した関数に対してはその入力に関する勾配の値が求まっていきます．そして，順伝播計算の終着点にあるのは目的関数の計算です．このとき，計算グラフの途中にでてきた2個めの線形変換
<span class="math notranslate nohighlight">\(f_2\)</span> に着目すると，この関数が持つパラメータ <span class="math notranslate nohighlight">\({\bf w}_2\)</span>
についての目的関数 <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> の勾配は，</p>
<p>「<strong>この関数より先にあるすべての関数の勾配をかけ合わせたもの</strong>＝<span class="math notranslate nohighlight">\(\frac{\partial a_2}{\partial {\bf u}_2} \frac{\partial \mathcal{L}}{\partial {\bf y}}\)</span>
に，<strong>この関数に対するパラメータについての勾配</strong>＝<span class="math notranslate nohighlight">\(\frac{\partial f_2}{\partial {\bf w}_2}\)</span>
を掛け合わせれば，求めることができます．上の図中にも式で示されている，以下の計算のことです．</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}}{\partial {\bf w}_2}
=
\frac{\partial f_2}{\partial {\bf w}_2}
\frac{\partial a_2}{\partial {\bf u}_2}
\frac{\partial \mathcal{L}}{\partial {\bf y}}\]</div>
<p>つまり，<strong>パラメータ :math:`{bf w}_2` の更新量＝目的関数に対する
:math:`{bf w}_2` についての勾配</strong>
は，この図が示すように，<strong>出力側から順に，逆向きに各関数の入力に関する勾配を計算していき，それを掛け合わせていく</strong>ことで計算できるわけです．</p>
<p>このように，微分の連鎖率の仕組みを用いてニューラルネットワーク <span class="math notranslate nohighlight">\(f\)</span>
の微分を計算し，パラメータの更新量を求めることを実際のデータを用いて繰り返し行ってパラメータ最適化を行うアルゴリズムを，<strong>誤差逆伝播法（backpropagation）</strong>
と呼びます．</p>
</div>
<div class="section" id="勾配消失">
<h2>3.5. 勾配消失<a class="headerlink" href="#勾配消失" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>活性化関数について初めに触れた際，シグモイド関数には勾配消失という現象が起きやすくなるという問題があり，現在はあまり使われていないと説明をしました．その理由についてもう少し詳しく見ていきましょう．</p>
<p>上で既に計算した，シグモイド関数の導関数を思い出してみます．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
f\left( u\right) &amp;=\dfrac {1}{1+e^{-u}} \\
f'\left( u\right) &amp;= f\left( u\right) \left( 1-f\left( u\right) \right)
\end{aligned}\end{split}\]</div>
<p>さて，この導関数を入力変数に関してプロットしてみると，下記のようになります．</p>
<div class="figure" id="id22">
<img alt="09.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/09.png" />
<p class="caption"><span class="caption-text">09.png</span></p>
</div>
<p>この図を見ると，<strong>シグモイド関数の勾配の最大値は0.25</strong>
であることに気づきます．それに対し，前述のReLU関数の場合，入力変数が0より大きければ常に勾配は1となるので，その最大値は1でした．</p>
<p>これにどのような差があるかを考えてみます．各パラメータの更新量を求めるには，そのパラメータが使われた関数よりも<strong>先のすべての関数の勾配をかけ合わせたもの</strong>
に，さらに自らの勾配を掛け合わせる必要がありました．しかし，活性化関数にシグモイド関数を用いた場合，線形変換が計算グラフ中に現れるたびにそれに続いて現れる活性化関数の勾配が，どんなに大きくても0.25であるため，活性化関数が登場するたびに目的関数の勾配は少なくとも0.25倍ずつ小さくなっていくわけです．これは，層数が増えていけばいくほど最初の方のパラメータについての勾配は<span class="math notranslate nohighlight">\(0\)</span>に近づくという問題があります．別の言い方をすると，下の層のパラメータをどのように動かしても出力の値がほとんど変わらなくなることを意味します．</p>
<p>今回は3層のニューラルネットワークを用いて説明を行っていましたが，もし4層の場合，一番入力に近い線形変換のパラメータの勾配を求めようとすると，少なくとも目的関数の勾配が
<span class="math notranslate nohighlight">\(0.25 \times 0.25 = 0.0625\)</span>
倍されることとなります．結果として，ディープラーニングと呼ばれる分野でしばしば用いられているような，さらに多くの層を積み重ねたニューラルネットワークを訓練したい場合には，活性化関数としてシグモイド関数を使用すると，<strong>目的関数の勾配が入力に近いパラメータへほとんど伝わらなくなっていき</strong>，勾配降下法による最適化で用いるパラメータの更新量の値が極端に小さくなって，どんなに目的関数が大きなエラーを報告していようとも，ほとんどパラメータの値が更新されないということが起きます．これを<strong>勾配消失</strong>と呼び，長らく深い（十数層を超える）ニューラルネットワークの学習が困難であった一つの要因でした．</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Introduction_to_Chainer.html" class="btn btn-neutral float-right" title="4. Deep Learningフレームワークの基礎" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Introduction_to_ML_libs.html" class="btn btn-neutral" title="2. 機械学習ライブラリの基礎" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; キカガク

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>