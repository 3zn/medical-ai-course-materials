

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ja" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ja" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>3. ニューラルネットワーク &mdash; メディカルAIコース オンライン講義資料&lt;Paste&gt;  ドキュメント</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="4. Deep Learningフレームワークの基礎" href="Introduction_to_Chainer.html" />
    <link rel="prev" title="2. 機械学習ライブラリの基礎" href="Introduction_to_ML_libs.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAIコース オンライン講義資料<Paste>
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Basic_Math_for_ML.html">1. 機械学習に必要な数学の基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_ML_libs.html">2. 機械学習ライブラリの基礎</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">3. ニューラルネットワーク</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Step1.-モデルを決める">3.1. Step1. モデルを決める</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#線形変換">3.1.1. 線形変換</a></li>
<li class="toctree-l3"><a class="reference internal" href="#非線形変換">3.1.2. 非線形変換</a></li>
<li class="toctree-l3"><a class="reference internal" href="#数値例で計算の流れを確認">3.1.3. 数値例で計算の流れを確認</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Step2.-目的関数を決める">3.2. Step2. 目的関数を決める</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Step3.-パラメータを最適化する">3.3. Step3. パラメータを最適化する</a></li>
<li class="toctree-l2"><a class="reference internal" href="#勾配が消失する問題">3.4. 勾配が消失する問題</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">4. Deep Learningフレームワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. 実践編: MRI画像のセグメンテーション</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">6. 実践編: 血液の顕微鏡画像からの細胞検出</a></li>
<li class="toctree-l1"><a class="reference internal" href="Basenji.html">7. 実践編：ディープラーニングを使った配列解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html">8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAIコース オンライン講義資料<Paste></a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>3. ニューラルネットワーク</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Introduction_to_Neural_Network.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<p><a class="reference external" href="https://colab.research.google.com/github/japan-medical-ai/medical-ai-course-materials/blob/master/notebooks/Introduction_to_Neural_Network.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="ニューラルネットワーク">
<h1>3. ニューラルネットワーク<a class="headerlink" href="#ニューラルネットワーク" title="このヘッドラインへのパーマリンク">¶</a></h1>
<p>ここでは，ディープラーニングのベースとなっているニューラルネットワーク
(Neural Network)
について紹介していきます．このニューラルネットワークをベースに画像向けの
Convolutional Neural Network (CNN) や系列向けの Recurrent Neural Network
(RNN)
のように実問題における表現まで学習できるような工夫を追加したものを総称してディープラーニングと呼び，後半で扱っていきます．ディープラーニングの理論も魅力的ですが，まずはニューラルネットワークの数学をしっかりと理解しておくことが重要です．ニューラルネットワークの数学の理解には，重回帰分析の理解が大いに役立つはずです．またこれに加えて，非線形なモデリングや最適化の理論が新たに加わるため，楽しんで学んでいきましょう．ニューラルネットワークでもこれまでと同様に，モデルを決め，目的関数を決め，パラメータの更新を行います．</p>
<div class="section" id="Step1.-モデルを決める">
<h2>3.1. Step1. モデルを決める<a class="headerlink" href="#Step1.-モデルを決める" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>まずはニューラルネットワークの構造を数式ではなくグラフィカルに見てみましょう．</p>
<p><img alt="image1" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/01.png" /></p>
<p>ひとつひとつの丸い部分のことを<strong>ノード</strong>と呼び，そのノードの集まりを<strong>層</strong>と呼びます．そして，一番初めの層を<strong>入力層</strong>，最後の層を<strong>出力層</strong>，そしてその間を<strong>中間層</strong>もしくは<strong>隠れ層</strong>と呼びます．このモデルは入力層，中間層，出力層の３層の構造となっていますが，中間層の数を増やすことでさらに多層のニューラルネットワークとして定義することもできます．こちらでは各層間の全てのノードが結合されているため，<strong>全結合のニューラルネットワーク</strong>とも呼び，ニューラルネットワークの最も基礎的な構造です．</p>
<p>入力変数の扱い方はこれまでと同様ですが，出力変数の扱い方がこれまでと異なります．例えば，上図では白ワインと赤ワインを分類するような問題を例に挙げていますが，SVMなどの機械学習の場合，<span class="math notranslate nohighlight">\(y \in \{-1, 1\}\)</span>
のようにひとつの出力変数のとる値で属するカテゴリを指定していました．それに対して，ニューラルネットワークでは，白ワイン用の出力変数
<span class="math notranslate nohighlight">\(y_{1}\)</span> と赤ワイン用の出力変数 <span class="math notranslate nohighlight">\(y_{2}\)</span>
のカテゴリの数だけ出力の変数が準備されています．なぜこのような構造となっているのでしょうか．</p>
<p><img alt="image2" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/02.png" /></p>
<p>この答えの前に，最終的にニューラルネットワークではどのような出力の結果が得られるかを見てみましょう．例えば，年数が3年物でアルコール度数が14度，色合いが0.2，匂いが0.8のように定量評価できるワインがあるとしましょう．内部の計算は後述するとして，このようなデータをニューラルネットワークに与えると，白ワイン
<span class="math notranslate nohighlight">\(y_{1} = 0.15\)</span>, 赤ワイン <span class="math notranslate nohighlight">\(y_{2}= 0.85\)</span>
という値が得られるとしましょう．そして，最終的に最も大きな値である赤ワインを分類の結果として採用します．ここで出力の値に注目すると，合計すると1になっており，これは偶然ではなく一般的に成立するような機構になっています．</p>
<p>いま注目して欲しい点としては，SVMのような分類の手法では，白ワインか赤ワインかといったどのカテゴリに属するかしかわからなかったものの，ニューラルネットワークでは，どのクラスにどのくらいの確率で属すかまで知ることができます．前者の各カテゴリの境界線だけを求める手法を<strong>識別関数</strong>，後者の各カテゴリに属する確率まで求める手法を<strong>識別モデル</strong>と呼びます．また，これと並んで確率の知見を取り入れてデータの分布を直接求める手法を<strong>生成モデル</strong>と呼びますが，これは<strong>ベイズ統計</strong>をベースとしているため，さらに余裕が出てきてから取り組むことをお勧めします．</p>
<p>そして，もう一点確認したいこととして，ニューラルネットワークは回帰か分類どちらを扱う手法であるかです．答えは両方扱うことができます．ただし，最終的な出力は連続値となっていることがわかると思います．そのため，ニューラルネットワークは基本的には回帰の手法であり，この出力の値を使って分類も行えるということで，回帰も分類も両方の問題設定が扱えるわけです．</p>
<p>それでは，ここからニューラルネットワークの数学を本格的に扱っていきましょう．ニューラルネットワークでは数式もプログラミングも<strong>2層で1セット</strong>として扱うことを覚えておいてください．多いものだと100層を超えるため全体のモデルで考えると複雑になりすぎるため，2層ごとに分割してその連結で考えていきます．線形変換と非線形変換の2つの手順を流れに沿って紹介していきます．</p>
<div class="section" id="線形変換">
<h3>3.1.1. 線形変換<a class="headerlink" href="#線形変換" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p><img alt="image3" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/03.png" /></p>
<p>線形変換では，重回帰分析のときと同様に，重み (<span class="math notranslate nohighlight">\(w\)</span>) ×入力
(<span class="math notranslate nohighlight">\(h\)</span>) + バイアス (<span class="math notranslate nohighlight">\(b\)</span>)
の計算を行います．これからは，<span class="math notranslate nohighlight">\(h\)</span>
が文字としてよく登場しますが，隠れ層 (hidden layer) の頭文字である
<span class="math notranslate nohighlight">\(h\)</span> です．全てのノードについてそれぞれ考えると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
u_{11}&amp;=w_{11}h_{01}+w_{12}h_{02}+w_{13}h_{03}+w_{14}h_{04}+b_{1} \\
u_{12}&amp;=w_{21}h_{01}+w_{22}h_{02}+w_{23}h_{03}+w_{24}h_{04}+b_{2} \\
u_{13}&amp;=w_{31}h_{01}+w_{32}h_{02}+w_{33}h_{03}+w_{34}h_{04}+b_{2}
\end{aligned}\end{split}\]</div>
<p>のように計算を行います．これを行列でまとめると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\begin{bmatrix}
u_{11} \\
u_{12} \\
u_{13}
\end{bmatrix}&amp;=\begin{bmatrix}
w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14} \\
w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24} \\
w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34}
\end{bmatrix}\begin{bmatrix}
h_{01} \\
h_{02} \\
h_{03} \\
h_{04}
\end{bmatrix}+\begin{bmatrix}
b_{1} \\
b_{2} \\
b_{3}
\end{bmatrix}\\
\boldsymbol{u}_{1}&amp;=\boldsymbol{W}\boldsymbol{h}_{0}+\boldsymbol{b}
\end{aligned}\end{split}\]</div>
<p>のようにまとめることができます．本来は <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> や
<span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span>
にも添え字をつけるべきではありますが，記号が複雑になってくるため今回は省略します．厳密な定義としては
<span class="math notranslate nohighlight">\(\boldsymbol{u} = \boldsymbol{W}\boldsymbol{h}\)</span>
が線形変換であるが，重回帰分析でも紹介した通り，バイアス
<span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span> を <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>
に包含すればこのような式でも表現できるため，この
<span class="math notranslate nohighlight">\(\boldsymbol{u} = \boldsymbol{W}\boldsymbol{h} + \boldsymbol{b}\)</span>
の計算でも線形変換と呼んでいます．</p>
</div>
<div class="section" id="非線形変換">
<h3>3.1.2. 非線形変換<a class="headerlink" href="#非線形変換" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p><img alt="image4" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/04.png" /></p>
<p>これまでは，上図左側のように <span class="math notranslate nohighlight">\(y = wx + b\)</span>
で扱える範囲の手法のみ紹介していましたが，実データでは上図右のように入出力間の関係が直線（や超平面）では表現できない非線形な関係性の場合もあります．この場合，どのように対処すれば良いでしょうか．すでに紹介していたサポートベクターマシンでは<strong>カーネルトリック</strong>と呼ばれる方法を使いますが，数式を理解するのが一苦労です．ニューラルネットワークではもう少し考え方が簡単であり，内部に非線形な関数を各ノードが取り入れます．この組み合わせをうまく調整していくことにより，入出力間の非線形性に対応できると考えました．この非線形変換を行う関数を
<strong>活性化関数</strong> と呼びます．</p>
<p><img alt="image5" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/05.png" /></p>
<p>ニューラルネットワークでは上図に示す<strong>シグモイド関数</strong></p>
<div class="math notranslate nohighlight">
\[h = f(u) = \dfrac{1}{1+e^{-u}}\]</div>
<p>がよく用いられてきて，教科書でも良く紹介されています．しかし，最近ではあまり使われないものとなってきており，理由は後述しますが<strong>勾配が消失する</strong>といわれる問題であり，多層のニューラルネットワークにする際のボトルネックになってしまいました．そこで，最近では，よく登場するものとして，<strong>Relu関数</strong></p>
<p><img alt="image6" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/06.png" /></p>
<div class="math notranslate nohighlight">
\[h = f(u) = \max(0, u)\]</div>
<p>が使われることが多くなりました．この関数は入力が負の値の場合に出力が0で，正の値はそのまま出力するといった特性を持っています．このRelu関数はシグモイド関数で問題となっていた勾配が消失する問題に対する解決策のひとつであり，多層のニューラルネットワークを構成する際になくてはならない存在となりました．</p>
</div>
<div class="section" id="数値例で計算の流れを確認">
<h3>3.1.3. 数値例で計算の流れを確認<a class="headerlink" href="#数値例で計算の流れを確認" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p><img alt="image7" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/07.png" /></p>
<p>ここでは，これまでに数式が多くイメージが湧きにくかったため，上図の数値例で出力
<span class="math notranslate nohighlight">\(y\)</span>
までのニューラルネットワークの計算の流れを確認していき，具体的な計算手順のイメージを把握しておきましょう．今回は計算を簡略化するためバイアス
<span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span>
は省略しています．数値例として，<span class="math notranslate nohighlight">\(\boldsymbol{x}^T = \begin{bmatrix} 2 &amp; 3 &amp; 1 \end{bmatrix}\)</span>
が与えられた時の出力 <span class="math notranslate nohighlight">\(y\)</span> を計算してみましょう．</p>
<p>機械学習のアルゴリズムを考える場合に，肝となることが「どこから計算を始めるか」です．パラメータ
<span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>
が計算の過程で登場するのですが，この値が決まっていないと計算を始めることができません．重回帰分析では，定式化したまま微分の値が0となるように計算ができましたが，ニューラルネットワークではモデルの構造上，この微分の値が0となるようなパラメータを直接求めることは難しいという背景があります．その代わり，微分を利用した別の方法でうまくパラメータを最適化していけるため，その方法は後述します．そして，パラメータを直接求めることができない場合は，ひとまずランダムでも良いので，パラメータの初期値を仮で決めます．そこからどの程度悪かったのか・良かったのかを見て，良い方向に進むようにさらにパラメータを調整していくといったアプローチをとります．</p>
<p>パラメータの初期値に対して，線形変換を計算すると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
u_{11}&amp;=3\times 2+1\times 3+2\times 1=11\\
u_{12}&amp;=-2\times 2-3\times 3-1\times 1=-11
\end{aligned}\end{split}\]</div>
<p>となります．次に非線形変換ですが，活性化関数としてRelu関数を採用した場合，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
h_{11} &amp;= \max(0, 11) = 11 \\
h_{12} &amp;= \max(0, -11)  = 0
\end{aligned}\end{split}\]</div>
<p>となります．そして，<span class="math notranslate nohighlight">\(y\)</span> まで同様に計算すると，</p>
<div class="math notranslate nohighlight">
\[y = 3 \times 11 + 2 \times 0 = 33\]</div>
<p>となります．理論の話の際には難しく感じたかもしれませんが，実際の計算は足し算と掛け算が主であり，とても簡単であることがわかります．</p>
<p>さて，次項以降では残していたパラメータ更新のための原理について数式を使ってさらに理解を深めていきましょう．</p>
</div>
</div>
<div class="section" id="Step2.-目的関数を決める">
<h2>3.2. Step2. 目的関数を決める<a class="headerlink" href="#Step2.-目的関数を決める" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ニューラルネットワークでは，回帰と分類の両方に対応することができますが，問題設定によって目的関数は異なります．回帰では重回帰分析と同様に<strong>二乗誤差</strong>を使用します．Chainerのフレームワークで実装されているものは<strong>平均二乗誤差</strong>
(mean squared error)</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \dfrac{1}{N} \sum_{n=1}^{N}(t_{n} - y_{n})^{2}\]</div>
<p>であり，重回帰分析の総和に対して平均を採用しているだけであり，これまでの議論と本質的な部分が変わることはありません．つぎに，分類では<strong>クロスエントロピー</strong></p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = - \dfrac{1}{N}\sum_{n=1}^{N}t_{n}\log y_{n}\]</div>
<p>を目的関数として採用します．<span class="math notranslate nohighlight">\(log\)</span>
が登場したりと難しそうに見えるかもしれませんが，この意味をさらに知りたい方は<strong>情報理論</strong>の基礎的な部分を学ぶと理解することができます．ひとまず，分類における損失を定量評価できる関数だと考えて進めていくことにしましょう．</p>
<p>それでは先ほどの例題に対して，教師データ <span class="math notranslate nohighlight">\(t = 20\)</span>
の場合の計算を行ってみると，回帰の目的関数となる平均二乗誤差は</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \dfrac{1}{1} (20 - 33)^2 = 169\]</div>
<p>となります．</p>
</div>
<div class="section" id="Step3.-パラメータを最適化する">
<h2>3.3. Step3. パラメータを最適化する<a class="headerlink" href="#Step3.-パラメータを最適化する" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>それでは，重回帰分析とは異なる点であるニューラルネットワークにおけるパラメータの最適化について考え方を理解していきましょう．まず確認として，ニューラルネットワークにおけるパラメータは，各層における
<span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> と <span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span>
です．式で書かないと少しわかりにくいのですが，ニューラルネットワークでは各パラメータに対する微分を定式化したまま求めることは難しいのですが，各パラメータの値を決めた状態では各点での導関数の値は求めることができます．この各点での導関数の値は<strong>微分係数</strong>と呼びますが，この値がわかるということはパラメータを最適化するときには大きなメリットがあります．</p>
<p>それでは，図も使って微分係数を使ったパラメータの最適化について考えていきましょう．まず何も計算していない状況ですが，例えばこの点線のようなパラメータ
<span class="math notranslate nohighlight">\(w\)</span> に対する目的関数 <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
が存在するとしましょう．この例では二次関数になっていますが，実際にはもっと複雑な関数であることが多いです．実際には目的関数の形を把握することができないため，点線で示されている目的関数は「神のみぞ知る」といったものです．さて，この全体像も全く見えない中，どうやって計算機を使って最適なパラメータを見つけていくのでしょうか．</p>
<p><img alt="image8" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/13.png" /></p>
<p>ニューラルネットワークでは，はじめにランダムでも良いのでパラメータの初期値を決定すると数値例で紹介しました．例えば，ランダムに決定した結果
<span class="math notranslate nohighlight">\(w=3\)</span>
になったとしましょう．そうなると，後述する計算式で接線の傾きである微分係数が求まります．</p>
<p><img alt="image9" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/10.png" /></p>
<p>例えば，求めた結果，微分係数が 3
であったとしましょう．もちろんランダムに決めた初期値であるため，神のみぞ知る目的関数としては最適なパラメータとして決定できているわけではありません．それでは，神のみぞしる目的関数の全体像が今見えているみなさんであれば，つぎに移動するとすれば「＋方向」か「ー方向」のどちらを選ぶでしょうか．「ー方向」を選ぶ方が多いはずです．それでは，この処理を計算機で行わせるために，その戦略を考えると，傾きが正（＋）の値であったため，逆に負（ー）の方向に進むという方法はいかがでしょうか．さらに傾きの値も使って，「-1
× 3 進む」という風に考えてみます．ただし，「-1 × 3
進む」と逆に行き過ぎてしまうかもしれないため，「-1 × 0.5 × 3
進む」という風に進み具合を調整できる<strong>学習係数</strong> (learning rate)
を導入すると，「-1.5 進む」となり，次のパラメータの値へ移動します．</p>
<p><img alt="image10" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/11.png" /></p>
<p>そして，この点においても，傾きを求めてみると，例えば -1
であったとしましょう．右肩下がりであるため，負の値となります．そうすれば，傾きが負の値の時は，逆に正（＋）の方向に移動すればよさそうであるため，「-1
× 0.5 × -1 進む」としましょう．</p>
<p><img alt="image11" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/12.png" /></p>
<p>このように「-1 × 学習係数×
微分係数」と移動していきながら，傾きが0となれば終了とすれば，全体像が見えない中でも目的関数が最小となる点に到達することができます．厳密には，初期値に最も近い点での谷である<strong>局所最適解</strong>に落ちる方法であり，下図に示すように常に<strong>大域的最適解</strong>が求まるわけではありません．初期値の選び方に大きく依存するのですが，その初期値をうまく選ぶことは難しい問題であるため，基本的には局所最適解で良しとしています．複数の初期値で学習を行い，最終的にもっとも良かったものを選択するという方法もありますが，ニューラルネットワーク以降のディープラーニングでは学習に大きく時間がかかるため，この方法も現実的ではない場合が多いといえます．</p>
<p>微分係数を使って上述の手順で最適化を行っていく手法を<strong>最急降下法</strong>と呼びます．最適化の数学で基礎として出てくる有名な方法です．また，ニューラルネットワークも含めディープラーニングでは，すべてのサンプルに対する目的関数の計算ではなく，<strong>ミニバッチ</strong>と呼ばれるサンプルをランダムに複数のかたまりに分割した単位で最適化を行っていくことが多く，この場合には，<strong>確率的勾配降下法</strong>
(SGD: Stocastic Gradient Descent) と呼ばれており，Chainerの中では
<code class="docutils literal notranslate"><span class="pre">SGD</span></code> としてよく登場するため覚えておきましょう．</p>
<p><img alt="image12" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/14.png" /></p>
<p>さて，本題に戻ります．先ほどの最急降下法や確率的勾配降下法を利用するためには，微分係数が求まることが前提でした．そこで，数値例の問題設定に対して，実際に微分係数を求めていきましょう．文字の取り扱いを簡単かするため，下記のように
<span class="math notranslate nohighlight">\(w_{1}\)</span> と <span class="math notranslate nohighlight">\(w_{2}\)</span>
の２つの場合について紹介します．この2つを選択する理由としては，各層ごとに計算が異なってくるためです．</p>
<p><img alt="image13" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/08.png" /></p>
<p>それでは，<span class="math notranslate nohighlight">\(w_{1}\)</span>
での微分から考えましょう．はじめに，目的関数からターゲットとなっている
<span class="math notranslate nohighlight">\(w_{1}\)</span> までの計算を洗い出すと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L} &amp;= \dfrac{1}{N}\sum_{n=1}^{N}(t_n - y_{n})^{2} \\
y_{n} &amp;= w_{1}h_{11} + \cdots
\end{aligned}\end{split}\]</div>
<p>のようになります．ポイントとして，<span class="math notranslate nohighlight">\(w_{1}\)</span> に関係しない項は
<span class="math notranslate nohighlight">\(w_{1}\)</span>
で偏微分すると0になるため，「<span class="math notranslate nohighlight">\(\cdots\)</span>」のように書いています．微分の合成関数の時にも紹介しましたが，</p>
<div class="math notranslate nohighlight">
\[\dfrac {\partial \mathcal{L}}{\partial w_{1}}=\dfrac {\partial y_{n}}{\partial w_{1}}\dfrac {\partial \mathcal{L}}{\partial y_{n}}\]</div>
<p>のように関連する項目に分解していく方法を<strong>チェインルール</strong>と呼びます．そして，各項目の値を求めると，</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}  \begin{aligned}
  \dfrac {\partial y_{n}}{\partial w_{1}}&amp;=h_{11} = 11 \\
  \dfrac {\partial \mathcal{L}}{\partial y_{n}}&amp;=\dfrac {1}{N}\sum ^{N}_{n=1}2\left( t_{n}-y_{n}\right) \times \left( -1\right) \\
  &amp;=-\dfrac {2}{N}\sum_{n=1}^{N}\left( t_{n}-y_{n}\right) \\
  &amp;= -2 \times (20 - 33) = 26
  \end{aligned}\end{split}\\となり，最終的な微分係数は\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}  \dfrac {\partial \mathcal{L}}{\partial w_{1}}=\dfrac {\partial y_{n}}{\partial w_{1}}\dfrac {\partial \mathcal{L}}{\partial y_{n}} = 11 \times 26 =286\\となりました．\end{aligned}\end{align} \]</div>
<p>次に，<span class="math notranslate nohighlight">\(w_{2}\)</span>
についても同様に考えていきましょう．目的関数からターゲットとなっている
<span class="math notranslate nohighlight">\(w_{2}\)</span> までの計算を洗い出すと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathcal{L} &amp;= \dfrac{1}{N}\sum_{n=1}^{N}(t_n - y_{n})^{2} \\
y_{n} &amp;= w_{1}h_{11} + \cdots \\
&amp;= w_{1} f(u_{11})  + \cdots \\
u_{11}&amp;=w_{2}x_{1} + \cdots
\end{aligned}\end{split}\]</div>
<p>となります．この考え方には慣れが必要であるため，ひとつひとつ順を追っていく練習を行いましょう．これをもとにチェインルールを活用しながら
<span class="math notranslate nohighlight">\(w_{2}\)</span> での微分係数を求めていくと，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac {\partial \mathcal{L}}{\partial w_{2}}&amp;=\dfrac {\partial y_{n}}{\partial w_{2}}\dfrac {\partial L}{\partial y_{n}}\\
&amp;=\dfrac {\partial u_{11}}{\partial w_{2}}\dfrac {\partial y_{n}}{\partial u_{11}}\dfrac {\partial \mathcal{L}}{\partial y_{n}}\\
\dfrac {\partial u_{11}}{\partial w_2}&amp;=x_{1}=2\\
\dfrac {\partial y_{n}}{\partial u_{11}}&amp;=w_{1}f'\left( u_{11}\right) \\
\dfrac {\partial \mathcal{L}}{\partial y_{n}}&amp;=-\dfrac {2}{N}\sum ^{N}_{n=1}\left( t_{n}-y_{n}\right) =26
\end{aligned}\end{split}\]</div>
<p>となります．ここで，<span class="math notranslate nohighlight">\(f'(u_{11})\)</span>
のところで，活性化関数であるRelu関数の微分が必要となり，Relu関数の微分は</p>
<div class="math notranslate nohighlight">
\[\begin{split}f'\left( u\right) =
\begin{cases}
1 \quad u &gt;0\\
0 \quad u\leq 0\end{cases}\end{split}\]</div>
<p>のように入力される <span class="math notranslate nohighlight">\(u\)</span>
の値の符号によって変わっています．これが全体として微分を定式化できない理由であり，値によって場合分けというものが扱いにくいのですが，そのパラメータの1点における微分係数であれば問題なく，</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}  \begin{aligned}
  \dfrac {\partial y_{n}}{\partial u_{11}}&amp;=w_{1}f'\left( u_{11}\right) =3\times 1=3\\
  \end{aligned}\end{split}\\のように計算できます最終的に，\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac {\partial \mathcal{L}}{\partial w_{2}}&amp;=\dfrac {\partial u_{11}}{\partial w_{2}}\dfrac {\partial y_{n}}{\partial u_{11}}\dfrac {\partial \mathcal{L}}{\partial y_{n}}\\
&amp;= 2 \times 3 \times 26 = 156
\end{aligned}\end{split}\]</div>
<p>と求まります．たとえば，学習係数 <span class="math notranslate nohighlight">\(\rho=0.01\)</span>
として最急降下法によりパラメータを更新すると，更新後の
<span class="math notranslate nohighlight">\(w_{1}^{(1)}\)</span> は更新前の <span class="math notranslate nohighlight">\(w_{1}^{(0)}\)</span> を使って，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
w_{1}^{(1)} &amp;= w_{1}^{(0)} - \rho\dfrac{\partial \mathcal{L}}{\partial w_{1}} \\
&amp;= 3 - 0.01 \times 286 \\
&amp;= 0.14
\end{aligned}\end{split}\]</div>
<p>となります．また，更新後の <span class="math notranslate nohighlight">\(w_{2}^{(1)}\)</span> は更新前の
<span class="math notranslate nohighlight">\(w_{2}^{(0)}\)</span> を使って，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
w_{2}^{(1)} &amp;= w_{2}^{(0)} - \rho\dfrac{\partial \mathcal{L}}{\partial w_{2}} \\
&amp;= 3 - 0.01 \times 156 \\
&amp;= 1.44
\end{aligned}\end{split}\]</div>
<p>となります．このように，パラメータの更新を行い，また，更新後のパラメータで予測値の計算および目的関数の計算を行い（<strong>順伝播</strong>），微分係数を求めてパラメータの更新（<strong>逆伝播</strong>）と繰り返していきます．重回帰分析では最初から最適なパラメータが求まっていましたが，ニューラルネットワークではパラメータの更新を行うために，順伝播と逆伝播を何度も繰り返すことが特徴的です．逆伝播における微分係数を求める効率的な方法は<strong>誤差逆伝播法</strong>として体系化されていますが，まずは前述した逆伝播の計算をしっかりと理解で充分であるため，余裕が出てきたタイミングで参考書を見ながら勉強してみてください．</p>
</div>
<div class="section" id="勾配が消失する問題">
<h2>3.4. 勾配が消失する問題<a class="headerlink" href="#勾配が消失する問題" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ニューラルネットワーク最後のパートとして，活性化関数のパートで，標準的であった活性化関数であるシグモイド関数が使われなくなっていると紹介していましたが，その理由について考えていきます．関数の形を見ると，直感的にはシグモイド関数の方が対応力が高そうで，Relu関数は退屈な関数に見えます．パラメータ更新のところで実際に計算したところ，<span class="math notranslate nohighlight">\(w_{2}\)</span>
の微分を計算する際に</p>
<div class="math notranslate nohighlight">
\[\dfrac {\partial y_{n}}{\partial u_{11}}=w_{1}f'\left( u_{11}\right)\]</div>
<p>の計算が登場し，特に <span class="math notranslate nohighlight">\(f'(u_{11})\)</span>
のように活性化関数に関する微分が登場していることがわかると思います．</p>
<p>まずRelu関数と比較するために，シグモイド関数の微分を計算してみると，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
f\left( u\right) &amp;=\dfrac {1}{1+e^{-u}}=\left( 1+e^{-u}\right) ^{-1}\\
f'\left( u\right) &amp;=-1\times \left( 1+e^{-u}\right) ^{-2}\times \left( -e^{-u}\right) \\
&amp;=\dfrac {e^{-u}}{\left( 1+e^{-u}\right) ^{2}}\\
&amp;=\dfrac {1+e^{-u}-1}{\left( 1+e^{-u}\right) ^{2}}\\
&amp;=\dfrac {1+e^{-u}}{\left( 1+e^{-u}\right) ^{2}}-\dfrac {1}{\left( 1+e^{-u}\right) ^{2}} \\
&amp;=\dfrac {1}{1+e^{-u}}-\left( \dfrac {1}{1+e^{-u}}\right) ^{2}\\
&amp;=f\left( u\right) -\left( f\left( u\right) \right) ^{2}\\
&amp;=f\left( u\right) \left( 1-f\left( u\right) \right)
\end{aligned}\end{split}\]</div>
<p>となります．ここで，微分の公式として，</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
(x^{-1}) ' &amp;= -x^{-2} \\
(e^{ax})' &amp;= ae^{ax}
\end{aligned}\end{split}\]</div>
<p>の 2
つも利用しており，合成関数も使っています．そして，このシグモイド関数を微分した関数のグラフは下記のようになります．</p>
<p><img alt="image14" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/3/09.png" /></p>
<p>このように，シグモイド関数の微分を計算してみると，<strong>シグモイド関数の微分係数の最大値が0.25</strong>であることがわかります．それに対し，前述していたRelu関数の微分係数の最大値は1でした．</p>
<p>これにどのような差があるかというと，各パラメータの微分係数を求めた際に，シグモイド関数の場合，大きくても0.25倍となり，微分係数（各パラメータの微分係数をまとめて<strong>勾配</strong>ベクトルと呼びます）の値が小さくなることがわかります．今回は3層であったため，気にならない程度ですが，もし4層の場合，一番後ろの層のパラメータの微分係数を求める場合，最低でも
<span class="math notranslate nohighlight">\(0.25 \times 0.25 = 0.0625\)</span>
となり，さらに勾配の値が小さくなります．つまり，ディープラーニングのように多層のニューラルネットワークの場合にシグモイド関数を使用すると，後ろの層に行くほど勾配情報が極端に小さくなっていき，最急降下法や確率的勾配降下法のような方法では，後ろの層ではパラメータの更新が実質行えないような状況となってしまいます．これが<strong>勾配が消失する</strong>という問題の正体です．そして，シグモイド関数に対し，Relu関数では微分係数の最大値が1であるため，層の数が増えても勾配の値が小さくなっていくことがなく，勾配が消失する問題に対処することができるわけです．ただし，シグモイド関数のような形状の活性化関数を使用したい状況もあり，その時には，形が近しく勾配の消失に関する改善がなされている<strong>ハイパボリックタンジェント</strong>を使用するため，こちらも余裕がでれば関数の式と微分の計算，微分係数の最大値をおさえておきましょう．</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Introduction_to_Chainer.html" class="btn btn-neutral float-right" title="4. Deep Learningフレームワークの基礎" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Introduction_to_ML_libs.html" class="btn btn-neutral" title="2. 機械学習ライブラリの基礎" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; キカガク

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>