

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ja" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ja" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>3. 実践編: 血液の顕微鏡画像からの細胞検出 &mdash; メディカルAI学会認定資格向け学習資料  ドキュメント</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="4. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析" href="Sequential-Data-Analysis-with-Deep-Learning.html" />
    <link rel="prev" title="2. 実践編: CT/MRI画像のセグメンテーション" href="Image_Segmentation_with_Chainer.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAI学会認定資格向け学習資料
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Chainer_Beginner's_Hands_on.html">1. Deep Learningフレームワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation_with_Chainer.html">2. 実践編: CT/MRI画像のセグメンテーション</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">3. 実践編: 血液の顕微鏡画像からの細胞検出</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#環境構築">3.1. 環境構築</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Object-Detectionについて">3.2. Object Detectionについて</a></li>
<li class="toctree-l2"><a class="reference internal" href="#データセットの準備">3.3. データセットの準備</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#データセットダウンロード">3.3.1. データセットダウンロード</a></li>
<li class="toctree-l3"><a class="reference internal" href="#データセットオブジェクト作成">3.3.2. データセットオブジェクト作成</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Single-Shot-Multibox-Detector-(SSD)">3.4. Single Shot Multibox Detector (SSD)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#モデルの定義">3.5. モデルの定義</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Sequential-Data-Analysis-with-Deep-Learning.html">4. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential-Data-Analysis-with-Deep-Learning.html#目次">5. 目次</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAI学会認定資格向け学習資料</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>3. 実践編: 血液の顕微鏡画像からの細胞検出</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Blood_Cell_Detection.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="実践編:-血液の顕微鏡画像からの細胞検出">
<h1>3. 実践編: 血液の顕微鏡画像からの細胞検出<a class="headerlink" href="#実践編:-血液の顕微鏡画像からの細胞検出" title="このヘッドラインへのパーマリンク">¶</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/mitmul/medical-ai-course-materials/blob/master/docs/source/notebooks/Blood_Cell_Detection.ipynb">Open with
Colab</a></p>
<p>ここでは血液細胞の検出タスクに取り組みます。人の血液の顕微鏡画像が与えられたときに、</p>
<ul class="simple">
<li>赤血球（Red Blood Cell; RBC）</li>
<li>白血球（White Blood Cell; WBC）</li>
<li>血小板（Platelet）</li>
</ul>
<p>の3種の細胞について、それぞれ<strong>何がどの位置にあるか</strong>を個別に認識します。これによって、与えられた画像内にそれらの細胞が何個づつあるか、また、どういう位置にあるか、ということが分かります。</p>
<p>このようなタスクを一般に<strong>物体検出（object
detection）</strong>と呼び、画像を入力として、対象の物体（ここでは例えば、上の3種の細胞）ごとに、別々に</p>
<ol class="arabic simple">
<li>四角い矩形（bounding boxと呼ばれる）</li>
<li>「内側にある物体が何か」＝クラスラベル</li>
</ol>
<p>を出力していくことが目的となります。ただし、<strong>画像中にいくつの物体が含まれるかは事前に分からない</strong>ため、任意個（または十分な数）の物体の<strong>bounding
boxとクラスラベルの予測値の組</strong>を出力できるような手法である必要があります。</p>
<p>bounding
box（bboxとよく略される）は、<code class="docutils literal notranslate"><span class="pre">[矩形の左上のy座標,</span> <span class="pre">矩形の左上のx座標,</span> <span class="pre">矩形の右下のy座標,</span> <span class="pre">矩形の右下のx座標]</span></code>のような形式で定義されることが多く、クラスは物体の種類ごとに割り振られたIDで表されることが多いようです。例えば、RBCは0、WBCは1、Plateletは2とします。</p>
<p><img alt="image0" src="https://github.com/mitmul/medical-ai-course-materials/blob/master/notebooks/images/speed-accuracy-tradeoffs.png?raw=1" /> 血液の顕微鏡画像からRBC, WBC, Plateletを検出している例</p>
<div class="section" id="環境構築">
<h2>3.1. 環境構築<a class="headerlink" href="#環境構築" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>まず環境構築のためColab上で以下のセルを実行してChainer, CuPy, ChainerCV,
matplotlibといったPythonパッケージのインストールを済ませます。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># !curl https://colab.chainer.org/install | sh -
# !pip install chainercv matplotlib
</pre></div>
</div>
</div>
<p>環境のセットアップが成功したことを以下のセルを実行して確認しましょう。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!python -c &#39;import chainer; chainer.print_runtime_info()&#39;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Platform: Linux-4.4.0-116-generic-x86_64-with-debian-stretch-sid
Chainer: 5.0.0rc1
NumPy: 1.15.2
CuPy:
  CuPy Version          : 5.0.0rc1
  CUDA Root             : /usr/local/cuda
  CUDA Build Version    : 9020
  CUDA Driver Version   : 9020
  CUDA Runtime Version  : 9020
  cuDNN Build Version   : 7201
  cuDNN Version         : 7201
  NCCL Build Version    : 2213
</pre></div></div>
</div>
</div>
<div class="section" id="Object-Detectionについて">
<h2>3.2. Object Detectionについて<a class="headerlink" href="#Object-Detectionについて" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>物体検出（object detection）は、Computer
Visionの分野で現在も活発に研究が行われているタスクの一つで、自動運転分野やロボティクスなど幅広い領域で重要な役割を果たす技術です。Semantic
Segmentationと違い、物体の形（輪郭）までは認識しませんが、物の種類と位置を、物体ごとに個別に出力します。「物の種類」をクラスと呼ぶとき、そのクラスに属する個別の物体をインスタンスと呼ぶことができます。犬が2匹写っている写真があるとすれば、それは「犬」というクラスに属する個別の犬インスタンスが2個あるという状態だと言えます。つまり、Semantic
Segmentationではインスタンスごとに区別される領域が出力されるわけではなかった一方で、物体検出の出力はインスタンスごとになるという違いがあります。</p>
<p>ニューラルネットワークを用いた物体検出手法は、<a class="reference external" href="https://arxiv.org/abs/1311.2524">R-CNN</a>という2014年に発表された手法を皮切りに、様々な改善手法が提案されています。まず一つ、重要なのは<a class="reference external" href="https://arxiv.org/abs/1311.2524">R-CNN</a>,
<a class="reference external" href="https://arxiv.org/abs/1504.08083">Fast R-CNN</a>, そして<a class="reference external" href="https://arxiv.org/abs/1506.01497">Faster
R-CNN</a>という、同一の著者らを中心として提案されているConvolutional
Neural Networks
(CNN)をベースとした物体検出手法の発展流れです。現在two-streamタイプと呼ばれているものはほぼこのFaster
R-CNNをベースとしたものとなります。</p>
</div>
<div class="section" id="データセットの準備">
<h2>3.3. データセットの準備<a class="headerlink" href="#データセットの準備" title="このヘッドラインへのパーマリンク">¶</a></h2>
<div class="section" id="データセットダウンロード">
<h3>3.3.1. データセットダウンロード<a class="headerlink" href="#データセットダウンロード" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>今回は<a class="reference external" href="https://github.com/Shenggan/BCCD_Dataset">BCCD
Dataset</a>という血液の顕微鏡画像と、画像それぞれに対してRBC,
WBC, Plateletの3つの物体に対するbounding
boxのアノテーションが用意された小規模なデータセットを用います。このデータセットはGithubリポジトリで配布されていますので、以下のセルを実行してデータセットをダウンロードしましょう。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>!if [ ! -d BCCD_Dataset ]; then git clone https://github.com/Shenggan/BCCD_Dataset.git; fi
</pre></div>
</div>
</div>
<p>データセットは以下のうようなファイル構成で配布されています。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>BCCD
|-- Annotations
|   |
|   `-- BloodImage_00XYZ.xml (364 items)
|
|-- ImageSets
|   |
|   `-- Main
|       |
|       |-- test.txt
|       |-- train.txt
|       |-- trainval.txt
|       `-- val.txt
|
`-- JPEGImages
  |
   `-- BloodImage_00XYZ.jpg (364 items)
</pre></div>
</div>
<p>他にもディレクトリがありますが、今回用いるのは上記のファイルだけですので、こちらのみに着目しましょう。</p>
<ul class="simple">
<li><strong>Annotationsディレクトリ：</strong>VOC
formatと呼ばれる形式で細胞画像それぞれに対して<strong>どの位置に何があるか</strong>という、一般的に画像からの物体検出タスクで必要となるラベルの情報が格納されています。</li>
<li><strong>ImageSetsディレクトリ：</strong>学習用データセット・検証用データセット・テスト用データセットのそれぞれに用いる画像のリストが記されたテキストファイルが入っています。これを使ってデータセットの分割を行います。</li>
<li><strong>JPEGImagesディレクトリ：</strong>実際に学習や検証・テストに用いる画像データが入っています。</li>
</ul>
</div>
<div class="section" id="データセットオブジェクト作成">
<h3>3.3.2. データセットオブジェクト作成<a class="headerlink" href="#データセットオブジェクト作成" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ChainerCVにはPascal
VOCデータセットを簡単に読み込むための便利なクラスが用意されています。これを継承し、<code class="docutils literal notranslate"><span class="pre">_get_annotations</span></code>メソッドをオーバーライドして今回使用するデータセットに合わせて変更を加えます。変更が必要な行は１行だけです。<a class="reference external" href="https://github.com/chainer/chainercv/blob/v0.10.0/chainercv/datasets/voc/voc_bbox_dataset.py#L90-L115">こちら</a>から該当するコードをコピーしてきて、以下の変更を行い、<code class="docutils literal notranslate"><span class="pre">BCCDDataset</span></code>クラスを定義してみましょう。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">voc_utils</span><span class="o">.</span><span class="n">voc_bbox_label_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
<span class="o">+</span> <span class="n">label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bccd_labels</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
</pre></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import os
import xml.etree.ElementTree as ET

import numpy as np

from chainercv.datasets import VOCBboxDataset


bccd_labels = (&#39;rbc&#39;, &#39;wbc&#39;, &#39;platelets&#39;)


class BCCDDataset(VOCBboxDataset):

    def _get_annotations(self, i):
        id_ = self.ids[i]

        # Pascal VOC形式のアノテーションデータは、XML形式で配布されています
        anno = ET.parse(
            os.path.join(self.data_dir, &#39;Annotations&#39;, id_ + &#39;.xml&#39;))

        bbox = []
        label = []
        difficult = []
        for obj in anno.findall(&#39;object&#39;):
            bndbox_anno = obj.find(&#39;bndbox&#39;)

            # バウンディングボックスの
            # subtract 1 to make pixel indexes 0-based
            bbox.append([
                int(bndbox_anno.find(tag).text) - 1
                for tag in (&#39;ymin&#39;, &#39;xmin&#39;, &#39;ymax&#39;, &#39;xmax&#39;)])
            name = obj.find(&#39;name&#39;).text.lower().strip()
            label.append(bccd_labels.index(name))
        bbox = np.stack(bbox).astype(np.float32)
        label = np.stack(label).astype(np.int32)
        # When `use_difficult==False`, all elements in `difficult` are False.
        difficult = np.array(difficult, dtype=np.bool)
        return bbox, label, difficult
</pre></div>
</div>
</div>
<p>これで、今回用いるデータセットクラスを定義することができました。では、これを用いて学習・検証・テスト用のデータセットオブジェクトを作成してみましょう。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>train_dataset = BCCDDataset(&#39;BCCD_Dataset/BCCD&#39;, &#39;train&#39;)
valid_dataset = BCCDDataset(&#39;BCCD_Dataset/BCCD&#39;, &#39;val&#39;)
test_dataset = BCCDDataset(&#39;BCCD_Dataset/BCCD&#39;, &#39;test&#39;)
</pre></div>
</div>
</div>
<p>ではデータを可視化して確認してみましょう。trainデータセットから一つデータと対応するラベル情報（bounding
boxとクラスの組）を取り出し、ChainerCVが用意している可視化用の便利な関数を使って、画像の上にbounding
boxおよび対応するクラスラベルを表示してみます。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>%matplotlib inline
from chainercv.visualizations import vis_bbox

img, bbox, label = train_dataset[0]
ax = vis_bbox(img, bbox, label, label_names=bccd_labels)
ax.set_axis_off()
ax.figure.tight_layout()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Blood_Cell_Detection_14_0.png" src="../_images/notebooks_Blood_Cell_Detection_14_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="Single-Shot-Multibox-Detector-(SSD)">
<h2>3.4. Single Shot Multibox Detector (SSD)<a class="headerlink" href="#Single-Shot-Multibox-Detector-(SSD)" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>データの準備が完了したので、今回訓練するモデルについて説明を行います。今回は、<a class="reference external" href="https://arxiv.org/abs/1512.02325">Single
Shot MultiBox Detector
(SSD)</a>というモデルを使ってみます。</p>
<p>SSDは、single
stageタイプと呼ばれる物体検出手法の一種で、<a class="reference external" href="https://arxiv.org/abs/1506.02640">YOLO</a>などもその一種とされます。これに対し、two
stageタイプと呼ばれる手法としては<a class="reference external" href="https://arxiv.org/abs/1506.01497">Faster
R-CNN</a>が有名です。</p>
<p>これらは共通して、まずVGGやResNetのような画像分類で大きな成果をあげたネットワーク構造を用いて画像から特徴マップを抽出します。そのあと、single
stageタイプのもの、例えばSSDでは、予め特徴マップの空間方向にびっしりと候補領域を仮定しておき（default
boxと呼ばれます）、そのそれぞれについて「どの程度正解のbounding
boxからずれているか」を計算し、これを最小化するように学習を行います。また、この位置の補正と同時にそれぞれが「どのクラスに属しているか」も予測させ、この間違いも少なくするよう学習を行います。</p>
<p>一方、two stageタイプの手法、例えばFaster
R-CNNでは、抽出された特徴マップに対してさらに別のネットワークが物体の候補領域（region
proposal）を予測し、その結果を使って候補領域ごとの特徴ベクトルを作成し（RoI
poolingと呼ばれる計算が用いられます）、それらをクラス分類問題と候補領域の修正量を求める回帰問題を解くための2つの異なる小さなネットワークにさらに渡す、という構造をとります。</p>
<p>このため、一般にsingle
stageタイプのネットワークの方が高速であると言われます。一方、two
stageタイプのものの方が精度は高い、と言われます。このようなトレードオフについては、これを調査した論文（<a class="reference external" href="https://arxiv.org/abs/1611.10012">Speed/accuracy
trade-offs for modern convolutional object
detectors</a>）より、以下の図がしばしば参照されます。
<img alt="image0" src="https://github.com/mitmul/medical-ai-course-materials/blob/master/notebooks/images/speed-accuracy-tradeoffs.png?raw=1" /></p>
<p>今回用いるSSDという手法のネットワークアーキテクチャは、SSD論文のFig.
2より以下のような形をしています。</p>
<p><img alt="image1" src="https://github.com/mitmul/medical-ai-course-materials/blob/master/notebooks/images/ssd-architecture.png?raw=1" /></p>
<p>特徴抽出を行うVGG-16ネットワークの途中の出力に対してそれぞれ別々の畳み込み層を適用して検出結果（bounding
boxの位置と、その中の物体のクラス）を出力していくことで、複数のスケールでの予測を起こっているのが特徴です。</p>
</div>
<div class="section" id="モデルの定義">
<h2>3.5. モデルの定義<a class="headerlink" href="#モデルの定義" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>SSDのネットワーク部分の実装は、ChainerCVが提供してくれています。ChainerCVの
<code class="docutils literal notranslate"><span class="pre">chainercv.links.SSD300</span></code>
というクラスは、縦横が300ピクセルの画像を入力にとるSSDのモデルを表していて、デフォルトで特徴抽出器にはVGG16が用いられます。</p>
<p>学習に必要なロス関数を計算するクラスを用意しましょう。</p>
<p>以下に定義するクラスは、まずSSDモデルのオブジェクトと、ロス計算のためのハイパーパラメータである
<code class="docutils literal notranslate"><span class="pre">alpha</span></code> と <code class="docutils literal notranslate"><span class="pre">k</span></code> をコンストラクタで受け取っています。<code class="docutils literal notranslate"><span class="pre">alpha</span></code>
は、位置の予測に対する誤差とクラスの予測に対する誤差それぞれの間の重み付けを行う係数です。<code class="docutils literal notranslate"><span class="pre">k</span></code>
は hard negative mining
のためのパラメータです。学習時、一つの正解bounding
boxに対して、モデルは一つの近しい（positiveな）予測と、多くの間違った（negativeな）予測を出力します。この多くの間違った予測をconfidence
score（モデルがどの程度確信を持ってその予測を出力しているかを表す値）によってソートした上で、上から
positive : negative が 1:k になるように negative
サンプルを選択し、ロスの計算に使用します。このバランスを決めているのが
<code class="docutils literal notranslate"><span class="pre">k</span></code> というパラメータで、上記論文中では <span class="math notranslate nohighlight">\(k = 3\)</span>
とされているため、ここでもデフォルトで3を使っています。</p>
<p><code class="docutils literal notranslate"><span class="pre">forward</span></code>
メソッドでは、入力画像と正解の位置・ラベルのリストを受け取って、実際にロスの計算を行っています。物体検出は、物体のlocalization（位置の予測）とclassification（種類（＝クラス）の予測）の二つの問題を同時に解きます。SSDでは、localization
lossとclassification lossを別々に計算します。</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import chainer

from chainercv.links.model.ssd import multibox_loss


class MultiboxTrainChain(chainer.Chain):

    def __init__(self, model, alpha=1, k=3):
        super(MultiboxTrainChain, self).__init__()
        with self.init_scope():
            self.model = model
        self.alpha = alpha
        self.k = k

    def forward(self, imgs, gt_mb_locs, gt_mb_labels):
        mb_locs, mb_confs = self.model(imgs)
        loc_loss, conf_loss = multibox_loss(
            mb_locs, mb_confs, gt_mb_locs, gt_mb_labels, self.k)
        loss = loc_loss * self.alpha + conf_loss

        chainer.reporter.report(
            {&#39;loss&#39;: loss, &#39;loss/loc&#39;: loc_loss, &#39;loss/conf&#39;: conf_loss},
            self)

        return loss
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>import copy

import numpy as np

from chainercv import transforms
from chainercv.links.model.ssd import random_crop_with_bbox_constraints
from chainercv.links.model.ssd import random_distort
from chainercv.links.model.ssd import resize_with_random_interpolation


class Transform(object):

    def __init__(self, coder, size, mean):
        # to send cpu, make a copy
        self.coder = copy.copy(coder)
        self.coder.to_cpu()

        self.size = size
        self.mean = mean

    def __call__(self, in_data):
        # There are five data augmentation steps
        # 1. Color augmentation
        # 2. Random expansion
        # 3. Random cropping
        # 4. Resizing with random interpolation
        # 5. Random horizontal flipping

        img, bbox, label = in_data

        # 1. Color augmentation
        img = random_distort(img)

        # 2. Random expansion
        if np.random.randint(2):
            img, param = transforms.random_expand(
                img, fill=self.mean, return_param=True)
            bbox = transforms.translate_bbox(
                bbox, y_offset=param[&#39;y_offset&#39;], x_offset=param[&#39;x_offset&#39;])

        # 3. Random cropping
        img, param = random_crop_with_bbox_constraints(
            img, bbox, return_param=True)
        bbox, param = transforms.crop_bbox(
            bbox, y_slice=param[&#39;y_slice&#39;], x_slice=param[&#39;x_slice&#39;],
            allow_outside_center=False, return_param=True)
        label = label[param[&#39;index&#39;]]

        # 4. Resizing with random interpolatation
        _, H, W = img.shape
        img = resize_with_random_interpolation(img, (self.size, self.size))
        bbox = transforms.resize_bbox(bbox, (H, W), (self.size, self.size))

        # 5. Random horizontal flipping
        img, params = transforms.random_flip(
            img, x_random=True, return_param=True)
        bbox = transforms.flip_bbox(
            bbox, (self.size, self.size), x_flip=params[&#39;x_flip&#39;])

        # Preparation for SSD network
        img -= self.mean
        mb_loc, mb_label = self.coder.encode(bbox, label)

        return img, mb_loc, mb_label
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from chainer.datasets import TransformDataset
from chainer.optimizer_hooks import WeightDecay
from chainer import serializers
from chainer import training
from chainer.training import extensions
from chainer.training import triggers
from chainercv.links import SSD300
from chainercv.extensions import DetectionVOCEvaluator
from chainercv.links.model.ssd import GradientScaling


batchsize = 32
gpu_id = 0
out = &#39;results&#39;

model = SSD300(n_fg_class=len(bccd_labels), pretrained_model=&#39;imagenet&#39;)
model.nms_thresh = 0.5
model.score_thresh = 0.6
train_chain = MultiboxTrainChain(model)

transformed_train_dataset = TransformDataset(train_dataset, Transform(model.coder, model.insize, model.mean))

train_iter = chainer.iterators.MultiprocessIterator(transformed_train_dataset, batchsize)
valid_iter = chainer.iterators.SerialIterator(valid_dataset, batchsize, repeat=False, shuffle=False)

optimizer = chainer.optimizers.MomentumSGD(lr=0.001)
optimizer.setup(train_chain)
for param in train_chain.params():
    if param.name == &#39;b&#39;:
        param.update_rule.add_hook(GradientScaling(2))
    else:
        param.update_rule.add_hook(WeightDecay(0.0005))

updater = training.updaters.StandardUpdater(
    train_iter, optimizer, device=gpu_id)

trainer = training.Trainer(updater, (100, &#39;epoch&#39;), out)
# trainer.extend(
#     extensions.ExponentialShift(&#39;lr&#39;, 0.1, init=1e-3),
#     trigger=triggers.ManualScheduleTrigger([20, 25], &#39;epoch&#39;))

trainer.extend(
    DetectionVOCEvaluator(
        valid_iter, model, use_07_metric=False,
        label_names=bccd_labels),
    trigger=(1, &#39;epoch&#39;))

log_interval = 1, &#39;epoch&#39;
trainer.extend(extensions.LogReport(trigger=log_interval))
trainer.extend(extensions.observe_lr(), trigger=log_interval)
trainer.extend(extensions.PrintReport(
    [&#39;epoch&#39;, &#39;iteration&#39;, &#39;lr&#39;,
     &#39;main/loss&#39;, &#39;main/loss/loc&#39;, &#39;main/loss/conf&#39;,
     &#39;validation/main/map&#39;, &#39;elapsed_time&#39;]),
    trigger=log_interval)
# trainer.extend(extensions.ProgressBar(update_interval=10))
trainer.extend(extensions.snapshot(), trigger=(10, &#39;epoch&#39;))

trainer.run()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
epoch       iteration   lr          main/loss   main/loss/loc  main/loss/conf  validation/main/map  elapsed_time
1           7           0.001       11.8782     3.23653        8.64165         0.0980714            71.9904
2           13          0.001       9.16868     2.56908        6.5996          0.165262             107.619
3           20          0.001       7.63901     2.20711        5.4319          0.0392458            141.407
4           26          0.001       6.77472     2.02828        4.74645         0.0625553            172.973
5           33          0.001       6.00165     1.92091        4.08074         0.01575              209.684
6           39          0.001       5.72283     1.90063        3.8222          0.0375237            243.523
7           45          0.001       5.46967     1.90054        3.56913         0.192544             277.311
8           52          0.001       5.02496     1.8396         3.18536         0.0783293            315.223
9           58          0.001       4.7914      1.79555        2.99585         0.0229994            349.747
10          65          0.001       4.47251     1.68303        2.78948         0.184961             391.085
11          71          0.001       4.25388     1.64025        2.61363         0.0331558            444.477
12          77          0.001       4.31887     1.71153        2.60734         0.0626426            479.472
13          84          0.001       4.13904     1.60035        2.53869         0.101018             515.848
14          90          0.001       4.10636     1.59019        2.51616         0.0832518            548.074
15          97          0.001       4.11546     1.59837        2.51709         0.0867705            587.419
16          103         0.001       3.89917     1.48416        2.415           0.115205             623.181
17          109         0.001       3.86355     1.47014        2.3934          0.100266             658.764
18          116         0.001       3.86995     1.45616        2.41379         0.0989513            695.319
19          122         0.001       3.86198     1.45437        2.40761         0.0871414            726.346
20          129         0.001       3.72478     1.38095        2.34383         0.19373              765.663
21          135         0.001       3.61199     1.32303        2.28895         0.192676             820.075
22          141         0.001       3.5567      1.27821        2.27849         0.203297             853.449
23          148         0.001       3.54244     1.25287        2.28957         0.181341             889.78
24          154         0.001       3.58337     1.28987        2.2935          0.220468             921.073
25          161         0.001       3.52916     1.25276        2.2764          0.232689             963.104
26          167         0.001       3.49038     1.24806        2.24232         0.384777             997.44
27          173         0.001       3.46042     1.22845        2.23197         0.269057             1032.77
28          180         0.001       3.37256     1.17553        2.19703         0.374551             1071.3
29          186         0.001       3.31132     1.13068        2.18064         0.345331             1103.81
30          193         0.001       3.26747     1.10303        2.16444         0.511601             1140.19
31          199         0.001       3.20539     1.0804         2.12499         0.509968             1190.94
32          205         0.001       3.31584     1.13581        2.18003         0.513901             1223.47
33          212         0.001       3.22962     1.0787         2.15092         0.583382             1260.78
34          218         0.001       3.13557     1.0356         2.09997         0.544041             1293.8
35          225         0.001       3.12583     1.02284        2.10298         0.658161             1330.79
36          231         0.001       3.06385     0.986989       2.07686         0.615888             1363.57
37          238         0.001       3.08662     1.00355        2.08308         0.558108             1401.43
38          244         0.001       3.14251     1.01848        2.12403         0.62947              1434.4
39          250         0.001       3.08935     0.996988       2.09236         0.607305             1468.81
40          257         0.001       3.08343     1.00345        2.07999         0.559888             1509.66
41          263         0.001       3.08997     1.02132        2.06864         0.602576             1566.16
42          270         0.001       3.07386     1.01009        2.06377         0.563735             1604.57
43          276         0.001       2.96419     0.950791       2.0134          0.63316              1638.12
44          282         0.001       2.89678     0.900299       1.99648         0.624261             1673.83
45          289         0.001       2.92591     0.935856       1.99005         0.661047             1713.96
46          295         0.001       2.87614     0.893266       1.98287         0.6215               1749.92
47          302         0.001       2.88201     0.898596       1.98342         0.664016             1792.47
48          308         0.001       2.9655      0.959246       2.00625         0.644537             1830.21
49          314         0.001       2.86308     0.905211       1.95787         0.634526             1867.88
50          321         0.001       2.9902      0.972136       2.01807         0.641891             1910.76
51          327         0.001       3.00249     0.997949       2.00454         0.603342             1967.01
52          334         0.001       2.94919     0.956403       1.99279         0.606005             2007.27
53          340         0.001       2.93272     0.932038       2.00068         0.660515             2043.4
54          346         0.001       2.82625     0.859792       1.96646         0.664095             2078.26
55          353         0.001       2.77573     0.845684       1.93004         0.610805             2114.95
56          359         0.001       2.87067     0.879248       1.99142         0.696267             2145.69
57          366         0.001       2.84074     0.873552       1.96719         0.68861              2182.42
58          372         0.001       2.83281     0.861554       1.97126         0.669522             2218.69
59          378         0.001       2.84461     0.889716       1.95489         0.636267             2256.1
60          385         0.001       2.82032     0.866435       1.95389         0.665094             2297.38
61          391         0.001       2.79179     0.867727       1.92407         0.680844             2348.31
62          398         0.001       2.80361     0.86265        1.94096         0.70237              2387.17
63          404         0.001       2.81777     0.857976       1.9598          0.659083             2422.43
64          410         0.001       2.65247     0.773944       1.87853         0.637573             2457
65          417         0.001       2.84679     0.887051       1.95974         0.701297             2498.3
66          423         0.001       2.83364     0.862742       1.97089         0.707915             2533.33
67          430         0.001       2.74856     0.828725       1.91983         0.694727             2572.22
68          436         0.001       2.69787     0.815684       1.88219         0.65534              2608.67
69          443         0.001       2.75005     0.83338        1.91667         0.666714             2648.36
70          449         0.001       2.7552      0.842554       1.91264         0.677461             2682.24
71          455         0.001       2.74273     0.839133       1.9036          0.708858             2737.39
72          462         0.001       2.76378     0.865904       1.89787         0.707195             2782.54
73          468         0.001       2.75783     0.870711       1.88712         0.704631             2820.99
74          475         0.001       2.66259     0.801817       1.86077         0.658652             2862.03
75          481         0.001       2.8216      0.89393        1.92767         0.665332             2900.35
76          487         0.001       2.83334     0.887237       1.9461          0.66194              2938.25
77          494         0.001       2.65268     0.798555       1.85413         0.683943             2979.56
78          500         0.001       2.73477     0.829035       1.90574         0.693721             3012.28
79          507         0.001       2.64163     0.773723       1.86791         0.694821             3051.28
80          513         0.001       2.59643     0.747141       1.84929         0.690146             3084.5
81          519         0.001       2.5978      0.743915       1.85389         0.723527             3137.85
82          526         0.001       2.65733     0.790018       1.86731         0.715197             3176.41
83          532         0.001       2.59824     0.741599       1.85664         0.695787             3210.59
84          539         0.001       2.62904     0.769579       1.85946         0.700843             3252.15
85          545         0.001       2.6041      0.756013       1.84809         0.694008             3287.26
86          551         0.001       2.67289     0.79024        1.88265         0.714115             3320.19
87          558         0.001       2.55346     0.731702       1.82176         0.710323             3359.39
88          564         0.001       2.56855     0.736754       1.8318          0.717852             3394.12
89          571         0.001       2.60706     0.767257       1.83981         0.699169             3435.02
90          577         0.001       2.52976     0.727263       1.8025          0.702023             3470.06
91          583         0.001       2.49689     0.704674       1.79222         0.707821             3523.85
92          590         0.001       2.49338     0.705096       1.78828         0.701372             3563.18
93          596         0.001       2.50495     0.696262       1.80869         0.716217             3596.64
94          603         0.001       2.4497      0.677967       1.77173         0.748874             3634.02
95          609         0.001       2.4561      0.678888       1.77722         0.732009             3666.91
96          615         0.001       2.50889     0.70419        1.80471         0.719306             3699.48
97          622         0.001       2.50843     0.696823       1.81161         0.715291             3736.77
98          628         0.001       2.42557     0.671169       1.7544          0.721482             3770.33
99          635         0.001       2.45225     0.677589       1.77466         0.722778             3808.45
100         641         0.001       2.48838     0.692238       1.79614         0.726167             3841.66
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Sequential-Data-Analysis-with-Deep-Learning.html" class="btn btn-neutral float-right" title="4. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Image_Segmentation_with_Chainer.html" class="btn btn-neutral" title="2. 実践編: CT/MRI画像のセグメンテーション" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, キカガク, Preferred Networks

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>