

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ja" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ja" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1. 機械学習に必要な数学の基礎 &mdash; メディカルAI学会認定資格向け学習資料  ドキュメント</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="2. 機械学習ライブラリの基礎" href="Introduction_to_ML_libs.html" />
    <link rel="prev" title="メディカルAI学会認定資格向け学習資料" href="../index.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAI学会認定資格向け学習資料
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">1. 機械学習に必要な数学の基礎</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#">1.1. 微分</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#">1.1.1. なぜ微分が必要か？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.1.2. 中学校の数学</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.1.3. 高校の数学</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.1.4. 微分の公式</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.1.5. 合成関数の微分</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.1.6. 偏微分</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#">1.2. 線形代数 ~基礎編~</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#">1.2.1. 線形代数は何に役立つのか</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.2.2. スカラー、ベクトル、行列、テンソルとは</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.2.3. 足し算・引き算</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.2.4. かけ算（行列積）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.2.5. サイズ感</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.2.6. 最初に押さえておくべき演算と行列</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#">1.2.6.1. 転置</a></li>
<li class="toctree-l4"><a class="reference internal" href="#">1.2.6.2. 単位行列</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inverse-matrix">1.2.6.3. 逆行列（Inverse Matrix）</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#">1.3. 線形結合と二次形式</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#">1.3.1. ベクトルで微分</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#">1.4. 統計</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#">1.4.1. 確率や統計は何に使えるの？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.4.2. 統計量</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.4.3. 正規分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.4.4. スケーリング</a></li>
<li class="toctree-l3"><a class="reference internal" href="#">1.4.5. 外れ値除去</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_ML_libs.html">2. 機械学習ライブラリの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Neural_Network.html">3. ニューラルネットワーク</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chainer_Basics.html">4. Chainer入門</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">5. Deep Learningフレームワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">6. 実践編: CT/MRI画像のセグメンテーション</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">7. 実践編: 血液の顕微鏡画像からの細胞検出</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html">8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="Basenji.html">9. 実践編：ディープラーニングを使った配列解析</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAI学会認定資格向け学習資料</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>1. 機械学習に必要な数学の基礎</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Basic_Math_for_ML.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="">
<span id="id1"></span><h1>1.4.5. 機械学習に必要な数学の基礎<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h1>
<div class="section" id="">
<span id="id2"></span><h2>1.4.5. 微分<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>まず基礎数学編のはじまりとして、微分について学んでいく。微分は高校で習うが、結局実社会で使うことはほとんどなく、微分は何に使うのか？と不明確なまま終わってしまっている人も少なくないだろう。そこで、まずは機械学習との結びつきを紹介してから、その数式について解説していく。</p>
<div class="section" id="">
<span id="id3"></span><h3>1.4.5. なぜ微分が必要か？<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>微分は何に使えるか？と問うと何人の人が答えられるだろうか。受講生にこの質問を投げかけると手が上がるのは10%未満である。高校では全員が学ぶにもかかわらず、これは由々しき事態である。</p>
<p>それでは、まず微分が何に使えるのかの質問の前に、微分は何が求まるのかを考えよう。高校の時には微分では何が求まると習っただろうか。その答えは「接線の傾き」である。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/2/01.png" /></p>
<p>具体的に見ていこう。この図の関数において、$a$という点での接線の傾きというのは具体的にはこの赤い直線の傾きを指す。具体的な数値で考えるほうがわかりやすいためここでは傾き3としている。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/2/02.png" /></p>
<p>例えば、この$b$の点においては、接線の傾きは右肩下がりであるため、負の値であり、傾き-1のようになっている。微分では、この各点における「+3」や「-1」といった傾きを求めることができるのである。</p>
<p>それでは、本題に戻そう。この各点の傾きが求まったら何が嬉しいのだろうか。結局はこの問いに答えられなければ意味がないのである。正直傾きが求まると聞いたところで、使い道が浮かばない。それは当然である。なぜなら、何かを達成したいという目標がなければ、どれだけ良いツールがあったとしてもその使い道が明確にならないためである。</p>
<p>機械学習ではどのような目標を達成するのであろうか。機械学習では、良い予測値を得たいということがゴールである。学習の際には教師データ$t$に対して、その予測値$y$が近いことが望まれる。そのため、その差分である $t-y$ が小さくなることが望ましい。ただし、$t-y$ を小さくしたいと考えれば、$y\rightarrow\infty$ とすれば小さくなるがそういう話ではなく、$t$と$y$の差を小さくしたいため、$(t-y)^{2}$ が小さくなることが望ましい。設定としては、$|t-y|$ でも良いが、$(t-y)^{2}$ の方が、数学的に取り扱いがしやすい。この $(t-y)^2$ のことを<strong>二乗誤差</strong>と呼ぶ。つまり、機械学習では、この二乗誤差を最も小さくできるようなパラメータ $w$ を求めたいというゴール設定となる。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/2/03.png" /></p>
<p>そして、二乗誤差が最小となる点を求めたいと考え、その点での接線を見てみると、そこでは傾きが0となっていることがわかる。つまり、接線の傾きを求めることができるということは、ある関数における最小（もしくは最大）となる点を求めることができるのである。これが微分を使う大きな目的である。コストを最小化したいや売り上げを最大化したい。これらはビジネスの現場では当然のように考えられている課題であり、これらの最適な点を求めるために活躍するツールが微分である。</p>
<p>いかがだろうか。微分が使われている問題設定がイメージできることで、微分を学ぶモチベーションが高まっていれば嬉しい。この例のように、機械学習を学ぶ上で、微分は切っても切れない関係にあるため、ぜひこの機会に習得していただきたい。</p>
</div>
<div class="section" id="">
<span id="id4"></span><h3>1.4.5. 中学校の数学<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>微分を学ぶために、まず中学校の2点を通る直線の復習を行う。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/2/04.png" /></p>
<p>横軸が$x$で、縦軸が$f(x)$に対して、2点 ($x_{1}$, $f(x_{1})$), ($x_{2}, f(x_{2})$)を通る直線の傾き$a$を求める。これは中学校の時にならった以下の公式で求まる。
$$
傾きa = \dfrac{f(x)の変化量}{xの変化量}
$$
つまり、これを適用すると、
$$
傾きa = \dfrac{f(x_{1}) - f(x_{2})}{x_{2}-x_{1}}
$$
となる。懐かしいなと思い出してもらえれば大丈夫である。これで中学の復習は完了で、これが微分の考え方の大半を占める。</p>
</div>
<div class="section" id="">
<span id="id5"></span><h3>1.4.5. 高校の数学<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>それでは、微分の最終系に近づけていく。そのためには、<strong>極限</strong>を知っておく必要がある。極限とは、$\lim$の下に書いた条件に近づけていく考え方である。
$$
\displaystyle \lim _{x\rightarrow 0}3x=3\times 0=0
$$
一見、$x=0$を代入しただけにしか見えないかも知れないが、点の「動き」を表したいときに有効な手段であり、こちらが微分でも登場する。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/2/05.png" /></p>
<p>それでは、次の問題として、上図の$x$における接線の傾き$a$を求める。さて、ここで問題が生じる。接線の傾きを求めようにも1点では傾きを求めることができない。そこで、中学校の数学で考えた2点を通る直線と極限を組み合わせて考えてみる。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/2/06.png" /></p>
<p>$x$から$h$だけ離れた点 $x+h$ を考え、この2点を通る直線の傾きを求める。また、$h \rightarrow 0$ とすれば、理論上2点が1点となり、1点での接線を求めることができる。これを数式にすると以下のようになる。
$$
\begin{aligned}
f'\left( x\right) &amp;=\lim _{h\rightarrow 0}\dfrac {f\left( x+h\right) -f\left( x\right) }{\left( x+h\right) -x}\
&amp;=\lim _{h\rightarrow 0}\dfrac {f\left( x+h\right) -f\left( x\right) }{h}\
&amp;=\lim _{h\rightarrow 0}\dfrac {f\left( x+h\right) -f\left( x\right) }{h}\
\end{aligned}
$$
$f'(x)$は関数$f(x)$の微分を表している。この微分で得られた関数のことを<strong>導関数</strong>と呼ぶ。また、微分は $(\cdot)'$ と記述しているが、
$$
(\cdot)' = \dfrac{d}{dx}(\cdot)
$$
も同じであり、今後は主に右辺の書き方が多くなってきて複雑に見えるが、難しいものではないため安心してほしい。このように、中学校の数学と極限の考え方を合わせるだけで微分を理解することができる。</p>
</div>
<div class="section" id="">
<span id="id6"></span><h3>1.4.5. 微分の公式<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>機械学習では、以下の3つの微分の公式だけ把握しておけば、大抵の計算を行うことができる。
$$
\begin{aligned}
\left( 1\right) ^{'}&amp;=0\
\left( x\right) ^{'}&amp;=1\
\left( x^{2}\right) ^{'}&amp;=2x
\end{aligned}
$$
これらの微分の公式をこれからよく使うため、暗記しておくことが望ましい。また、特殊な関数の微分が出てきた際には追って解説を行う。この3つの微分の公式の導出を行う。</p>
<p>まずは、$f(x)=1$のときである。$f(x)=1$より、$f(x+h)=1$となる。この考え方に多少癖があるが、$f(x)$のときに$x$となっている箇所が f(x+h) のときに $x+h$ へと変わる。今回は $f(x)=1$ より、$x$ が出てこなかったため、$f(x+1)$ も何も変わらず 1 なのである。このとき、
$$
\begin{aligned}
f^{'}(x)&amp;=\left( 1\right) ^{'}\
&amp;=\lim _{h\rightarrow 0}\dfrac {f\left( x+h\right) -f\left( x\right) }{h}\
&amp;=\lim _{h\rightarrow 0}\dfrac {1-1}{h} \
&amp;=\lim _{h\rightarrow 0}\dfrac {0}{h}\
&amp;=\lim _{x\rightarrow 0}0\
&amp;=0
\end{aligned}
$$
が得られる。</p>
<p>つぎに、$f(x)=x$のときは、$f(x+h) = x+h$ となるため、
$$
\begin{aligned}
f'\left( x\right) &amp;=\left( x\right) ^{'}\
&amp;=\lim _{h\rightarrow 0}\dfrac {f\left( x+h\right) -f\left( x\right) }{h}\
&amp;=\lim _{h\rightarrow 0}\dfrac {\left( x+h\right) -x}{h}=\lim _{h\rightarrow 0}\dfrac {h}{h}\
&amp;=\lim _{h\rightarrow 0}1\
&amp;=1\end
{aligned}
$$
が得られる。</p>
<p>最後に、$f(x) = x^{2}$ の場合、$f(x+h) = (x+h)^{2}$ である。よくある間違いとして、$f(x+h) = x^{2} + h$ や $f(x+h) = x^{2} + h^{2}$ を見かけるが、$f(x) = (x)^{2}$ と考えると、$x$ の部分が $x+h$ に代わるため、$f(x+h) = (x+h)^{2}$ である。このとき、
$$
\begin{aligned}
f\left( x\right) &amp;=\left( x^{2}\right) ^{'}\
&amp;=\lim_{h\rightarrow 0}\dfrac {f{\left( x+h\right) }-f\left( x\right) }{h}\
&amp;=\lim _{h\rightarrow 0}\dfrac {\left( x+h\right) ^{2}-x^{2}}{h}\
&amp;=\lim _{h\rightarrow 0}\dfrac {\left( x^{2}+2xh+h^{2}\right) -x^{2}}{h}\
&amp;=\lim _{h\rightarrow 0}\dfrac {2xh+h^{2}}{h}\
&amp;=\lim _{h\rightarrow 0}\dfrac {\left( 2x+h\right) h}{h}\
&amp;=\lim _{h\rightarrow 0}2x\
&amp;=2x
\end{aligned}
$$
が得られる。</p>
<p>もう一つ、微分で便利な性質として<strong>線形性</strong>を紹介しておく。
$$
\begin{aligned}
(af(x))' &amp;= af'(x)\
(af_{1}(x) + bf_{2}(x))' &amp;= af_{1}'(x) + bf_{2}'(x)
\end{aligned}
$$
2つ目の式は1つ目の式も包含しているが、要するに微分する前に定数倍するものと微分してから定数倍は同じという性質、それから足し算してから微分するものと、それぞれを微分してから足し算をすることが同じという２つの性質がある。これらの導出を行っても良いが、まずは具体的な計算がわかりやすいように、2つの例題を見ていく。
$$
\begin{aligned}
( 3x^{2})'&amp;=3\times (x^{2})'\
&amp;=3\times 2x\
&amp;=6x
\end{aligned}
$$
微分の線形性を使用して、定数の3を微分の外側に出して計算できる。
$$
\begin{aligned}
\left( 3x^{2}+4\right)^{'}&amp;=\left( 3x^{2}\right) '+\left( 4\right)^{'}\
&amp;=3\times \left( x^{2}\right)^{'}+4\times \left( 1\right)^{'}\
&amp;=3\times 2x+4\times 0\
&amp;=6x
\end{aligned}
$$
このように、足し算の場合は、それぞれの微分に分割して計算を進めることができる。この微分の持つ線形性の性質は計算を細切れにして楽に進めることができ、これからの計算に大きく貢献することになる。</p>
<p>それでは、使うという観点では必ずしも必要ではないが、線形性の導出を行う。
$$
\begin{aligned}
\left( af\left( x\right) \right) '
&amp;=\lim_{h\rightarrow 0} \dfrac {af\left( x+h\right) -af\left( x\right) }{h}\
&amp;=\lim <em>{h\rightarrow 0}a\dfrac {f\left( x+h\right) -f\left( x\right) }{h}\
&amp;=a\lim</em>{h\rightarrow 0}\dfrac {f{\left( x+h\right) }-f\left( x\right) }{h}\
&amp;=af\left( x\right)
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
(af_{1}(x) +bf_{2}(x))^{'}
&amp;=\lim_{h\rightarrow 0}\dfrac {af_{1}(x+h) +bf_{2}(x+h) -af_{1}(x) +bf_{2}(x) }{h}\
&amp;=\lim_{h\rightarrow 0}\left{ \dfrac {af_{1}(x+h) -af_{1}(x) }{h}+\dfrac {bf_{2}\left( x+h\right) -bf_{2}(x)}{h}\right} \
&amp;=a\lim_{h\rightarrow 0}\dfrac {f_{1}\left( x+h\right) -f_{1}\left( x\right) }{h}+b\lim <em>{h\rightarrow0}\dfrac {f</em>{2}\left( x+h\right) -f_{2}\left( x\right) }{h}\
&amp;=af_{1}' \left( x\right) +bf_{2}'\left( x\right)
\end{aligned}
$$</p>
</div>
<div class="section" id="">
<span id="id7"></span><h3>1.4.5. 合成関数の微分<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>実際に機械学習のアルゴリズム内で登場する関数は複雑なものも多い。具体的には、
$$
\left{ (3x + 4)^{2} \right}'
$$
のように、$3x+4$ の内側の部分と $(\cdot)^{2}$ の外側の部分の2つで構成されているような場合である。この微分はすべて展開してからそれぞれ微分をすることもできるが、これが$(\cdot)^{3}$ や $(\cdot)^{4}$ となってくると、展開してから微分といった方法が現実的でなくなってくる。そのときに役に立つものが<strong>合成関数の微分</strong>である。</p>
<p>内側の関数を $u = (3x+4)$ とおくと、
$$
\left{ (3x + 4)^{2} \right}' = (u^{2})'
$$
となる。ここで、$(\cdot)'$ をもう少し厳密に考える必要が出てくる。いまは、$x$ と $u$ の2つの変数が登場しており、$(\cdot)'$ では、$x$ で微分しているのか $u$ で微分しているのかわからない。そこで、厳密に記述すると、
$$
\begin{aligned}
\left{ (3x + 4)^{2} \right}' &amp;= \dfrac{d}{dx} \left{ (3x + 4)^{2} \right} \
&amp;=  \dfrac{d}{dx} u^2 \
&amp;=  \dfrac{d}{dx} f(u)
\end{aligned}
$$
となり、$u$ の関数に対して、$x$ で微分してしまっていることがわかる。ここで、合成関数の微分
$$
\begin{aligned}
\dfrac{df(u)}{dx} = \dfrac{du}{dx} \dfrac{df(u)}{du}
\end{aligned}
$$
を適用することができる。右辺と左辺は分数として約分してしまえば同じである。つまり、合成関数の計算は内側の微分と外側の微分をそれぞれ行い、その結果を掛け合わせれ良い。それぞれの微分の計算は
$$
\begin{aligned}
\dfrac{du}{dx} &amp;= \dfrac{d}{dx} (3x+4) = 3 \
\dfrac{df(u)}{du} &amp;= \dfrac{d}{du} u^{2} = 2u \
\end{aligned}
$$
となり、これより、
$$
\begin{aligned}
\dfrac{df(u)}{dx} &amp;= \dfrac{du}{dx} \dfrac{df(u)}{du} \
&amp;= 3 \times 2u \
&amp;= 3 \times 2(3x+4) \
&amp;= 6(3x+4)
\end{aligned}
$$
が得られる。数式こそ複雑に見えるが、内側と外側をそれぞれ微分して掛け合わせるだけであるため、実際の計算は慣れると簡単に行える。本書でも合成関数の微分を使用する場面が何度も登場し、これからは当たり前の計算のひとつとして使っていくためよく覚えておいてほしい。</p>
</div>
<div class="section" id="">
<span id="id8"></span><h3>1.4.5. 偏微分<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>微分最後のトピックとして、偏微分を紹介する。偏微分と聞くと難しそうと思われがちであるが、まったく難しいものではなく、多変数関数の微分のことである。多変数関数とは？となると思うが、機械学習では、1つの入力変数$x$から出力変数$y$を予測するケースは稀であり、基本的には、複数の入力変数$x_{1}$, $x_{2}$, $\ldots$, $x_{M}$ から出力変数 $y$ を予測することが多い。例えば、家賃を予測する場合、部屋の広さだけで予測するよりも、駅からの距離や犯罪発生率などを考慮した方が予測の性能は高まりそうだと考えれば、複数の入力変数を使用する必要がある。多変数 $x_{1}$, $x_{2}$, $\ldots$, $x_{M}$ を考慮した多変数関数 $f(x_{1}, x_{2}, \ldots, x_{M})$ では、各変数で微分することを偏微分と呼び、以下のようにあらわす。
$$
\dfrac{\partial}{\partial x_{m}} f(x_{1}, x_{2}, \ldots, x_{M})
$$
大雑把には、$d$ が $\partial$ に変わっただけである。$d$ と $\partial$ の違いは、$d$ の場合、その微分する変数しか出てこない関数での微分であり、$\partial$ の場合は、それ以外の変数も含んでいる関数に対する微分である。つまり、$\partial$ は関数全体の中の一部分にだけ着目して微分していることになる。そのため、偏った微分として、「偏」微分と呼ばれている。計算方法は至って単純であり、$\dfrac{\partial}{\partial x_{m}} $の場合は $x_{m}$ 以外は定数と考えて、$x_{m}$ のみ着目して微分を行う。</p>
<p>それでは、例題で具体的な計算の流れを確認していく。
$$
\begin{aligned}
\dfrac {\partial }{\partial x_{1}}\left( 3x_{1}+4x_{2}\right) &amp;=\dfrac {\partial }{\partial x_{1}}\left( 3x_{1}\right) +\dfrac {\partial }{\partial x_{1}}\left( 4x_{2}\right) \
&amp;=3\times \dfrac {\partial }{\partial x_{1}}\left( x_{1}\right) +4x_{2}\times \dfrac {\partial }{\partial x_{1}}\left( 1\right) \
&amp;=3\times 1+4x_{2}\times 0\
&amp;= 3
\end{aligned}
$$
基本的には微分と同じ性質が適用されるため、それぞれの微分に分解できたり、定数は外に出すことができる線形性を利用できる。あとは、今回のケースでは、$x_{1}$ にだけ着目するため、$x_{2}$ ですら定数として扱うことを把握しておけば上記の計算の流れが理解できるはずである。</p>
<p>これが偏微分であり、参考書にはここからさらに全微分の話に入っていくことが多いが、ひとまずここまでの計算の方法を理解しておけば、この後の計算は理解することができる。</p>
</div>
</div>
<div class="section" id="">
<span id="id9"></span><h2>1.4.5. 線形代数 ~基礎編~<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h2>
<div class="section" id="">
<span id="id10"></span><h3>1.4.5. 線形代数は何に役立つのか<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>理系の大学に進んだ方にとっては一番最初の一般教養として受講する線形代数。ベクトル、行列、ランク、逆行列などなど、習いはするものの、どのような場面で役に立つのか見えないまま試験対策だけして過ぎ去った記憶はないであろうか。もちろん筆者もそのうちの一人である。</p>
<p>さて、線形代数が何に使えるかの前に、前章の単回帰分析からの発展を考えてみる。単回帰分析ではひとつの入力変数からひとつの出力変数を予測するようなモデルであった。例えば、部屋の広さ（$x$）から家賃（$y$）するといった例で紹介した。さて、この次は、家賃を予測する際に、部屋の広さだけでなく、駅からの距離や犯罪発生率など、ほかにも家賃に影響のありそうな要因を考慮してモデル化を行いたい。そうなると入力変数が $x_{1}$, $x_{2}$, $\ldots$, $x_{M}$ と $M$ 個を考慮できるモデル化の方法を検討しなければならない。単回帰分析でさえ、最適なパラメータ$w$を１つ求めるために結構な式変形を行ったが、これが $M$ 個あるとなれば、さらにその $M$ 倍の式変形を行わなければならない。</p>
<p>もちろんこれは事実ではあるが、同じような規則性を持っているものに対し、愚直に計算する量を増やすのではなく、その規則性を使って、もっと簡略化した計算の記述方法はないかと考えるのが筋であろう。要するに、もっと楽をできないかと考えるわけである。そこで、登場するのが今回紹介する線形代数である。大学で習う線形代数は抽象的な問題設定に対する議論であったため、難しく感じたかもしれないが、機械学習にとっての線形代数は途中の計算で楽をさせてくれるための心強い武器である。楽をするためであれば今は多少つらくても学ぶ気力が湧いてくる。それでは、単回帰分析からさらに議論の幅を増やせるように、線形代数の基礎を身に着けていく。</p>
</div>
<div class="section" id="">
<span id="id11"></span><h3>1.4.5. スカラー、ベクトル、行列、テンソルとは<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>線形代数を学ぶ上で一番最初に絶対に抑えておくべきことがこの4つである。<strong>スカラー</strong>、<strong>ベクトル</strong>、<strong>行列</strong>、<strong>テンソル</strong>である。この4つの違いを明確に述べることができるだろうか。</p>
<p>スカラーは、1つの値もしくは変数のことである。
$$
x, y, M, N
$$
のような記号で表される。スカラー単体で説明することは難しいが、次のベクトルと比較するとわかりやすい。</p>
<p>ベクトルは、複数のスカラーを縦方向（もしくは横方向）に集めたものであり、
$$
x=\begin{bmatrix}
x_{1} \
x_{2} \
x_{3}
\end{bmatrix}, y=\begin{bmatrix}
y_{1} \
y_{2} \
\vdots \
y_{N}
\end{bmatrix}
$$
ように表される。ベクトルを縦方向に定義するか横方向に定義するかは業界によって違っている。機械学習では縦方向で定義している論文や参考書が多いと感じるため、本書では<strong>ベクトルは縦方向</strong>で統一する。</p>
<p>行列は複数のベクトルをまとめたものであり、
$$
X=\begin{bmatrix}
x_{11} &amp; x_{12} \
x_{21} &amp; x_{22} \
x_{31} &amp; x_{32}
\end{bmatrix}
$$
のように表す。また、行列ではサイズを確認することが多く、この$X$は3行2列であり、サイズが(3, 2)の行列という。</p>
<p>最後にテンソルは行列をさらにまとめたものであり、記号では表現しきれないが、図のように行列を奥行き方向にさらに展開したものである。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/4/01.png" /></p>
<p>例えば、RGB (Red Green Blue) などの色空間で表現するカラー画像などがこのテンソルに対応する。</p>
<p>スカラー $\subset$ ベクトル $\subset$ 行列 $\subset$ テンソルのような関係であることがわかる。この形のイメージだけでもしっかりと把握しているだけでも線形代数を学ぶ上では理解を大きく助けてくれる。線形代数では $y$ や $X$ といった文字だけで式変形をしていくため、どのような形の数値が取り扱われているかわかりにくいが、これはベクトルなどと常に意識しておくことでその形を見失うことはない。</p>
<p>また、文字の使い分けもしっかり定義していく。$X$ や $y$ は適当に使っているわけではなく、本書では以下のルールで文字を使い分ける。</p>
<p>| -      | 小文字         | 大文字         |
| ------ | -------------- | -------------- |
| 細文字 | スカラーの変数 | スカラーの定数 |
| 太文字 | ベクトル       | 行列、テンソル |</p>
</div>
<div class="section" id="">
<span id="id12"></span><h3>1.4.5. 足し算・引き算<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>$$
\begin{aligned}\begin{bmatrix}
1 \
2 \
3
\end{bmatrix}+\begin{bmatrix}
4 \
5 \
6
\end{bmatrix}&amp;=\begin{bmatrix}
1+4 \
2+5 \
3+6
\end{bmatrix}=\begin{bmatrix}
7 \
8 \
9
\end{bmatrix}\
\begin{bmatrix}
1 &amp; 2 &amp; 3 \
4 &amp; 5 &amp; 6
\end{bmatrix}+\begin{bmatrix}
7 &amp; 8 &amp; 9 \
10 &amp; 11 &amp; 12
\end{bmatrix}&amp;=\begin{bmatrix}
8 &amp; 10 &amp; 12 \
14 &amp; 16 &amp; 18
\end{bmatrix}\end{aligned}
$$</p>
<p>このように行列やベクトルの足し算では、要素同士の対応する場所を足し合わせる。計算としては非常に簡単なものである。</p>
<p>ここでポイントとして、同じサイズでないと計算が成立しないということを覚えておく。</p>
</div>
<div class="section" id="">
<span id="id13"></span><h3>1.4.5. かけ算（行列積）<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>行列の掛け算は複数パターンあり、一般的に掛け算として用いられるものは<strong>行列積</strong>と呼ばれる。それ以外には外積や要素積（アダマール積）などがある。行列積は計算の方法が少し変わっており、以下のように線を引きながら計算する。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/4/02.png" /></p>
<p>このように、単純に要素ごとの積を扱うわけではないため、計算上の注意が必要である。足し算・引き算の場合とは異なり、行列積では以下のような条件となる。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/4/12.png" /></p>
<p>このように行列積は特殊な計算方法になっているが、この変わった計算方法が機械学習にとってはうまく作用する。このあたりは、重回帰分析の章で体験していただきたい。</p>
<p>また、行列ではかけ算はありますが割り算はありません。理由としては、みなさんが知っているスカラーの計算の</p>
<p>$4 / 2$</p>
<p>は</p>
<p>$4 \times \dfrac{1}{2}$</p>
<p>でも計算できるためである。そのため、割り算がなくても同じ計算を行うことができるため、割り算に相当する演算は存在しない。</p>
<p>それでは、この計算条件の確認も踏まえて、下記の３つを練習問題として解いてください。
$$
\begin{aligned}
&amp;\left( 1\right) \begin{bmatrix} 1 &amp; 2 \end{bmatrix}\begin{bmatrix} 3 \ 4 \end{bmatrix}\
&amp;\left( 2\right) \begin{bmatrix} 1 &amp; 2 \ 3 &amp; 4 \end{bmatrix}\begin{bmatrix} 5 \ 6 \end{bmatrix}\
&amp;\left( 3\right) \begin{bmatrix} 1 &amp; 2 \end{bmatrix}\begin{bmatrix} 3 &amp; 4 \ 5 &amp; 6 \end{bmatrix}\begin{bmatrix} 3 \ 1 \end{bmatrix}\end{aligned}
$$
それでは、こちらが解答です。サイズの確認も含めて行う。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/4/14.png" /></p>
<p>こちらのように、1 $\times$ 1 の場合は$[11]$のように行列風にはかかず、解答のようにスカラーで記述する。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/4/15.png" /></p>
<p>３つの行列積の場合、一度にすべてを計算できないため、２つずつ計算する。前２つを計算しても良いですし、後ろ２つから計算してもどちらでも計算結果は同じとなり。今回は後ろから計算する。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/4/16.png" /></p>
<p>計算のイメージは沸いただろうか。実はこの３つの計算は機械学習において非常によく出てくる形の計算である。押さえておくべきポイントとして、演算後に形が変わることを覚えておこう。</p>
</div>
<div class="section" id="">
<span id="id14"></span><h3>1.4.5. サイズ感<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>この言葉は筆者が定義した言葉であるが、計算を行う上でこの<strong>サイズ感</strong>を覚えておくことが機械学習を円滑に理解するための一つのポイントではないかと思う。先ほどの練習問題の３つのサイズがどのように変化したかをまとめた。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/4/17.png" /></p>
<p>先ほど計算したため、納得していただけるだろう。なぜこのサイズが変化する感覚をつかんでおくことが大事になるのだろうか。</p>
<p>今回は数値で計算結果を追っていったが、これからの計算はすべて数値ではなく文字で表していく。前述したが、ベクトルは縦向きで定義するため、横向きのベクトルは<strong>転置</strong>（記号は上付きの$T$）を使うことによって表現でき、</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/4/18.png" /></p>
<p>Xやyは例としてであるが、文字列だけで表すと、線形代数初心者が$x^{T}Ay$がスカラーであることに気づくことは難しいだろう。しかし、このサイズ感の章でこの文字とサイズの対応関係を把握できたため、重回帰分析などこれから始まる機械学習の数学で大きな助けとなる。慣れるまではこのサイズ感のページは何度も見返すことになると思うため、このページに目印をつけておくことを勧める。</p>
</div>
<div class="section" id="">
<span id="id15"></span><h3>1.4.5. 最初に押さえておくべき演算と行列<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<div class="section" id="">
<span id="id16"></span><h4>1.4.5. 転置<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>ベクトルは縦が基本と先述したが、先程の計算問題などでは、しばしば横のベクトルが出てきた。ベクトルの縦と横を入れ替える演算のことを<strong>転置</strong>（Transpose）という。
$$
\begin{aligned}x&amp;=\begin{bmatrix}
1 \
2 \
3
\end{bmatrix}\
x^{T}&amp;=\begin{bmatrix} 1 &amp; 2 &amp; 3 \end{bmatrix} \
X&amp;=\begin{bmatrix}
1 &amp; 4 \
2 &amp; 5 \
3 &amp; 6
\end{bmatrix}\
X^{T}&amp;=\begin{bmatrix}
1 &amp; 2 &amp; 3 \
4 &amp; 5 &amp; 6
\end{bmatrix}\end{aligned}
$$
このように転置自体の演算は簡単です。</p>
<p>転置では下記３つの公式を覚えておくと、この後の計算が楽になります。
$$
\begin{aligned}&amp;\left( 1\right) \left( A^{T}\right) ^{T}=A\
&amp;\left( 2\right) \left( AB\right) ^{T}=B^{T}A^{T}\
&amp;\left( 3\right) \left( ABC\right) ^{T}=C^{T}B^{T}A^{T}\end{aligned}
$$</p>
</div>
<div class="section" id="">
<span id="id17"></span><h4>1.4.5. 単位行列<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>単位行列とは、スカラーの１に対応した性質をもつ行列です。どのような性質かというと、10$\times$1のように、かけても変わらないという性質です。行列の演算において、これと同様の働きをする行列が単位行列になります。
$$
I=\begin{bmatrix}
1 &amp; 0 &amp; \ldots  &amp; 0 \
0 &amp; 1 &amp; \ldots  &amp; 0 \
\vdots &amp; \vdots  &amp; \ddots  &amp; \vdots  \
0 &amp; 0 &amp; \ldots  &amp; 1
\end{bmatrix}
$$
もう少し具体的に$2\times 2$の場合、
$$
I_{2\times 2}=\begin{bmatrix}
1 &amp; 0 \
0 &amp; 1
\end{bmatrix}
$$
であり、$3\times 3$の場合、
$$
I_{3\times 3}=\begin{bmatrix}
1 &amp; 0 &amp; 0 \
0 &amp; 1 &amp; 0 \
0 &amp; 0 &amp; 1
\end{bmatrix}
$$
となる。単位行列の成分を見てみると、斜め方向の対角要素が全て1であり、他の成分は全て0になっている。</p>
<p>実際に計算して、値が変わらないかを確認してみよう。
$$
\begin{aligned}\begin{bmatrix}
1 &amp; 2 \
3 &amp; 4
\end{bmatrix}\begin{bmatrix}
1 &amp; 0 \
0 &amp; 1
\end{bmatrix}
&amp;=\begin{bmatrix}
1\times 1+2\times 6 &amp; 1\times 0+2\times 1 \
3\times 1+4\times 0 &amp; 3\times 0+4\times 1
\end{bmatrix}\
&amp;=
\begin{bmatrix}
1 &amp; 2 \
3 &amp; 4
\end{bmatrix}
\end{aligned}
$$
単位行列 $I$ の算は以下のようにまとめることができる。
$$
\begin{aligned}AI&amp;=A\
IA&amp;=A\end{aligned}
$$</p>
</div>
<div class="section" id="inverse-matrix">
<span id="inverse-matrix"></span><h4>1.2.6.3. 逆行列（Inverse Matrix）<a class="headerlink" href="#inverse-matrix" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p><strong>逆行列</strong>とは下記のようなスカラーの逆数に対応する行列です。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/4/24.png" /></p>
<p>英語では<strong>Inverse Matrix</strong>（インバースマトリックス）と言う。逆行列の定義は下記のとおりです。
$$
\begin{aligned}
AA^{-1}=I\
A^{-1}A=I
\end{aligned}
$$
ここで、$I$は単位行列である。</p>
<p>どのような行列においても逆行列を計算できるわけではなく条件がある。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/4/26.png" /></p>
<p>上記は最低限の条件であり、重回帰分析の時に重要になる。また、厳密な話は線形代数の発展編で紹介する。</p>
</div>
</div>
</div>
<div class="section" id="">
<span id="id18"></span><h2>1.4.5. 線形結合と二次形式<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>これからの機械学習の数学でもよく出てくる形式として、$b^{T}x$と$x^{T}Ax$のような形式がある。この$b^{T}$のような形式を線形結合もしくは一次結合と呼び、$x^{T}Ax$のような形式を二次形式という。中学校の数学で言うなら、一次式か二次式かといった程度である。
$$
\begin{aligned}b=\begin{bmatrix}
1 \
2
\end{bmatrix},x=\begin{bmatrix}
x_{1} \
x_{2}
\end{bmatrix}\
b^{T}x=\begin{bmatrix}
1 &amp; 2
\end{bmatrix}\begin{bmatrix}
x_{1} \
x_{2}
\end{bmatrix}\
=x_{1}+2x_{2}\end{aligned}
$$
このように、$x$の要素である$x_{1}$もしくは$x_{2}$に関して、1次式となっていることがわかる。</p>
<p>また、二次形式では、
$$
\begin{aligned}A=\begin{bmatrix}
1 &amp; 2 \
3 &amp; 4
\end{bmatrix},x=\begin{bmatrix}
x_{1} \
x_{2}
\end{bmatrix}\
x^{T}Ax=\begin{bmatrix} x_{1} &amp; x_{2}\end{bmatrix}
\begin{bmatrix}
1 &amp; 2 \
3 &amp; 4
\end{bmatrix}\begin{bmatrix}
x_{1} \
x_{2}
\end{bmatrix}\
=\begin{bmatrix}x_{1} &amp; x_{2}\end{bmatrix} \begin{bmatrix}
x_{1}+2x_{2} \
3x_{1}+4x_{2}
\end{bmatrix}\
=x_{1}\left( x_{1}+2x_{2}\right) +x_{2}\left( 3x_{1}+4x_{2}\right) \
=x^{2}<em>{1}+5x</em>{1}x_{2}+4x_{2}^{2}\end{aligned}
$$
となり、各要素において二次式となっていることがわかる。</p>
<p>一般にこれらを足し合わせて、
$$
x^{T}A x + b^{T}x + c
$$
のように二次関数を表現する。ここで、$c$は定数項である。</p>
<div class="section" id="">
<span id="id19"></span><h3>1.4.5. ベクトルで微分<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>機械学習において何度も出てくる重要な演算である。それに対し、参考書では積極的に書かれていないことが多く、暗黙の了解となっていることが多い。微分と線形代数の参考書の融合した領域であるため、微分や線形代数のそれぞれの参考書では扱いにくいのかも知れない。</p>
<p>それではベクトルで微分の計算の前に、まず例題として下記の計算を行う。
$$
\begin{aligned}
b=\begin{bmatrix}
3 \
4
\end{bmatrix}, \ x=\begin{bmatrix}
x_{1} \
x_{2}
\end{bmatrix}\
b^{T}x=\begin{bmatrix}
3 &amp; 4
\end{bmatrix}\begin{bmatrix}
x_{1} \
x_{2}
\end{bmatrix}\
=3x_{1}+4x_{2}\end{aligned}
$$
この計算は線形結合として前節で紹介している。</p>
<p>ここからが本題である。この$b^{T}x$をベクトル$x$で微分したい。つまり、
$$
\dfrac {\partial }{\partial x}\left( b^{T}x\right)
$$
を求めたい。これを<strong>ベクトルで微分</strong>するという。果たして、このベクトルで微分は複雑な計算か、シンプルな計算なのか。その答えは非常にシンプルである。
$$
\begin{aligned}\dfrac {\partial }{\partial x}\left( b^{T}x\right) &amp;=\dfrac {\partial }{\partial x}\left( 3x_{1}+4x_{2}\right) \
&amp;=\begin{bmatrix}
\dfrac {\partial }{\partial x_{1}} \left( 3x_{1}+4x_{2}\right)  \
\dfrac {\partial }{\partial x_{2}} \left( 3x_{1}+4x_{2}\right)
\end{bmatrix}\end{aligned}
$$
上記に示すように、ベクトルの要素（今回だと $x_1$と$x_{2}$）のそれぞれで偏微分した値をベクトルとして格納していくだけである。一見複雑そうに見えるが、シンプルな演算で構成されていることがわかる。</p>
<p>実際に計算していく。
$$
\begin{aligned}\dfrac {\partial }{\partial x_{1}}\left( 3x_{1}+4x_{2}\right) &amp;=\dfrac {\partial }{\partial x_{1}}\left( 3x_{1}\right) +\dfrac {\partial }{\partial x_{1}}\left( 4x_{2}\right) \
&amp;=3\times \dfrac {\partial }{\partial x_{1}}\left( x_{1}\right) +4x_{2}\times \dfrac {\partial }{\partial x_{1}}\left( 1\right) \
&amp;=3\times 1+4x_{2}\times 0\
&amp;=3\end{aligned}
$$</p>
<p>$$
\begin{aligned}\dfrac {\partial }{\partial x_{2}}\left( 3x_{1}+4x_{2}\right)&amp;=\dfrac {\partial }{\partial x_{2}}\left( 3x_{1}\right) +\dfrac {\partial }{\partial x_{2}}\left( 4x_{2}\right) \
&amp;=3x_{1}\times \dfrac {\partial }{\partial x_{2}}\left( 1\right) +4\times \dfrac {\partial }{ax_{2}}\left( x_{2}\right) \
&amp;=3x_{1} \times 0 + 4 \times 1 \
&amp;= 4
\end{aligned}
$$</p>
<p>したがって、下記の計算結果が得られます。
$$
\begin{aligned}\dfrac {\partial }{\partial x}\left( b^{T}x\right)
&amp;=\begin{bmatrix}
\dfrac {\partial }{\partial x_{1}} \left( 3x_{1}+4x_{2}\right)  \
\dfrac {\partial }{\partial x_{2}} \left( 3x_{1}+4x_{2}\right)
\end{bmatrix} =\begin{bmatrix}
3  \
4
\end{bmatrix} = b
\end{aligned}
$$
ここで、ベクトルで微分した結果が最初の$c$と同じになっていることがわかります。この結果は最後に公式としてまとめます。</p>
<p>もう一問、以下の例題を考える。
$$
\begin{aligned}x=\begin{bmatrix}
x_{1} \
x_{2}
\end{bmatrix}\
\dfrac {\partial }{\partial x}\left( 1\right) =\begin{bmatrix}
\dfrac {\partial }{\partial x_{1}}\left( 1\right)  \
\dfrac {\partial }{\partial x_{2}}\left( 1\right)
\end{bmatrix}
=\begin{bmatrix}
0 \
0
\end{bmatrix}=0\end{aligned}
$$
もちろん、偏微分を行う対象の変数が含まれていない場合は0となります。要素が0のみで構成されたベクトルを<strong>零（ゼロ）ベクトル</strong>と言います。</p>
<p>こちらも踏まえて、<strong>ベクトルで微分の公式</strong>としてまとめておきましょう。
$$
\begin{aligned}
&amp;\left( 1\right) \dfrac {\partial}{\partial x}\left( c\right) =0\
&amp;\left( 2\right) \dfrac {\partial }{\partial x}\left( b^{T}x\right) =b\
&amp;\left( 3\right) \dfrac {\partial }{\partial x}\left( x^{T}Ax\right) =\left( A+A^{T}\right) x\end{aligned}
$$
(1)と(2)に関しては先程計算したものとなっております。(3)に関して導出は少し大変なので省略するが数値を代入して確認してみてほしい。こちらの3つの公式は機械学習を学んでいく上で非常に重要な公式となっているので、必ず覚えておくことが望ましい。</p>
<p>こういった行列などにおける公式は他にもたくさんあり、論文などを読む際にはどういった公式があるのかを知っていることも重要である。その際に、私がよく用いる便利な公式集がネット上にあり、Webで「The Matrix Cookbook」と検索するとPDFが公開されている。</p>
</div>
</div>
<div class="section" id="">
<span id="id20"></span><h2>1.4.5. 統計<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h2>
<div class="section" id="">
<span id="id21"></span><h3>1.4.5. 確率や統計は何に使えるの？<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>機械学習といえば確率や統計といったイメージで参考書を買ってきて勉強する人もいるだろうが、先ほどまでの重回帰分析では、微分と線形代数を理解しておくだけで説明することができた。それでは、確率統計では何を行うことができるのだろうか。</p>
<p>筆者は大きく2点あると考えている。データの分布の情報を定量評価できることと、データの分布を定式化できることである。一見、同じように見えるかもしれないが、目的がそれぞれ異なる。前者では、データの平均などといった情報を知ることで、アルゴリズムの前に、データの前処理として使う。後者では、重回帰分析などと同様にアルゴリズムのモデルとして、分布の情報を定式化して取り入れていくのである。要するに使い道が前処理なのか、モデル化なのかである。前者の使い方の場合、学ぶべき数学が非常に少なくて済む。そして、後者の場合、<strong>ベイズ統計</strong>と呼ばれる生成モデルを取り扱うための数学が必要となり、多くの前提知識を必要とする。確率統計とひとつにまとめて呼ばれがちであるが、前者は統計で、後者は確率である。</p>
<p>本書では、この両者のアプローチを紹介するが、最初からベイズ統計向けに必要な数学をまとめると多くの人が挫折してしまうために、これは本書の後半で扱うこととする。そこで、まずこの章ではデータの前処理のための統計を学ぶ。</p>
<p>この統計を学ぶことによって得られるスキルとしては、次の2つの問題に対して解決策を提示できることである。</p>
<p>各カラムの値が0～100であったり、0～1であったり、スケールがばらばらであり、アルゴリズム内部でデータを扱う際に良くないことがある。各サンプル間の距離を計算する際に、大きなスケールの値に大きく引っ張られてしまうなどである。各列のスケールに依存しないスケーリングを行いたい。</p>
<p>データの欠損値であればデータ上で簡単に見つけることができるため取り除くのは難しくないが、データの外れ値は「どのようなデータが外れているのか」といった定義が必要であり、外れ値を定義したい。</p>
<p>この<strong>スケーリング</strong>と<strong>外れ値除去</strong>は、データの前処理として実務ではほぼ必ず行うものである。</p>
</div>
<div class="section" id="">
<span id="id22"></span><h3>1.4.5. 統計量<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ここでは、とても簡単ではあるが、よく使う統計量とその意味について紹介する。</p>
<p>まずは初めに<strong>平均</strong>である。たとえば、300円, 400円, 500円の平均は、
$$
\dfrac{300 + 400 + 500}{3} = 400
$$
となり、すべてを足し合わせて候補の数で割れば良い。これを定式化すると、
$$
\begin{aligned}\overline {x}=\dfrac {x_{1}+x_{2}+\ldots +x_{N}}{N}
=\dfrac {1}{N}\sum ^{N}<em>{n=1}x</em>{n}\end{aligned}
$$
のようになる。平均は$\bar{x}$や$\mu$で表すことが多い。データの分布ではその中心に相当する値である。</p>
<p>次に、<strong>分散</strong>である。この時点で、日常生活では登場する場面が少なくなるため、分散とは何を表す値かご存知であろうか。まず分散の定義は以下の通りである。
$$
\begin{aligned}\sigma ^{2}=\dfrac {1}{N}\sum ^{N}<em>{n=1}\left( x</em>{n}-\overline {x}\right) ^{2}\end{aligned}
$$
このように、平均$\bar{x}$からの差分 $x- \bar{x}$ を計算し、それが二乗誤差の場合と同様、正と負の値を持ってしまうため、二乗してすべてを正にしてから総和を取って、平均しているのである。つまり、平均からどの程度離れているか（の二乗）の平均値である。これを何に使うことができるかの前に、分散にはもう一つ定義があり、
$$
\begin{aligned}
\sigma ^{2}=\dfrac {1}{N-1}\sum ^{N}<em>{n=1}\left( x</em>{n}-\overline {x}\right) ^{2}
\end{aligned}
$$
と表す場合もある。前者は<strong>母分散</strong>といい、後者は<strong>不偏分散</strong>という。なぜ$N$と$N-1$であるかは他の解説書〇〇（マセマの統計）にゆずるとして、大事なことはどちらを使うかである。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/6/01.png" /></p>
<p>データ解析を行う際に、<strong>母集団</strong>に対する解析か<strong>標本集団</strong>に対する解析かを意識する必要がある。母集団とは仮定しているすべてのデータセットがそろっている場合であり、標本集団はそのうちの一部を抽出する場合である。例えば、全国の小学生の身長と体重を集計する場合に、全国の小学生を一人の抜け漏れもなく集められれば、それは母集団である。それに対し、各都道府県で100人ずつ抜き出して、集計する場合は標本集団である。母集団として想定しているすべてのデータを集めることはほとんどのケースで不可能であるため、基本的に実問題で取り扱うのは標本集団である。ピックアップされた一部のデータから母集団の情報を推定するのである。そのため、基本的には標本集団向けである不偏分散を使用することとなる。</p>
<p>さて、分散についての定義がわかったところで、これは何に使えるのだろうか。分散はデータのばらつきを定量評価することができるツールである。中心からどの程度ばらついているか。実験をしたときに、このばらつきが多ければ、各実験で再現性を確保できていないことを見つけることができる。このように、何度か試行して平均に集まっていることが望ましい状況において、ざっくりと感覚で議論するのではなく、定量評価できることはとてもありがたい。また、0～1のスケールのデータでは、分散は小さな値になり、0～1000のスケールのデータでは、分散は大きな値になる。もちろん、そのばらつきにもよるが、分散ではスケールの違いも評価することができる。そのため、この情報はスケーリングに使えそうであることがわかる。</p>
<p>最後に<strong>標準偏差</strong>である。分散では二乗を行うため、元のスケールの二乗となってしまう欠点がある。これでは、データに基づいて議論する場合に、二乗のスケールを考慮しながら話す必要があり、混乱しやすい。そこで、一般的には、元のスケールで議論するため、
$$
\sigma = \sqrt{\sigma^{2}}
$$
のように、分散のルートをとる。</p>
<p>それでは、定義と使い道が見えてきたところで、練習問題で具体的な計算手順の確認を行う。以下の平均、分散、標準偏差を求めよ。ただし、今回は母分散を使用することとする。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/6/02.png" /></p>
<p>①の解答は以下のとおりである。
$$
\begin{aligned}
\bar{x}&amp;=\dfrac {1}{5}\left( -2-1+0+1+2\right) =0\
\sigma ^{2}&amp;=\dfrac {1}{5}\left{ \left( -2-0\right) ^{2}+\left( -1-0\right) ^{2}+(0-0)^{2}+(1-0)^{2}+(2-0)^{2}\right} \
&amp;=\dfrac {1}{5}\times 10=2\
\sigma &amp;=\sqrt {2}
\end{aligned}
$$
また、②の解答は以下の通りである。
$$
\begin{aligned}
\overline {x}&amp;=\dfrac {1}{5}\left( -4-2+0+2+4\right) =0\
\sigma ^{2}&amp;=\dfrac {1}{5}\left{ \left( -4-0\right) ^{2}+\left( -2-0\right) ^{2}+\left( 0-0\right) ^{2}+\left( 2-0\right) ^{2}+\left( 4-0\right) ^{2}\right} \
&amp;=\dfrac {1}{5}\times 40=8\
\sigma &amp;=\sqrt {8}=2\sqrt {2}
\end{aligned}
$$
これより、②のケースの方が分散が大きく、データのばらつきが大きいことがわかる。</p>
</div>
<div class="section" id="">
<span id="id23"></span><h3>1.4.5. 正規分布<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>確率では何度も登場する<strong>正規分布</strong>。ガウス分布とも呼ばれている。まずは数式の前に、以下のような形をしている。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/6/03.png" /></p>
<p>では、なぜこの正規分布が広く使われているのだろうか。以下の二点が挙げられると考えている。</p>
<ul class="simple">
<li>物理現象でこの分布に従うことがよくある</li>
<li>数式が扱いやすい</li>
</ul>
<p>このような物理現象の面と、工学的な取り扱い易さの面と両方兼ね備えている。</p>
<p>そして、正規分布では平均$\mu$と標準偏差$\sigma$に対して、何%がその分布に入っているかがわかる。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/6/04.png" /></p>
<p>このように、$\mu \pm 3\sigma$ の範囲内に、データの全体の99.7%が入っていることがわかる。さて、それはそれとして、これを何に使うのであろうか。</p>
<p>実はこの$\mu \pm 3\sigma$が外れ値除去に使えるのである。外れ値という定義を行うことは一般的に難しいが、この方法であれば、全体の0.3%、つまり1000個中3個が外れ値になるようなラインを設定できるのである。</p>
</div>
<div class="section" id="">
<span id="id24"></span><h3>1.4.5. スケーリング<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>スケーリングはどのアルゴリズムでも重要になってくるが、ここでは簡単に２つの事例を紹介する。</p>
<p>まず１つ目が距離の問題である。以下の図において、</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/6/05.png" /></p>
<p>この２点間の距離$d$を求めると、
$$
\begin{aligned}
d&amp;=\sqrt {\left( 100-1000\right) ^{2}+\left( 0.1-1\right) ^{2}}\
&amp;= \sqrt {900^{2}+0.9^{2}}\
&amp;= \sqrt {810000+0.81} \
&amp;= \sqrt {810000.81}
\end{aligned}
$$
のようになる。ここで着目したい点として、$x_{1}$ と $x_{2}$のどちらが距離 $d$ に対して影響を与えているかであるが、明らかに $x_{1}$ である。$x_{2}$ に関しては、スケールが小さいがゆえに、影響を与えることがほとんどできずに終わっている。これでは、$x_{2}$ がデータの意味として重要な場合においても考慮できずに終わってしまうといった問題点がある。</p>
<p>また、もうひとつが重回帰分析の重みである。重回帰分析によって最適なパラメータが求まった場合、新しいサンプルに対する予測値の計算は</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/6/06.png" /></p>
<p>のようになる。もちろん、予測値の計算としてはこれで十分である。</p>
<p>ここで、予測値の計算だけでなく、どの変数がこの予測に大きく影響を与えているかを知りたいとする。そう考えたときに、絶対値として最も大きな$w_{M}$の犯罪発生率であると決めて良いだろうか。予測値を計算してみるとわかるが、重み×入力の総和を取るため、重みが直接その入力変数の与える影響を表しているわけではない。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/6/07.png" /></p>
<p>このように、重み×入力で見てみると、最も影響を与えている変数は駅からの距離であることがわかる。実際の数値を入れてみるまで各変数の影響度を測れなければ、クライアントに説明することはできない。そこで、考え方として、各入力変数のスケールを統一することができれば、重み×入力の入力が統一されることとなり、重みが影響度を直接表すことができるのである。</p>
<p>さて、実例でのスケーリングの使い道が見てきたところで、どのようにスケーリングを行えばよいか。大きく２つの方法がある。</p>
<p>１つ目が、最小値0、最大値1にスケーリングする方法である。これを<strong>Min-Max スケーリング</strong>と呼ぶ。この方法は至って単純で、データの最小値$x_{\min}$ と最大値$x_{\max}$ を事前に求めておき、すべてのデータに対して、以下の操作を行う。
$$
\widetilde{x} = \dfrac{x - x_{\min}}{x_{\max} - x_{\min}}
$$
ここで、これは各入力変数ごとに行う。例えば、入力変数が10個ある場合では、10個の最小値・最大値を求め、10列それぞれに対して、上記の計算を行う。Min-Maxスケーリングには計算が単純というメリットの反面、</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/6/08.png" /></p>
<p>上図のように、$x_1$で外れ値が存在し、$x_{\max}$ が外れ値であるこの一点に大きく引っ張られてしまう。そのため、これは外れ値に弱い方法である。</p>
<p>もう１つのスケーリングの方法として、平均0, 標準偏差1にスケーリングする方法がある。分散含め、標準偏差ではデータのばらつきを定量評価することができ、
$$
\widetilde{x}  = \dfrac{x - \bar{x}}{\sigma}
$$
のように、標準偏差で割ることで、スケールを統一することができる。分散で計算した例題の①に対して、適用してみると、
$$
\begin{aligned}
x_{1}&amp;=\dfrac {-2-0}{\sqrt {2}}=-\dfrac {2}{\sqrt {2}}\
x_{2}&amp;=\dfrac {-1-0}{\sqrt {2}}=-\dfrac {1}{\sqrt {2}}\
x_{3}&amp;=\dfrac {0-0}{\sqrt {2}}=0\
x_{4}&amp;=\dfrac {1-0}{\sqrt {2}}=\dfrac {1}{\sqrt {2}}\
x_{5}&amp;=\dfrac {2-0}{\sqrt {2}}=\dfrac {2}{\sqrt {2}}
\end{aligned}
$$
のように、データが変換される。この時の平均と標準偏差を求めてみると、
$$
\begin{aligned}
\overline {x}&amp;=\dfrac {1}{5}\left( -\dfrac {2}{\sqrt {2}}-\dfrac {1}{\sqrt {2}}+0+\dfrac {1}{\sqrt {2}}+\dfrac {2}{\sqrt {2}}\right) =0\
\sigma ^{2}&amp;=\dfrac {1}{5}\left{ \left( -\dfrac {2}{\sqrt {2}}-0\right) ^{2}+\left( -\dfrac {1}{\sqrt {2}}-0\right) ^{2}+\left( 0-0\right) ^{2}
+\left( \dfrac {1}{\sqrt {2}}-0\right) ^{2}+\left( \dfrac {2}{\sqrt {2}}-0\right) ^{2}\right} =1\
\sigma &amp;=\sqrt {\sigma ^{2}}=1
\end{aligned}
$$
のように、平均0、標準偏差1にスケーリングできていることがわかる。この方法であれば、統計量を使用するため全体の傾向で議論することができ、一点だけの外れ値のようなケースには強い（これを<strong>ロバスト</strong>と表現する）。</p>
</div>
<div class="section" id="">
<span id="id25"></span><h3>1.4.5. 外れ値除去<a class="headerlink" href="#" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>例えば、以下の図のように時間によって変動するようなデータを扱うとする。例えば、横軸が時刻、縦軸がCPUの負荷率(%)と考えるとわかりやすい。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/6/10.png" /></p>
<p>このデータに対して、CPUの負荷率が異常な場合（外れ値）を検出したいという場合、どのようにこの外れ値を検出すれば良いか。答えは、その値の頻度に着目すればよい。</p>
<p><img alt="" src="notebooks/C:/Users/ryosu/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E3%83%A1%E3%83%87%E3%82%A3%E3%82%AB%E3%83%ABAI%E5%AD%A6%E4%BC%9A/images/6/11.png" /></p>
<p>このように、平均に対して線を引き、それぞれの値において頻度を計算してみると、正規分布が現れる。物理現象として正規分布に従うものが多いと説明していたが、普段の生活で正規分布を目にすることがないので違和感を感じていた人も多いと思うが、このような頻度では中心付近の値の頻度は多く、離れるほど頻度が少なくなっていく現象に関しては違和感がないはずである。そして、このデータの平均$\mu$と標準偏差$\sigma$を計算し、$\mu \pm 3\sigma$の値に線を引けば、外れ値除去を行うことができる。これを<strong>3σ法</strong>と呼び、理論がシンプルかつ、プログラムの実装的にも平均と標準偏差だけで行えるため簡単であり、実用的な方法といえます。</p>
<p>外れ値の回数が多い場合などは平均や標準偏差がその外れ値に引っ張られ、3σ法ではうまく対処できない場合があり、そのような場合には中央値をベースとした<strong>Hampel判別法</strong>を用いることもある。</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Introduction_to_ML_libs.html" class="btn btn-neutral float-right" title="2. 機械学習ライブラリの基礎" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../index.html" class="btn btn-neutral" title="メディカルAI学会認定資格向け学習資料" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, キカガク, Preferred Networks

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>