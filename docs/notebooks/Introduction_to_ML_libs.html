

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ja" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ja" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>2. 機械学習ライブラリの基礎 &mdash; メディカルAIコース オンライン講義資料&lt;Paste&gt;  ドキュメント</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="検索" href="../search.html" />
    <link rel="next" title="3. ニューラルネットワークの基礎" href="Introduction_to_Neural_Network.html" />
    <link rel="prev" title="1. 機械学習に必要な数学の基礎" href="Basic_Math_for_ML.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> メディカルAIコース オンライン講義資料<Paste>
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Basic_Math_for_ML.html">1. 機械学習に必要な数学の基礎</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2. 機械学習ライブラリの基礎</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#単回帰分析">2.1. 単回帰分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#問題設定">2.1.1. 問題設定</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step1.-モデルを決める">2.1.2. Step1. モデルを決める</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step2.-目的関数を決める">2.1.3. Step2. 目的関数を決める</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step3.-最適なパラメータを求める">2.1.4. Step3. 最適なパラメータを求める</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#重回帰分析">2.2. 重回帰分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#問題設定">2.2.1. 問題設定</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step1.-モデルを決める">2.2.2. Step1. モデルを決める</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step2.-目的関数を決める">2.2.3. Step2. 目的関数を決める</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step3.-パラメータを最適化する">2.2.4. Step3. パラメータを最適化する</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Numpyによる実装">2.3. Numpyによる実装</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Scikit-learnによる本格的な実装">2.4. Scikit-learnによる本格的な実装</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Scikit-learn-基礎編">2.4.1. Scikit-learn 基礎編</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scikit-learn-応用編">2.4.2. Scikit-learn 応用編</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#実用的な機械学習アルゴリズムの紹介">2.5. 実用的な機械学習アルゴリズムの紹介</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Support-Vector-Machine-(SVM)">2.5.1. Support Vector Machine (SVM)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Support-Vector-Regression-(SVR)">2.5.1.1. Support Vector Regression (SVR)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Support-Vector-Classification-(SVC)">2.5.1.2. Support Vector Classification (SVC)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Random-Forest">2.5.2. Random Forest</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#回帰-(Regression)">2.5.2.1. 回帰 (Regression)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#分類-(Classification)">2.5.2.2. 分類 (Classification)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ロジスティック回帰">2.5.3. ロジスティック回帰</a></li>
<li class="toctree-l3"><a class="reference internal" href="#k-means">2.5.4. k-means</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Neural_Network.html">3. ニューラルネットワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Introduction_to_Chainer.html">4. Deep Learningフレームワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_Segmentation.html">5. 実践編: MRI画像のセグメンテーション</a></li>
<li class="toctree-l1"><a class="reference internal" href="Blood_Cell_Detection.html">6. 実践編: 血液の顕微鏡画像からの細胞検出</a></li>
<li class="toctree-l1"><a class="reference internal" href="DNA_Sequence_Data_Analysis.html">7. 実践編：ディープラーニングを使った配列解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sequential_Data_Analysis_with_Deep_Learning.html">8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">メディカルAIコース オンライン講義資料<Paste></a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>2. 機械学習ライブラリの基礎</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Introduction_to_ML_libs.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<p><a class="reference external" href="https://colab.research.google.com/github/japan-medical-ai/medical-ai-course-materials/blob/master/notebooks/Introduction_to_ML_libs.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="section" id="機械学習ライブラリの基礎">
<h1>2. 機械学習ライブラリの基礎<a class="headerlink" href="#機械学習ライブラリの基礎" title="このヘッドラインへのパーマリンク">¶</a></h1>
<p>ここでは，代表的な機械学習アルゴリズムの紹介とチューニングのポイントを数学的な背景と合わせて紹介していきます．
機械学習の考え方を身に着ける練習として，<strong>単回帰分析</strong>と<strong>重回帰分析</strong>のアルゴリズムを数式と一緒に考えていきましょう．これらを学ぶことで微分と線形代数，統計の使い方が見えてくると思います．重回帰分析は次章で紹介するニューラルネットワークでもその考え方のベースになるところが多いため，しっかりと数式を理解しておきましょう．</p>
<div class="section" id="単回帰分析">
<h2>2.1. 単回帰分析<a class="headerlink" href="#単回帰分析" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>まずはじめに，機械学習の中でも最も基礎的な単回帰分析について説明します．機械学習アルゴリズムは，<strong>教師あり学習</strong>と<strong>教師なし学習</strong>に大別され，単回帰分析は教師あり学習の一種です．教師あり学習の典型的な問題として，10や0.1のように数値（厳密には連続値）を予測する<strong>回帰</strong>と，赤ワイン
or
白ワインのようにカテゴリを予測する<strong>分類</strong>があります．単回帰分析はその名の通り，回帰を取り扱う手法で，ひとつの入力変数からひとつの出力変数を予測する機械学習アルゴリズムです．</p>
<div class="section" id="問題設定">
<h3>2.2.1. 問題設定<a class="headerlink" href="#問題設定" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>機械学習では，データをもとに学習を行いますが，データに含まれる情報の中から何を利用し，何を予測させるかは人間が決める必要があります．</p>
<p>ここでは例として，家賃を予測する問題を考えることにします．従って，家賃が<strong>出力変数</strong>
<span class="math">\(y\)</span> となります．</p>
<p>次に，<strong>入力変数</strong>として何を採用するかを考えます．家賃の予測では，たとえば部屋の広さ，駅からの距離，犯罪発生率などを，入力変数として検討することができます．ここでは部屋の広さを入力変数
<span class="math">\(x\)</span>
として採用することにしましょう．実際には，複数の入力変数候補があった際に，それらすべてを扱うことができるようなモデル化が一般的ですが，それは次の重回帰分析以降で紹介することにします．</p>
<p>機械学習のアルゴリズムでは，どの手法も大きく分けて次の3ステップで成り立っています．</p>
<ul class="simple">
<li>Step1: モデルを決める</li>
<li>Step2: 目的関数を決める</li>
<li>Step3: 最適なパラメータを求める</li>
</ul>
<p>上記の3ステップについて，順に説明していきます．</p>
</div>
<div class="section" id="Step1.-モデルを決める">
<h3>2.2.2. Step1. モデルを決める<a class="headerlink" href="#Step1.-モデルを決める" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>まずStep1では<strong>モデル</strong>を決めます．モデルとは，出力変数 <span class="math">\(y\)</span>
と入力変数 <span class="math">\(x\)</span>
の関係性を<strong>定式化</strong>したものです．どのように定式化すれば，家賃をうまく予測することができるのでしょうか．このモデル設計は現在は人手で行うのが一般的であり，機械が自動的に決めてくれるわけではありません（最近ではAutoMLなど，モデルも自動決定する研究も進展してきています）</p>
<p>例えば，与えられたデータセットにおいて，家賃と部屋の広さの関係性が次のようになっていたとしましょう．</p>
<div class="figure" id="id14">
<img alt="01.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/01.png" />
<p class="caption"><span class="caption-text">01.png</span></p>
</div>
<p>この場合，部屋が広くなるほど，家賃が高くなるという関係がみられ．予測のために直線を引くのが妥当にみえます．</p>
<div class="figure" id="id15">
<img alt="02.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/02.png" />
<p class="caption"><span class="caption-text">02.png</span></p>
</div>
<p>今回は，直線を引くことが適切であると判断し，Step1のモデルを以下のように定式化します．</p>
<div class="math">
\[y = wx + b\]</div>
<p>ここで<span class="math">\(w\)</span>は傾き，<span class="math">\(b\)</span>は切片とよばれるパラメータです（機械学習では，傾きを<strong>重み
(weight)</strong> <span class="math">\(w\)</span>, 切片を<strong>バイアス (bias)</strong> <span class="math">\(b\)</span>
という記号で表現するのが一般的です）．</p>
<p>単回帰分析では，このように直線 <span class="math">\(y = wx + b\)</span>
と決めて，重みとバイアスの値をデータにうまくフィットするように調整していきます．この調整すべき変数のことを<strong>パラメータ</strong>と呼びます．今回は
<span class="math">\(w\)</span> と <span class="math">\(b\)</span> がパラメータとなります．</p>
<p>多くの機械学習ではこのようなパラメータで特徴付けられたモデルを使い，与えられたデータセットに適合するように最適なパラメータを求めることが目標となります．ここでデータセットは，入力変数である部屋の広さ
<span class="math">\(x\)</span> と<strong>教師データ</strong>となる家賃 <span class="math">\(t\)</span>
の組からなるデータの集合です（本解説では，機械学習による予測値を
<span class="math">\(y\)</span> ，教師データとして与えるものを <span class="math">\(t\)</span>
，と使い分けています）．データセットは
<span class="math">\(\mathcal{D} = \{x_n, t_n\}_{n=1}^{N}\)</span>
として表されることもあります．ここで，添え字 <span class="math">\(n\)</span>
(<span class="math">\(n=1,2,\ldots,N\)</span>) は <span class="math">\(n\)</span>
番目の物件という意味であり，<span class="math">\(N\)</span> は全体の物件数のことです．この
<span class="math">\(N\)</span> は<strong>サンプル数</strong>とよばれています．</p>
<p>ここで，この後の計算を楽に進めるために，<strong>データの中心化</strong>というテクニックを紹介します．下図に示すように，部屋の広さと家賃は両方とも正の値であるため，左のグラフのような形になります．中心化では，<strong>平均を0</strong>とした中央に配置するような変換の処理を施します．この中心化は多くのアルゴリズムで前処理として行うことが一般的です．厳密には前章で紹介した中心化込みのスケーリングがよく用いられます．
<img alt="03.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/03.png" /></p>
<p>この中心化の処理を行う理由は下図の通り，データの中心化によってバイアス
<span class="math">\(b\)</span> が0となり，<span class="math">\(y_{c} = wx_{c}\)</span>
のように，バイアス成分なしで表現することができるためです，これによって，調整すべきパラメータを<strong>2つから1つ</strong>に減らすことができます．
<img alt="04.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/04.png" /></p>
<p>データの中心化は入出力の平均をデータの全体から引くことで実現されます．つまり，</p>
<div class="math">
\[\begin{split}\begin{aligned}
x_{c} &amp;= x - \bar{x} \\
t_{c} &amp;= t - \bar{t}
\end{aligned}\end{split}\]</div>
<p>となります．例えば，具体的な数値で見ると，下図の通りです． <img alt="05.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/05.png" /></p>
<p>中心化後を示す添え字の <span class="math">\(c\)</span>
に関しては表現が冗長となるため，今後はこの添え字を省略し，データの中心化を事前に行っていることを前提とします．この時，モデルは</p>
<div class="math">
\[y = wx\]</div>
<p>となり，単回帰分析の目標は，データセット
<span class="math">\(\mathcal{D} = \{x_n, t_n\}_{n=1}^{N}\)</span> に基づいて，パラメータ
<span class="math">\(w\)</span> を<strong>適切</strong>に調整することになります．</p>
</div>
<div class="section" id="Step2.-目的関数を決める">
<h3>2.2.3. Step2. 目的関数を決める<a class="headerlink" href="#Step2.-目的関数を決める" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>どれだけ目標を達成しているかを表す関数を<strong>目的関数</strong>とよびます．分野によっては<strong>評価関数</strong>ともよばれます．</p>
<p>今回は教師データと予測値が一致することが目標であり，それを表す目的関数として教師データと予測値の二乗誤差を使います．二乗誤差が0であるとき，またその時のみ<span class="math">\(t = y\)</span>
となり，完璧な予測を達成しているといえます，<span class="math">\(n\)</span>
番目の物件に対する教師データ<span class="math">\(t_{n}\)</span>
と予測値<span class="math">\(y_{n}\)</span>の二乗誤差は</p>
<div class="math">
\[(t_{n} - y_{n})^{2}\]</div>
<p>となります．これを全物件で考慮する必要があるため，最終的な目的関数はその総和をとり，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}&amp;=\left( t_{1}-y_{1}\right)^{2}+\left( t_{2}-y_{2}\right)^{2}+\ldots + (t_{N}-y_{N})^{2} \\
&amp;=\sum^{N}_{n=1}\left( t_{n}-y_{n}\right)^{2}\\
\end{aligned}\end{split}\]</div>
<p>となります．また，Step1で決めたモデル</p>
<div class="math">
\[y_{n} = wx_{n}\]</div>
<p>を代入すると，目的関数は</p>
<div class="math">
\[\mathcal{L}=\sum^{N}_{n=1}\left( t_{n}-wx_{n}\right)^{2}\]</div>
<p>とパラメータを含んだ形式で表現することができます．機械学習ではこのような望ましい時に値が<span class="math">\(0\)</span>であり，望ましくない時に大きな正の値となるような関数を目的関数とします．このような関数を<strong>損失関数</strong>と呼び，その値を<strong>損失</strong>とよびます．多くの場合，複数の教師データからなる学習問題では，各教師データに対する損失関数の和を目的関数とし，それを最小化することで望ましい状態を達成することを目指します．</p>
</div>
<div class="section" id="Step3.-最適なパラメータを求める">
<h3>2.1.4. Step3. 最適なパラメータを求める<a class="headerlink" href="#Step3.-最適なパラメータを求める" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>最後は目的関数を最小化するようなパラメータを求めます．ここで，ある関数を最小化する点を求める方法として，微分が使えることを既に学んでいます．今回のような差の二乗の場合，微分して「傾き0」となる点が損失が<span class="math">\(0\)</span>となる点です．目的関数の微分を求めると，次のようになります．</p>
<div class="math">
\[\begin{split}\begin{aligned}
\dfrac{\partial }{\partial w} \mathcal{L}  &amp;= \dfrac{\partial}{\partial w} { \sum^{N}_{n=1} ( t_{n}-wx_{n})^{2} }\\
\end{aligned}\end{split}\]</div>
<p>ここで，微分は<strong>線形性</strong>の性質を持っており，特に和の微分は，微分の和であることを利用して次を得ます．</p>
<div class="math">
\[\dfrac{\partial}{\partial w} \mathcal{L}=\sum^{N}_{n=1}\dfrac {\partial }{\partial w}\left( t_{n}-wx_{n}\right)^{2}\]</div>
<p>ここで微分と総和 <span class="math">\(\sum\)</span>
の記号が入れ替わっています（なお，微分と総和は常に交換可能とは限りません）．次に，和の各項をみると，</p>
<div class="math">
\[\dfrac {\partial }{\partial w}\left( t_{n}-wx_{n}\right)^{2}\]</div>
<p>の部分は<strong>合成関数</strong>になっていることがわかります．<span class="math">\(u_{n} = t_{n} - wx_{n}\)</span>,
<span class="math">\(f(u_{n}) = u_{n}^{2}\)</span> とおくと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial w}\left( t_{n}-wx_{n}\right)^{2} &amp;=  \dfrac {\partial }{\partial w} f(u_{n}) \\
\Rightarrow \dfrac {\partial }{\partial w} f(u_{n}) &amp;= \dfrac {\partial u_{n}}{\partial w} \dfrac{\partial f(u_{n})}{\partial w} \\
&amp;=-x_{n} \times 2 u_{n}  \\
&amp;= -2x_{n}( t_{n}-wx_{n} )
\end{aligned}\end{split}\]</div>
<p>が得られます．これより，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\dfrac{\partial }{\partial w} \mathcal{L}
&amp;=\sum^{N}_{n=1}\dfrac {\partial }{\partial w}\left( t_{n}-wx_{n}\right)^{2}
\\&amp;=-\sum^{N}_{n=1}2x_{n}\left( t_{n}-wx_{n}\right)
\end{aligned}\end{split}\]</div>
<p>となります．この微分の値が0となるように<span class="math">\(w\)</span>を求めていくと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial w} \mathcal{L} &amp;=0\\
-2\sum^{N}_{n=1}x_{n}\left( t_{n}-wx_{n}\right) &amp;=0\\
-2 \sum^{N}_{n=1}x_{n}t_{n} + 2\sum^{n}_{n=1}wx^{2}_{n}&amp;=0\\
-2\sum^{N}_{n=1}x_{n}t_{n}+2w\sum^{N}_{n=1}x^{2}_{n}&amp;=0\\
w\sum^{N}_{n=1}x^{2}_{n}&amp;=\sum^{n}_{n=1}x_{n}t_{n}\\
\Rightarrow w&amp;=\dfrac {\displaystyle  \sum^{N}_{n=1}x_{n}t_{n}}{\displaystyle  \sum^{N}_{n=1}x^{2}_{n}}
\end{aligned}\end{split}\]</div>
<p>と求まりました．この求まったパラメータ <span class="math">\(w\)</span>
を確認すると，与えられたデータセット
<span class="math">\(\mathcal{D} = \{x_n, t_n\}_{n=1}^{N}\)</span>
のみから決定できていることがわかります．</p>
<p>次に，例題にあげていた数値例でパラメータ <span class="math">\(w\)</span>
を求めてみましょう．まずは，データの中心化を行うために，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\bar{x} &amp;= \dfrac{1}{3} (1 + 2 + 3) = 2 \\
\bar{t} &amp;= \dfrac{1}{3}(2 + 3.9 + 6.1) = 4
\end{aligned}\end{split}\]</div>
<p>とそれぞれの平均を求め，各変数に対して前処理として，中心化の処理を施すと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
x_{1} &amp;= 1 - 2 = -1 \\
x_{2} &amp;= 2 -2 = 0 \\
x_{3} &amp;= 3- 2 = 1\\
t_{1} &amp;= 2 - 4 = -2\\
t_{2} &amp;= 3.9 - 4 = -0.1\\
t_{3} &amp;= 6.1 - 4 = 2.1
\end{aligned}\end{split}\]</div>
<p>となります．そして，中心化後の値を用いて，最適なパラメータ<span class="math">\(w\)</span>を導出すると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
w &amp;= \dfrac{\displaystyle \sum_{n=1}^{N}x_{n}t_{n}}{\displaystyle  \sum_{n=1}^{N}x_{n}^{2}} \\
&amp;= \dfrac{x_{1}t_{1} + x_{2}t_{2} + x_{3}t_{3}}{x_{1}^{2} + x_{2}^{2} + x_{3}^{2}} \\
&amp;= \dfrac{-1 \times (-2) + 0 \times 0.1 + 1 \times 2.1}{(-1)^{2} + 0^2 + 1^2} \\
&amp;= 2.05
\end{aligned}\end{split}\]</div>
<p>と求まりました．これで単回帰分析の学習が完了しました．この求まったパラメータを使用したモデルが<strong>学習済みモデル</strong>となります．</p>
<p>続いて，このモデルを使って新しいサンプルに対する<strong>推論</strong>を行ってみましょう．例えば，新しいサンプル
<span class="math">\(x_{q}=1.5\)</span> に対する予測値は次のように求まります，</p>
<div class="math">
\[\begin{split}\begin{aligned}
y_{c} &amp;= wx_{c} \\
y_{q} - \bar{t} &amp;= w(x_{q}-\bar{x}) \\
\Rightarrow y_{q} &amp;= w(x_{q}-\bar{x}) + \bar{t} \\
&amp;= 2.05 \times (1.5 - 2) + 4 \\
&amp;= 2.975
\end{aligned}\end{split}\]</div>
<p>以上が，単回帰分析の一連の手順となります．単回帰分析自体は単純な手法ですが，他の機械学習アルゴリズムも，基本的には同様の手順で進めていくことが出来ます．</p>
</div>
</div>
<div class="section" id="重回帰分析">
<h2>2.2. 重回帰分析<a class="headerlink" href="#重回帰分析" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>次に，多変数の入力変数を扱う重回帰分析を扱います．この重回帰分析を学ぶことで線形代数に関する知識が深まります．</p>
<p>重回帰分析は単回帰分析と同様に教師あり学習の一種であり，回帰を取り扱う手法です．問題設定は，ほとんど単回帰分析と同じですが，重回帰分析では入力変数の数が複数となります．つまり，複数の入力変数から出力変数を予測できるような機械学習アルゴリズムです．</p>
<div class="section" id="問題設定">
<span id="id5"></span><h3>2.2.1. 問題設定<a class="headerlink" href="#問題設定" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>ここでは単回帰分析の場合と同様に家賃を予測する問題を考え，家賃を出力変数
<span class="math">\(y\)</span>
とします．入力変数としては，単回帰分析では考慮しきれていなかった駅からの距離や犯罪発生率なども考慮していきます．例えば，部屋の広さ
<span class="math">\(x_{1}\)</span>, 駅からの距離 <span class="math">\(x_{2}\)</span>, …, 犯罪発生率 <span class="math">\(x_{M}\)</span>
のように <span class="math">\(M\)</span> 個の入力変数があるとします．</p>
<p>単回帰分析と同様，以下の3ステップで学習していきます．</p>
<ul class="simple">
<li>モデルを決める</li>
<li>目的関数を決める</li>
<li>最適なパラメータを求める</li>
</ul>
</div>
<div class="section" id="Step1.-モデルを決める">
<span id="step1-1"></span><h3>2.2.2. Step1. モデルを決める<a class="headerlink" href="#Step1.-モデルを決める" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>単回帰分析のモデルは，</p>
<div class="math">
\[y = wx + b\]</div>
<p>であり，<span class="math">\(w\)</span> を重み（weight），<span class="math">\(b\)</span> をバイアス (bias)
と呼びました．重回帰分析では，この式を複数の入力変数へと拡張し，</p>
<div class="math">
\[y=w_{1}x_{1}+w_{2}x_{2}+\ldots +w_{M}x_{M}+b\]</div>
<p>のような<strong>線形結合</strong>の形で表します．この場合，各入力変数は独立に出力変数に影響を与えることを仮定しており，かなり単純なモデル化といえます．実際には，入力変数間に依存関係が存在する場合には，そのことを考慮してモデル化を行う必要があります．それについては今後説明していきます．</p>
<p>重回帰分析のモデルは総和を使って整理すると，</p>
<div class="math">
\[y = \sum_{m=1}^{M} w_{m} x_{m} + b\]</div>
<p>のように書くことができます．さらにここで，<span class="math">\(x_0 = 1\)</span>，<span class="math">\(w_0 = b\)</span>とおくと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
y&amp;=w_{1}x_{1}+w_{2}x_{2}+\ldots +w_{M}x_{M}+b\\
&amp;=w_{1}x_{1}+w_{2}x_{2}+\ldots +w_{M}x_{M}+w_{0} x_{0}\\
&amp;=w_{0}x_{0}+w_{1}x_{1}+\ldots +w_{M}x_{M}\\
\end{aligned}\end{split}\]</div>
<p>のようにバイアス <span class="math">\(b\)</span>
を包含することができます．そして，この式を整理していくと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
y&amp;=w_{0}x_{0}+w_{1}x_{1}+\ldots +w_{M}x_{M}\\
&amp;=\begin{bmatrix}
w_{0} &amp; w_{1} &amp; \ldots  &amp; w_{M}
\end{bmatrix}\begin{bmatrix}
x_{0} \\
x_{1} \\
\vdots  \\
x_{M}
\end{bmatrix}\\
&amp;=\boldsymbol{w}^{T}\boldsymbol{x}
\end{aligned}\end{split}\]</div>
<p>のように，ベクトルの内積で表現することができます．また，今後取り扱う際には，<span class="math">\(\boldsymbol{x}\)</span>
が前に来ているほうが計算上便利であるため，</p>
<div class="math">
\[\begin{split}\begin{aligned}
y&amp;=w_{0}x_{0}+w_{1}x_{1}+\ldots +w_{M}x_{M}\\
&amp;=\begin{bmatrix}
x_{0} &amp; x_{1} &amp; \ldots  &amp; x_{M}
\end{bmatrix}\begin{bmatrix}
w_{0} \\
w_{1} \\
\vdots  \\
w_{M}
\end{bmatrix}\\
&amp;=\boldsymbol{x}^{T}\boldsymbol{w}
\end{aligned}\end{split}\]</div>
<p>として表現します．これが重回帰分析のモデルです．今回はパラメータとして
<span class="math">\(M+1\)</span> 個の重み <span class="math">\(\boldsymbol{w}\)</span> を求めていきます．</p>
</div>
<div class="section" id="Step2.-目的関数を決める">
<span id="step2-1"></span><h3>2.2.3. Step2. 目的関数を決める<a class="headerlink" href="#Step2.-目的関数を決める" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>単回帰分析では，教師データ<span class="math">\(t\)</span>と予測値<span class="math">\(y\)</span>の二乗誤差が小さいほど良い予測であるとし，その総和を目的関数として定めました．重回帰分析でも，予測値<span class="math">\(y\)</span>を求めるということは同じであるため，次のような同じ目的関数を使います．</p>
<div class="math">
\[\begin{aligned}
L&amp;=\left( t_{1}-y_{1}\right)^{2}+\left( t_{2}-y_{2}\right)^{2}+\ldots + \left( t_{N}-y_{N}\right)^{2}
\end{aligned}\]</div>
<p>このように，<strong>二乗誤差の総和</strong>を単回帰分析同様，目的関数として採用します．単回帰分析では，これを</p>
<div class="math">
\[\mathcal{L}=\sum^{N}_{n=1} ( t_{n} - y_{n})^{2}\]</div>
<p>のように，総和の記号を使ってまとめていましたが，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}&amp;=\left( t_{1}-y_{1}\right)^{2}+\left( t_{2}-y_{2}\right)^{2}+\ldots + \left( t_{N}-y_{N}\right)^{2}\\
&amp;=\begin{bmatrix} t_{1} - y_{1} &amp; t_{2}-y_{2} &amp; \ldots &amp; t_{N}-y_{N} \end{bmatrix} \begin{bmatrix}
t_{1}-y_{1} \\
t_{2}-y_{2} \\
\vdots \\
t_{N}-y_{N}
\end{bmatrix}\\
&amp;=\left( \boldsymbol{t}-\boldsymbol{y}\right)^{T}\left( \boldsymbol{t}-\boldsymbol{y}\right)
\end{aligned}\end{split}\]</div>
<p>のようにベクトル使って表現することもできます．また，<span class="math">\(\boldsymbol{y}\)</span>
に関して，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\boldsymbol{y}=\begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots \\
y_{N}
\end{bmatrix}=\begin{bmatrix}
\boldsymbol{x}_{1}^{T}\boldsymbol{w} \\
\boldsymbol{x}_{2}^{T}\boldsymbol{w} \\
\vdots  \\
\boldsymbol{x}_{N}^{T}\boldsymbol{w}
\end{bmatrix}
=\begin{bmatrix}
\boldsymbol{x}_{1}^{T} \\
\boldsymbol{x}_{2}^{T} \\
\vdots  \\
\boldsymbol{x}_{N}^{T}
\end{bmatrix}
\boldsymbol{w}
\end{aligned}\end{split}\]</div>
<p>のように書くことができます．これを整理すると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\boldsymbol{y}&amp;=
\begin{bmatrix}
x_{10} &amp; x_{11} &amp; x_{12} &amp; \ldots  &amp; x_{1M} \\
x_{20} &amp; x_{21} &amp; x_{22} &amp; \ldots  &amp; x_{2M} \\
\vdots  &amp; \vdots  &amp; \ddots  &amp; \vdots  \\
x_{N0} &amp; x_{N1} &amp; x_{N{2}} &amp; \ldots  &amp; x_{NM}
\end{bmatrix}\begin{bmatrix}
w_{0} \\
w_{1} \\
w_{2} \\
\vdots  \\
w_{M}
\end{bmatrix}\\
\boldsymbol{y}&amp;=\boldsymbol{X}\boldsymbol{w}
\end{aligned}\end{split}\]</div>
<p>と表記できます．ここで，行（横）方向がサンプルを表しており，例えば各物件に対応します．列（縦）方向が入力変数を表しており，例えば，部屋の広さや駅からの距離などが入っています．もう少し具体的な数値で考えると，部屋の広さ
<span class="math">\(= 50m^{2}\)</span> ，駅からの距離 <span class="math">\(= 600 m\)</span> ，犯罪発生率
<span class="math">\(= 2\)</span>% のような <span class="math">\(n\)</span> 番目の物件の場合，</p>
<div class="math">
\[\boldsymbol{x}_{n}^{T} = \begin{bmatrix}
1 &amp; 50 &amp; 600 &amp; \cdots &amp; 0.02
\end{bmatrix}\]</div>
<p>のようにデータが行方向格納されているイメージです．先頭の <span class="math">\(1\)</span>
はバイアスを包含する際に使用している <span class="math">\(x_{0}\)</span>
であることに注意してください．</p>
</div>
<div class="section" id="Step3.-パラメータを最適化する">
<h3>2.2.4. Step3. パラメータを最適化する<a class="headerlink" href="#Step3.-パラメータを最適化する" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>それでは，（Step2で定めた）目的関数を最小化する，（Step1で定めた）モデルのパラメータを求めていきましょう．</p>
<p>まずは目的関数に関して，パラメータ <span class="math">\(\boldsymbol{w}\)</span>
で表現できるように式変形を行うと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}&amp;=\left( \boldsymbol{t}-\boldsymbol{y}\right)^{T}\left( \boldsymbol{t}-\boldsymbol{y}\right) \\
&amp;=\left( \boldsymbol{t}-\boldsymbol{X}\boldsymbol{w}\right)^{T}\left( \boldsymbol{t}-\boldsymbol{X}\boldsymbol{w}\right) \\
&amp;= \left\{ \boldsymbol{t}^{T}-(\boldsymbol{X}\boldsymbol{w})^{T}\right\}\left( \boldsymbol{t}-\boldsymbol{X}\boldsymbol{w}\right) \\
&amp;=\left( \boldsymbol{t}^{T}-\boldsymbol{w}^{T}\boldsymbol{X}^{T}\right)\left( \boldsymbol{t}-\boldsymbol{X}\boldsymbol{w}\right)
\end{aligned}\end{split}\]</div>
<p>となります．ここで，転置の公式
<span class="math">\((\boldsymbol{A}\boldsymbol{B})^{T} = \boldsymbol{B}^{T}\boldsymbol{A}^{T}\)</span>
を使っていることに注意しましょう．さらに分配法則を使って展開を進めていくと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}&amp;=\boldsymbol{t}^{T}\boldsymbol{t}-\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w}-\boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{t} + \boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}\\
\end{aligned}\end{split}\]</div>
<p>となります．この目的関数に対しパラメータの<span class="math">\(w\)</span>について偏微分をとりたいが，その前にこの式はもう少し整理することができます．はじめに，</p>
<div class="math">
\[(1)^T = 1\]</div>
<p>というように，スカラーは転置しても同じです．上式の中で出てくる
<span class="math">\(\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w}\)</span>
はスカラーなので，</p>
<div class="math">
\[(\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w})^{T} = \boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w}\]</div>
<p>が成り立ちます．さらに，転置の公式
<span class="math">\((\boldsymbol{A}\boldsymbol{B}\boldsymbol{C})^T = \boldsymbol{A}^T\boldsymbol{B}^T\boldsymbol{C}^T\)</span>
より，</p>
<div class="math">
\[(\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w})^T = \boldsymbol{w}^{T} \boldsymbol{X}^{T} \boldsymbol{t}\]</div>
<p>も成り立ちます．これより，</p>
<div class="math">
\[(\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w})^{T} = \boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w} = \boldsymbol{w}^{T} \boldsymbol{X}^{T} \boldsymbol{t}\]</div>
<p>を導くことができます．目的関数を <span class="math">\(\mathcal{L}\)</span>
とおくと，上の式を利用して，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}=\boldsymbol{t}^{T}\boldsymbol{t}-2\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w} + \boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}\\
\end{aligned}\end{split}\]</div>
<p>とまとめることができます．ここで， <span class="math">\(\boldsymbol{w}\)</span>
に関する偏微分を行っていくため， <span class="math">\(\boldsymbol{w}\)</span>
以外の定数項をまとめていくと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}&amp;=\boldsymbol{t}^{T}\boldsymbol{t}-2\boldsymbol{t}^{T}\boldsymbol{X}\boldsymbol{w}+\boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}\\
&amp;=\boldsymbol{t}^{T}\boldsymbol{t}-2\left( \boldsymbol{X}^{T}\boldsymbol{t}\right)^{T} \boldsymbol{w}+\boldsymbol{w}^{T}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w} \\
&amp;=c+\boldsymbol{b}^{T}\boldsymbol{w}+\boldsymbol{w}^{T}\boldsymbol{A}\boldsymbol{w}
\end{aligned}\end{split}\]</div>
<p>のように，線形代数で学んだ <span class="math">\(\boldsymbol{w}\)</span>
に関する二次形式（二次関数）で表現することができました．ここで，<span class="math">\(\boldsymbol{A}= \boldsymbol{X}^{T}\boldsymbol{X}, \ \boldsymbol{b} =-2 \boldsymbol{X}^{T}\boldsymbol{t}, \ c=\boldsymbol{t}^{T}\boldsymbol{t}\)</span>
であり，<span class="math">\(\boldsymbol{b}\)</span>
を転置の形式にした理由は，線形代数で学んだベクトルで微分の公式の形式に合わせるための工夫です．</p>
<p>それでは，目的関数を最小化することができるパラメータ
<span class="math">\(\boldsymbol{w}\)</span>
の求め方を考えましょう．先述の通り，目的関数はパラメータ
<span class="math">\(\boldsymbol{w}\)</span>に関して二次関数です．例えば，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\boldsymbol{w} = \begin{bmatrix}
w_{1} \\ w_{2}
\end{bmatrix},
\boldsymbol{A}=\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix},\boldsymbol{b}=\begin{bmatrix}
1 \\
2
\end{bmatrix},c=1
\end{aligned}\end{split}\]</div>
<p>のように具体的な数値例で考えてみると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L} &amp;=
\boldsymbol{w}^{T}\boldsymbol{A}\boldsymbol{w}+\boldsymbol{b}^{T}\boldsymbol{w}+c\\
&amp;=
\begin{bmatrix}
w_{1} &amp; w_{2}
\end{bmatrix}\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}\begin{bmatrix}
w_{1} \\
w_{2}
\end{bmatrix}
+\begin{bmatrix}
1 &amp; 2
\end{bmatrix}\begin{bmatrix}
w_{1} \\
w_{2}
\end{bmatrix}+1\\
&amp;=
\begin{bmatrix}
w_{1} &amp; w_{2}
\end{bmatrix}
\begin{bmatrix}
w_{1}+2w_{2} \\
3w_{1}+4w_{2}
\end{bmatrix}+w_{1}+2w_{2}+1\\
&amp;=w_{1}\left( w_{1}+2w_{2}\right) +w_{1}\left( 3w_{1}+4w_{2}\right) +w _{1}+2w_{2}+1\\
&amp;=w^{2}_{1}+5w_{1}w_{2}+4w^{2}_{2}+w_{1}+2w_{2}+1 \\
\end{aligned}\end{split}\]</div>
<p>となり，<span class="math">\(w_{1}, w_{2}\)</span>に関してそれぞれまとめると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}
&amp;=w^{2}_{1}+\left( 5w_{2}+1\right) w_{1} +
\left( 4w^{2}_{2}+2w_{2}+1\right) \\
&amp;=4w^{2}_{2}+\left( 5w_{1}+2\right) w_{2}+\left( w^{2}_{1}+w_{1}+1\right) \end{aligned}\end{split}\]</div>
<p>のようにそれぞれの二次関数であることがわかります．ただし，<span class="math">\(w_{1}\)</span>
と <span class="math">\(w_{2}\)</span>
が<strong>独立である</strong>といった前提もありますが，最初から厳密な前提は数式が複雑になるため，ひとまず置いておくとしましょう．</p>
<p>そして，二次関数であれば，下図のような形となります．</p>
<div class="figure" id="id16">
<img alt="06.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/06.png" />
<p class="caption"><span class="caption-text">06.png</span></p>
</div>
<p>これを3次元でイメージすると，下図のようになります．</p>
<div class="figure" id="id17">
<img alt="08.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/08.png" />
<p class="caption"><span class="caption-text">08.png</span></p>
</div>
<p>そして，目的関数である二乗誤差の総和が最小となる点では各変数で微分した時の傾きが0となります．</p>
<div class="figure" id="id18">
<img alt="07.png" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/07.png" />
<p class="caption"><span class="caption-text">07.png</span></p>
</div>
<p>この例では，<span class="math">\(w_{1}\)</span> と <span class="math">\(w_{2}\)</span>
の２つのパラメータの場合で考えましたが，これは <span class="math">\(w_{0}\)</span>,
<span class="math">\(w_{1}\)</span>, <span class="math">\(w_{2}\)</span>, <span class="math">\(\ldots\)</span>, <span class="math">\(w_{M}\)</span>
の場合でも同様に考えることができ，目的関数が最小となる点は</p>
<div class="math">
\[\begin{split}\begin{cases}
\dfrac {\partial }{\partial w_{0}}\mathcal{L}=0\\
\dfrac {\partial }{\partial w_{1}}\mathcal{L}=0\\
\ \ \ \ \ \vdots \\
\dfrac {\partial }{\partial w_{M}}\mathcal{L}=0\\
\end{cases}\end{split}\]</div>
<p>となり，これをまとめると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\begin{bmatrix}
\dfrac {\partial}{\partial w_{0}} \mathcal{L} \\
\dfrac {\partial}{\partial w_{1}} \mathcal{L} \\
\vdots  \\
\dfrac {\partial}{\partial w_{M}} \mathcal{L} \\
\end{bmatrix}&amp;=\begin{bmatrix}
0 \\
0 \\
\vdots  \\
0 \\
\end{bmatrix} \\
\Rightarrow \dfrac {\partial}{\partial \boldsymbol{w}} \mathcal{L} &amp;= \boldsymbol{0} \\
\end{aligned}\end{split}\]</div>
<p>のようにベクトルでの微分として表されます．あとは，上式を満たすように
<span class="math">\(\boldsymbol{w}\)</span>
を決めていきます．（下記の計算にはベクトルでの微分をはじめとして，線形代数で学んだ内容を活用しているため，計算途中がわからなくなった場合には，線形代数のパートを確認しながら進めてみてください．）</p>
<div class="math">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial \boldsymbol{w}}\mathcal{L} =\dfrac {\partial }{\partial \boldsymbol{w}}\left( c+\boldsymbol{b}^{T}\boldsymbol{w}+\boldsymbol{w}^{T}\boldsymbol{A}\boldsymbol{w}\right)
= \boldsymbol{0}\\
\dfrac {\partial }{\partial \boldsymbol{w}}\left( c\right) +\dfrac {\partial }{\partial \boldsymbol{w}}\left( \boldsymbol{b}^{T}\boldsymbol{w}\right) +\dfrac {\partial }{\partial \boldsymbol{w}}\left( \boldsymbol{w}^{T}\boldsymbol{A}\boldsymbol{w}\right)
=\boldsymbol{0}\\
\boldsymbol{0}+\boldsymbol{b}+\left( \boldsymbol{A}+\boldsymbol{A}^{T}\right) \boldsymbol{w} =\boldsymbol{0}\\
-2\boldsymbol{X}^{T}\boldsymbol{t}+\left\{ \boldsymbol{X}^{T}\boldsymbol{X} + \left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{T}\right\} \boldsymbol{w}
=\boldsymbol{0}\\
-2\boldsymbol{X}^{T}\boldsymbol{t}+2\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}=\boldsymbol{0}\\
\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}=\boldsymbol{X}^{T}\boldsymbol{t}\\
\end{aligned}\end{split}\]</div>
<p>ここで，両辺に左側から
<span class="math">\(\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\)</span> をかけると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{X} \boldsymbol{w} =\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t} \\
\boldsymbol{I}\boldsymbol{w}=\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t} \\
\boldsymbol{w}=\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t}
\end{aligned}\end{split}\]</div>
<p>となり，与えられたデータセット <span class="math">\(\boldsymbol{X}, \boldsymbol{t}\)</span>
から，最適なパラメータ <span class="math">\(\boldsymbol{w}\)</span>
が求まりました．ここで，<span class="math">\(\boldsymbol{I}\)</span>
は単位行列です．また，式変形の際には，</p>
<div class="math">
\[\boldsymbol{w} = \dfrac{\boldsymbol{X}^{T}\boldsymbol{t}}{\boldsymbol{X}^{T}\boldsymbol{X}}\]</div>
<p>のような分数が表れないように注意してください．これは行列の計算には割り算がないためです．そのため，逆行列を使って行列積のみで計算しています．</p>
<p>また，もうひとつよくある間違いとして，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}&amp;=\boldsymbol{X}^{T}\boldsymbol{t}\\
\left( \boldsymbol{X}^{T}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{w}&amp;=\left( \boldsymbol{X}^{T}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t}\\
\boldsymbol{X}\boldsymbol{w}&amp;=\boldsymbol{t}\\
\boldsymbol{X}^{-1}\boldsymbol{X}\boldsymbol{w}&amp;=\boldsymbol{X}^{-1}\boldsymbol{t}\\
\boldsymbol{w}&amp;=\boldsymbol{X}^{-1}\boldsymbol{t}
\end{aligned}\end{split}\]</div>
<p>のような式変形をする場合もみられます．しかし，これは一般的には成立しません．その理由として，線形代数の章で説明した逆行列を持つための条件として，<strong>正方行列であること</strong>を満たしていないためです．バイアス
<span class="math">\(\boldsymbol{b}\)</span> を <span class="math">\(\boldsymbol{w}\)</span>
に包含することを無視する場合
<span class="math">\(\boldsymbol{X} \in \mathcal{R}^{N \times M}\)</span>
であり，バイアスの包含を考慮する場合は
<span class="math">\(\boldsymbol{X} \in \mathcal{R}^{N \times (M+1)}\)</span>
です．一般的に，サンプル数 <span class="math">\(N\)</span> と入力変数の数 <span class="math">\(M\)</span>
は等しくないため，<span class="math">\(\boldsymbol{X}\)</span>は正方行列ではなく，逆行列をもちません．それに対し，例えば，<span class="math">\(\boldsymbol{X} \in \mathcal{R}^{N \times M}\)</span>
の場合，<span class="math">\(\boldsymbol{X}^{T}\boldsymbol{X} \in \mathcal{R}^{M\times M}\)</span>
となり，サンプル数 <span class="math">\(N\)</span>
に依存することなく，常に正方行列となります．（逆行列が求まるためにはもう少し厳密な条件があるので，さらに詳しく知りたい方は各自調べてみてください．）</p>
<p>推論の際は学習で得られたパラメータ <span class="math">\(\boldsymbol{w}\)</span> を用いて，</p>
<div class="math">
\[y_{q} = \boldsymbol{w}^{T}\boldsymbol{x}_{q}\]</div>
<p>のように計算することで予測値が得られます．</p>
</div>
</div>
<div class="section" id="Numpyによる実装">
<h2>2.3. Numpyによる実装<a class="headerlink" href="#Numpyによる実装" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>それでは重回帰分析を例に，Pythonで線形代数を用いた実装を行っていきましょう．Pythonには<strong>Numpy</strong>と呼ばれる線形代数を簡単に扱えるライブラリが存在し，広く利用されています．次の章で紹介するChainerの中でもNumpyは多用されており，ディープラーニングを学ぶための第一歩として，まずはNumpyの使い方を習得することが重要です．</p>
<p>Pythonの文法に関しては把握していることを前提に進めています．具体的には，変数（数値・文字列，リスト，タプル，辞書），制御構文（for，if），関数，クラスを理解している必要があります．</p>
<p>重回帰分析では，最終的に最適なパラメータ <span class="math">\(\boldsymbol{w}\)</span> が</p>
<div class="math">
\[\boldsymbol{w}=\left( \boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{t}\]</div>
<p>で求まりました．この最適なパラメータを求めるために，以下の5つを扱います．</p>
<ul class="simple">
<li>ベクトルの定義</li>
<li>行列の定義</li>
<li>転置</li>
<li>行列積</li>
<li>逆行列</li>
</ul>
<p>具体的に，以下のようなデータセットが与えられているケースを想定してみましょう．</p>
<div class="math">
\[\begin{split}\boldsymbol{X} =
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
1 &amp; 2 &amp; 5 \\
1 &amp; 3 &amp; 4 \\
1 &amp; 5 &amp; 9
\end{bmatrix}, \
\boldsymbol{t} =
\begin{bmatrix}
1 \\ 5 \\ 6 \\ 8
\end{bmatrix}\end{split}\]</div>
<p>それぞれの実装について，見ていきましょう．まずは，Numpyの読み込みから始めます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>import numpy as np
</pre></div>
</div>
</div>
<p>ベクトルの定義は以下のように行います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>t = np.array([[1], [5], [6], [8]])
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(t)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1]
 [5]
 [6]
 [8]]
</pre></div></div>
</div>
<p>つぎに，行列の定義も行いましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>X = np.array([
    [1, 2, 3],
    [1, 2, 5],
    [1, 3, 4],
    [1, 5, 9]
])
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(X)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1 2 3]
 [1 2 5]
 [1 3 4]
 [1 5 9]]
</pre></div></div>
</div>
<p>ここで，Xの転置を行ってみましょう．Numpyの<code class="docutils literal"><span class="pre">array</span></code>で定義されている場合，<code class="docutils literal"><span class="pre">.T</span></code>をつけるだけで転置することができます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(X.T)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1 1 1 1]
 [2 2 3 5]
 [3 5 4 9]]
</pre></div></div>
</div>
<p>縦と横が入れ替わっていることを確認できます．</p>
<p>行列積は以下のように <code class="docutils literal"><span class="pre">np.dot</span></code>
によって実現できます．行列積を行う際には，一番目の行列の列数と，二番目の行列の行数が同じであることに注意して下さい．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>XX = np.dot(X.T, X)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(XX)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[  4  12  21]
 [ 12  42  73]
 [ 21  73 131]]
</pre></div></div>
</div>
<p>この逆行列を求めるには，<code class="docutils literal"><span class="pre">np.linalg.inv</span></code> を用います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>XX_inv = np.linalg.inv(XX)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(XX_inv)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[ 1.76530612 -0.39795918 -0.06122449]
 [-0.39795918  0.84693878 -0.40816327]
 [-0.06122449 -0.40816327  0.24489796]]
</pre></div></div>
</div>
<p>これで重回帰分析のために必要な演算が揃いました．最適なパラメータを求めると，</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>Xt = np.dot(X.T, t)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(Xt)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[ 20]
 [ 70]
 [124]]
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>w = np.dot(XX_inv, Xt)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(w)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[-0.14285714]
 [ 0.71428571]
 [ 0.57142857]]
</pre></div></div>
</div>
<p>このようにパラメータ <span class="math">\(\boldsymbol{w}\)</span>
が求まりました．Numpyを使うことで，数式をそのままプログラミング上で書くことができます．</p>
</div>
<div class="section" id="Scikit-learnによる本格的な実装">
<h2>2.4. Scikit-learnによる本格的な実装<a class="headerlink" href="#Scikit-learnによる本格的な実装" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>重回帰分析であればNumpyで簡単に実装することができましたが，実践的に使用する機械学習アルゴリズムの多くは複雑であり，初学者が一から書くには難しい場合も少なくありません．そこで，Pythonでは<strong>Scikit-learn</strong>と呼ばれる機械学習用のフレームワークが公開されており，初学者でも簡単に機械学習を扱うことができます．</p>
<p>まずは重回帰分析をScikit-learnによって実装してみましょう．</p>
<div class="section" id="Scikit-learn-基礎編">
<h3>2.4.1. Scikit-learn 基礎編<a class="headerlink" href="#Scikit-learn-基礎編" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>Scikit-learnは<code class="docutils literal"><span class="pre">sklearn</span></code>という名前で呼び出すことができます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>import sklearn
</pre></div>
</div>
</div>
<p>たとえば，重回帰分析を使用する場合は以下のように呼び出します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.linear_model import LinearRegression
</pre></div>
</div>
</div>
<p>なお，使い方を調べる際には，<a class="reference external" href="http://scikit-learn.org/">公式のリファレンス</a>に加えて，「重回帰分析
Scikit-learn」と検索して，実例のソースコードを見るほうが早い場合も多くあります．</p>
<p>重回帰分析のアルゴリズムはクラスとして定義されているので，インスタンス化を行います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [21]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>model = LinearRegression()
</pre></div>
</div>
</div>
<p>これだけで，重回帰分析を使用するための準備が完了です．パラメータの学習は以下のように行います．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [22]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>model.fit(X, t)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[22]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</pre></div>
</div>
</div>
<p>結果の検証は，次のように行います．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [23]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>model.score(X, t)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[23]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.6923076923076926
</pre></div>
</div>
</div>
<p>分類の場合は<strong>精度</strong>，回帰の場合は<strong>決定係数</strong>とよばれる指標が自動的に計算されるようになっています．
このように，Scikit-learnでは，簡単なインターフェースでやり取りができるようになっています．Scikit-learnの良い点は，最初にアルゴリズムを決めてしまえば，どのアルゴリズムでも，<code class="docutils literal"><span class="pre">.fit</span></code>で学習，<code class="docutils literal"><span class="pre">.score</span></code>で検証が行える点です．</p>
<p>また，アルゴリズムによって内容は多少異なりますが，パラメータもインスタンス変数として格納されているため，学習後に確認することができます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [24]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># パラメータw
model.coef_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[24]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>array([[0.        , 0.71428571, 0.57142857]])
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [26]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># バイアスb
model.intercept_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[26]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>array([-0.14285714])
</pre></div>
</div>
</div>
<p>この例からわかるように，Scikit-learnでは，パラメータとバイアスがそれぞれ準備されているため，入力変数
<span class="math">\(\boldsymbol{X}\)</span>
の左端の列に1を格納した変数を入れる必要がありません．</p>
</div>
<div class="section" id="Scikit-learn-応用編">
<h3>2.4.2. Scikit-learn 応用編<a class="headerlink" href="#Scikit-learn-応用編" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>Scikit-learnは機械学習の実装を支援する多くの機能を兼ね備えています．</p>
<p>まず最初にサンプルのデータセットの取り扱いを紹介します．Scikit-learnにサンプルのデータセットがいくつか提供されています．今回はその中から，<code class="docutils literal"><span class="pre">load_boston</span></code>というボストン近郊の家賃に関するデータセットを使用しましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [48]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.datasets import load_boston
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [49]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>boston = load_boston()
</pre></div>
</div>
</div>
<p>変数の<code class="docutils literal"><span class="pre">boston</span></code>には辞書と同じ形式で格納されており，変数の中身を見ながら入力変数と教師データに対応するものを見つけていきます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [51]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>X = boston[&#39;data&#39;]
t = boston[&#39;target&#39;]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [77]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(X)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]
 [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]
 [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]
 ...
 [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]
 [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]
 [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [78]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(t)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4
 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8
 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6
 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4
 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9
 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9
 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7
 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8
 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4
 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8
 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4
 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8
 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2
 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.
 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.
 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1
 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5
 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8
 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8
 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1
 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9
 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2
 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1
 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1
 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6
 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8
 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3
 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2
  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.
 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4
 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3
 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6
 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7
 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3
 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.
  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9
 22.  11.9]
</pre></div></div>
</div>
<p>Numpyの形式で入力変数と教師データが格納されており，<code class="docutils literal"><span class="pre">.shape</span></code>を使うことで行と列の数を確認できます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [79]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>X.shape
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[79]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>(506, 13)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [80]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>t.shape
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[80]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>(506,)
</pre></div>
</div>
</div>
<p>つぎに，<strong>訓練データ</strong>と<strong>検証データ</strong>の分割です．学習の時と同じデータを使って性能を検証した場合，モデルが学習データだけをうまくモデル化し，同じような分布からとられた未知のデータはうまくいかない場合があります．これを<strong>過学習</strong>とよびます．機械学習ではこれを防ぐために学習データと別に性能を評価する検証データを分けて評価します．このように分割して検証することを<strong>ホールドアウト法</strong>とよびます．</p>
<p>Scikit-learnでは訓練用と検証用を分割する機能が用意されています．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [81]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.model_selection import train_test_split
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [82]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [83]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>X_train.shape
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[83]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>(354, 13)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [84]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>X_test.shape
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[84]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>(152, 13)
</pre></div>
</div>
</div>
<p>引数の<code class="docutils literal"><span class="pre">test_size</span></code>は検証用に使うデータの比率であり，0.3と指定すると全体の30%が検証データとなります．また，<code class="docutils literal"><span class="pre">random_state</span></code>は乱数のシードであり，再現性を確保するためのものです．なぜ乱数が登場するかというと，前から70%を訓練用，残りを検証用とするのではなく，全体からランダムに選択した70%を訓練用，残り30%を検証用と選択しているためです．</p>
<p>訓練データを用いて学習します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [85]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>model = LinearRegression()
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [86]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>model.fit(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[86]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</pre></div>
</div>
</div>
<p>検証を行う場合は，訓練データと検証データの両方に対してチェックしておくと良いでしょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [87]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 訓練データ
model.score(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[87]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.7644563391821222
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [88]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証データ
model.score(X_test, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[88]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.673528086534723
</pre></div>
</div>
</div>
<p>検証データだけでなく，訓練データでも検証することで学習に失敗している場合の問題を切り分けることができます．</p>
<table border="1" class="docutils">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">訓練データ</th>
<th class="head">検証データ</th>
<th class="head">結果</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>×</td>
<td>×</td>
<td>アンダーフィッティング</td>
</tr>
<tr class="row-odd"><td>〇</td>
<td>×</td>
<td>オーバーフィッティング</td>
</tr>
<tr class="row-even"><td>〇</td>
<td>〇</td>
<td>ＯＫ</td>
</tr>
</tbody>
</table>
<p><strong>アンダーフィッティング</strong>の場合，現状の機械学習アルゴリズムでうまくデータの特徴を捉えられていないと考えられ，アルゴリズムを変更したり，入力データの特徴をより適切に表現できるような変換を考えます．逆に<strong>オーバーフィッティング</strong>の場合，アルゴリズムでデータの特徴をある程度捉えられていることは確認できているので，モデルが過学習しないように対策していきます．代表的な方法として，<strong>ハイパーパラメータ</strong>と呼ばれる各アルゴリズムのパラメータ学習に使われるパラメータ値を調整していくことで解決できる場合があります．このように，望ましい結果が得られない中にも，それぞれの状況を把握することで次に打つべき対策が変わってくるため，訓練データと検証データの両方に対する検証を行うことは重要であることが分かります．</p>
<p>また，Scikit-learnでは，スケーリングも行うことができます．例えば，平均0，標準偏差1に変換するデータ正規化を行う場合の手順は以下の通りです．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [89]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.preprocessing import StandardScaler
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [90]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># インスタンス化
scaler = StandardScaler()
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [91]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 平均の分散（標準偏差）を学習
scaler.fit(X_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[91]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>StandardScaler(copy=True, with_mean=True, with_std=True)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [92]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 変換
X_train_s = scaler.transform(X_train)
X_test_s  = scaler.transform(X_test)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [93]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(X_train_s)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[-0.20416267 -0.49997924  1.54801583 ...  1.2272573   0.42454294
   3.10807269]
 [-0.38584317  0.34677427 -0.58974728 ...  0.05696346  0.40185312
  -0.66643035]
 [-0.33266283 -0.49997924  1.54801583 ...  1.2272573   0.39846135
   0.63936662]
 ...
 [-0.38147768 -0.49997924 -0.15303077 ... -0.30312696  0.39659002
  -0.30284441]
 [-0.3720831  -0.49997924 -0.59690657 ... -0.25811566  0.37588849
   0.89967717]
 [-0.38289844 -0.49997924 -1.00641779 ... -0.84326258  0.42454294
   0.31822262]]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [94]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(X_test_s)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[-0.39152624 -0.49997924 -1.12239824 ... -0.70822867  0.17086147
  -0.72160487]
 [ 0.70825498 -0.49997924  1.00534187 ...  0.77714428  0.0648977
  -0.41177872]
 [-0.38588517 -0.49997924  0.4025299  ... -0.93328518  0.38758427
  -0.27454978]
 ...
 [ 1.6177735  -0.49997924  1.00534187 ...  0.77714428  0.42454294
   2.59876943]
 [-0.34043865 -0.49997924 -0.1687812  ... -0.03305915  0.42454294
  -1.11772962]
 [-0.39601293 -0.49997924 -1.27417512 ...  0.10197476  0.39202867
  -1.02294263]]
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="実用的な機械学習アルゴリズムの紹介">
<h2>2.5. 実用的な機械学習アルゴリズムの紹介<a class="headerlink" href="#実用的な機械学習アルゴリズムの紹介" title="このヘッドラインへのパーマリンク">¶</a></h2>
<p>ここからは，実践的に活用されている機械学習アルゴリズムについて特徴とともに紹介していきます．ここでは概要の紹介のみ行うため，気になった場合は参考図書を見て学びを深めてください．</p>
<p>Scikit-learnを使うことによって実装は非常に手軽に行うことができますが，数式を理解していないがゆえに，うまくいかないときの対処法がわからないという問題も出てきます．ここでは，この問題につまずかないように，ハイパーパラメータのチューニングも併せて紹介していきます．</p>
<div class="section" id="Support-Vector-Machine-(SVM)">
<h3>2.5.1. Support Vector Machine (SVM)<a class="headerlink" href="#Support-Vector-Machine-(SVM)" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>SVMは実用的によく使われる手法の一つであり，入出力間の非線形性を捉えることができます．ただし，非線形なモデルの場合，<span class="math">\(y = wx^2\)</span>や，<span class="math">\(y = w\sin(x)\)</span>やその重ね合わせといったように，組み合わせの候補が無限に存在するため，モデルの設計が難しくなります．事前にある程度，入出力間の関係性が把握できていれば定式化のアイディアも存在しますが，そのような事前知識があるような状況は，そう多くはありません．</p>
<p>そこで，SVMでは<strong>カーネルトリック</strong>と呼ばれるテクニックを利用して，データ間の類似度（カーネル）から入出力間の非線形性の定式化を行います．</p>
<p>SVMには連続値を予測する<strong>回帰 (Regression)</strong>
とカテゴリを予測する<strong>分類 (Classification)</strong>
の両方に対応した手法があります．それぞれ，<strong>，Support Vector
Regression (SVR)</strong> と <strong>Support Vector Classification (SVC)</strong>
と呼びます．まずは回帰の問題設定で紹介し，前回と同様，ボストン近郊の家賃予測の例題を取り扱います．</p>
<div class="section" id="Support-Vector-Regression-(SVR)">
<h4>2.5.1.1. Support Vector Regression (SVR)<a class="headerlink" href="#Support-Vector-Regression-(SVR)" title="このヘッドラインへのパーマリンク">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [103]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># データの準備
from sklearn.datasets import load_boston

boston = load_boston()
X = boston[&#39;data&#39;]
t = boston[&#39;target&#39;]
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [104]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 訓練と検証データの分割
from sklearn.model_selection import train_test_split

X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [105]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># モデルのインスタンス化，学習
from sklearn.svm import SVR

model = SVR()
model.fit(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[105]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto&#39;,
  kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [106]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[106]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.14680479454958428
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [107]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[107]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.01018093344367077
</pre></div>
</div>
</div>
<p>このようにSVRも，Scikit-learnを用いることで重回帰分析のケースとほとんど同じように実装できます．</p>
<p>しかし結果をみると，重回帰分析に比べて良くなっているとは言えません．ハイパーパラメータ調整が必要そうですが，その前に，スケーリングを行うことで改善できる場合もあります．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [109]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

X_train_s = scaler.transform(X_train)
X_test_s = scaler.transform(X_test)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [110]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># スケーリング後のデータを使って学習
model.fit(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[110]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto&#39;,
  kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [111]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[111]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.697669153907031
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [112]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test_s, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[112]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.5540391127752358
</pre></div>
</div>
</div>
<p>スケーリングにより，大幅に結果を改善することができました．機械学習アルゴリズムには，スケーリングの影響を受けやすいものとそうでないものが存在し，SVRを含むSVMは影響を受けやすいアルゴリズムといえます．</p>
<p>最後に，さらに精度を向上させるためにハイパーパラメータの調整を行います．ただしその際，注意点があります．それは，訓練データ（train）はパラメータの調整に用いますが，検証データ（test）を見ながらハイパーパラーメータの調整を行ってはいけないということです．検証データはあくまで未知のデータに対する性能検証を行うために用意したものであり，ハイパーパラメータの調整に使用してしまうと検証データの情報が学習にリークして，正しい検証ができなくなります．</p>
<p>そこで，ハイパーパラメータの調整用に<strong>バリデーションデータ</strong>（val）を追加することが一般的です．</p>
<p><img alt="image0" src="../_images/09.png" /></p>
<p>また，<strong>交差検証法（クロスバリデーション）</strong>という手法もあります．これは下図に示すように，データに対して複数パターンの分割を行い，それらの性能の平均を評価する手法です．</p>
<p><img alt="image1" src="../_images/10.png" /></p>
<p>この分割数 <span class="math">\(K\)</span> としたものは特に，<strong>K-fold Cross Validation
(CV)</strong>と呼ばれます．上記の例だと <span class="math">\(K=3\)</span> です．</p>
<p>それでは，SVRのハイパーパラメータ調整を交差検証法も使いながら行っていきます．Scikit-learnではハイパーパラメータ調整のための機能も<code class="docutils literal"><span class="pre">GridSearchCV</span></code>という名前で用意されています．<strong>グリッドサーチ</strong>とは各組合せをすべて試す探索方法です．それ以外にも，<strong>ランダムサーチ</strong>や<strong>ベイズ最適化</strong>による探索などの方法もあります．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [120]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.model_selection import GridSearchCV
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [121]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 調整を行うハイパーパラメータの値の候補
param_grid = [
    {&#39;C&#39;: [1, 10, 100], &#39;gamma&#39;: [0.01, 0.1, 1, 10]}
]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [122]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 交差検証法を使用したハイパーパラメータの各組合せでの学習
model_cv = GridSearchCV(SVR(), param_grid, cv=3, scoring=&#39;neg_mean_squared_error&#39;)
model_cv.fit(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[122]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>GridSearchCV(cv=3, error_score=&#39;raise&#39;,
       estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto&#39;,
  kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False),
       fit_params=None, iid=True, n_jobs=1,
       param_grid=[{&#39;C&#39;: [1, 10, 100], &#39;gamma&#39;: [0.01, 0.1, 1, 10]}],
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,
       scoring=&#39;neg_mean_squared_error&#39;, verbose=0)
</pre></div>
</div>
</div>
<p>交差検証法とハイパーパラメータのグリッドサーチはこれで完了です．各ハイパーパラメータでの結果も確認することができ，最も結果の良かったハイパーパラメータの値を引き継いだモデルの選択もできます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [126]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 結果の確認 (valの対する結果)
model_cv.grid_scores_
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
C:\Users\ryosu\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[126]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>[mean: -40.88957, std: 12.03388, params: {&#39;C&#39;: 1, &#39;gamma&#39;: 0.01},
 mean: -34.94548, std: 12.18057, params: {&#39;C&#39;: 1, &#39;gamma&#39;: 0.1},
 mean: -72.62060, std: 15.99632, params: {&#39;C&#39;: 1, &#39;gamma&#39;: 1},
 mean: -86.25200, std: 16.38372, params: {&#39;C&#39;: 1, &#39;gamma&#39;: 10},
 mean: -17.67763, std: 6.48783, params: {&#39;C&#39;: 10, &#39;gamma&#39;: 0.01},
 mean: -16.46703, std: 7.03969, params: {&#39;C&#39;: 10, &#39;gamma&#39;: 0.1},
 mean: -43.71719, std: 13.22953, params: {&#39;C&#39;: 10, &#39;gamma&#39;: 1},
 mean: -81.13324, std: 15.21847, params: {&#39;C&#39;: 10, &#39;gamma&#39;: 10},
 mean: -13.83363, std: 3.54540, params: {&#39;C&#39;: 100, &#39;gamma&#39;: 0.01},
 mean: -14.61609, std: 7.20850, params: {&#39;C&#39;: 100, &#39;gamma&#39;: 0.1},
 mean: -37.47299, std: 9.87515, params: {&#39;C&#39;: 100, &#39;gamma&#39;: 1},
 mean: -77.95797, std: 12.36436, params: {&#39;C&#39;: 100, &#39;gamma&#39;: 10}]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [161]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 最も結果が良かったハイパーパラメータ
model_cv.best_params_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[161]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>{&#39;C&#39;: 10, &#39;gamma&#39;: 0.01}
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [129]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 最も結果が良かったハイパーパラメータの値を設定したモデル
model = model_cv.best_estimator_
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [130]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test_s, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[130]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.7685336670918761
</pre></div>
</div>
</div>
<p>ここまでがアルゴリズムの実践的な調整です．実際には特徴量の選択や外れ値除去など前処理も込みで行うため，ここまでシンプルに完了できるものではありませんが，まずはこの流れを覚えてください．</p>
<ol class="arabic simple">
<li>スケーリング無　score:0.010</li>
<li>スケーリング有　score:0.554</li>
<li>スケーリング＋ハイパーパラメータの調整有　0.7685</li>
</ol>
</div>
<div class="section" id="Support-Vector-Classification-(SVC)">
<h4>2.5.1.2. Support Vector Classification (SVC)<a class="headerlink" href="#Support-Vector-Classification-(SVC)" title="このヘッドラインへのパーマリンク">¶</a></h4>
<p>次に，SVMの分類であるSVCも同様にスケーリングからハイパーパラメータの調整まで行ってみます．ここでは分類の例題として，Scikit-learnで用意されているデータセットの中から，乳がんの患者か否かを予測する問題を扱います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [134]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># データセットの準備
from sklearn.datasets import load_breast_cancer

breast_cancer = load_breast_cancer()
X = breast_cancer[&#39;data&#39;]
t = breast_cancer[&#39;target&#39;]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [137]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>X.shape
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[137]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>(569, 30)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [135]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(X)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]
 [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]
 [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]
 ...
 [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]
 [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]
 [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [136]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(t)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0
 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1
 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1
 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0
 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1
 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1
 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1
 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0
 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1
 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1
 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1
 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0
 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 0 0 0 0 0 0 1]
</pre></div></div>
</div>
<p>入力変数のスケールは統一されていないことがわかります．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [138]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 訓練データと検証データに分割
from sklearn.model_selection import train_test_split

X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [139]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># スケーリング無で学習
from sklearn.svm import SVC

model = SVC()
model.fit(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[139]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [140]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[140]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>1.0
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [141]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_test, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[141]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.631578947368421
</pre></div>
</div>
</div>
<p>分類では精度 (Accuracy)
と呼ばれる指標の結果が得られます．例えば，100問中3問間違えると，精度は0.97となります．</p>
<p>次にスケーリングを行った後に学習させていきましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [144]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># スケーリング
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

X_train_s = scaler.transform(X_train)
X_test_s  = scaler.transform(X_test)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [148]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># スケーリングしたデータを用いて学習
model.fit(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[148]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [149]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[149]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9824120603015075
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [150]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test_s, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[150]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9766081871345029
</pre></div>
</div>
</div>
<p>このように精度が大幅に向上したことがわかります．</p>
<p>最後にハイパーパラメータのチューニングも行っていきましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [151]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.model_selection import GridSearchCV
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [152]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 調整を行うハイパーパラメータの値の候補
param_grid = [
    {&#39;C&#39;: [1, 10, 100], &#39;gamma&#39;: [0.01, 0.1, 1, 10]}
]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [156]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 交差検証法を使用したハイパーパラメータの各組合せでの学習
model_cv = GridSearchCV(SVC(), param_grid, cv=3)
model_cv.fit(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[156]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>GridSearchCV(cv=3, error_score=&#39;raise&#39;,
       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False),
       fit_params=None, iid=True, n_jobs=1,
       param_grid=[{&#39;C&#39;: [1, 10, 100], &#39;gamma&#39;: [0.01, 0.1, 1, 10]}],
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,
       scoring=None, verbose=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [157]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 結果の確認 (valの対する結果)
model_cv.grid_scores_
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
C:\Users\ryosu\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[157]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>[mean: 0.96482, std: 0.01272, params: {&#39;C&#39;: 1, &#39;gamma&#39;: 0.01},
 mean: 0.95226, std: 0.01543, params: {&#39;C&#39;: 1, &#39;gamma&#39;: 0.1},
 mean: 0.62814, std: 0.00310, params: {&#39;C&#39;: 1, &#39;gamma&#39;: 1},
 mean: 0.62563, std: 0.00223, params: {&#39;C&#39;: 1, &#39;gamma&#39;: 10},
 mean: 0.97487, std: 0.01972, params: {&#39;C&#39;: 10, &#39;gamma&#39;: 0.01},
 mean: 0.94472, std: 0.02474, params: {&#39;C&#39;: 10, &#39;gamma&#39;: 0.1},
 mean: 0.63065, std: 0.00132, params: {&#39;C&#39;: 10, &#39;gamma&#39;: 1},
 mean: 0.62563, std: 0.00223, params: {&#39;C&#39;: 10, &#39;gamma&#39;: 10},
 mean: 0.94975, std: 0.01981, params: {&#39;C&#39;: 100, &#39;gamma&#39;: 0.01},
 mean: 0.94472, std: 0.02474, params: {&#39;C&#39;: 100, &#39;gamma&#39;: 0.1},
 mean: 0.63065, std: 0.00132, params: {&#39;C&#39;: 100, &#39;gamma&#39;: 1},
 mean: 0.62563, std: 0.00223, params: {&#39;C&#39;: 100, &#39;gamma&#39;: 10}]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [162]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 最も結果が良かったハイパーパラメータ
model_cv.best_params_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[162]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>{&#39;C&#39;: 10, &#39;gamma&#39;: 0.01}
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [163]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 最も結果が良かったハイパーパラメータの値を設定したモデル
model = model_cv.best_estimator_
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [164]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test_s, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[164]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9883040935672515
</pre></div>
</div>
</div>
<p>ハイパーパラメータの調整により，多少ですが精度を改善することができました．</p>
</div>
</div>
<div class="section" id="Random-Forest">
<h3>2.5.2. Random Forest<a class="headerlink" href="#Random-Forest" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>続いて紹介するのは，決定木 (Dicision Tree)
のアンサンブル学習であるランダムフォレストです．こちらも実用上良く使われる手法であり，ランダムフォレストを含めた決定木系の手法には，入力変数のスケールの違いによる影響をあまり受けないという特徴があります．また，<strong>カテゴリカル変数</strong>と呼ばれる定量評価を行うことが難しい変数（例えば，男性
or
女性）も定量化を気にすることなく扱うことができるメリットがあります．回帰，分類のどちらも用意されているため，それぞれについて紹介していきましょう．</p>
<div class="section" id="回帰-(Regression)">
<h4>2.5.2.1. 回帰 (Regression)<a class="headerlink" href="#回帰-(Regression)" title="このヘッドラインへのパーマリンク">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [214]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># データの準備
from sklearn.datasets import load_boston

boston = load_boston()
X = boston[&#39;data&#39;]
t = boston[&#39;target&#39;]
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [215]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 訓練と検証データの分割
from sklearn.model_selection import train_test_split

X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [216]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># モデルのインスタンス化，学習
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[216]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None,
           max_features=&#39;auto&#39;, max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
           oob_score=False, random_state=None, verbose=0, warm_start=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [217]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[217]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9683509759630142
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [218]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[218]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.8244110898822086
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [219]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

X_train_s = scaler.transform(X_train)
X_test_s = scaler.transform(X_test)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [220]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># スケーリング後のデータを使って学習
model.fit(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[220]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None,
           max_features=&#39;auto&#39;, max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
           oob_score=False, random_state=None, verbose=0, warm_start=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [221]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[221]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.980773304078463
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [222]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test_s, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[222]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.7568763837538154
</pre></div>
</div>
</div>
<p>このように，スケーリングによる影響はほとんど受けないことを，実際に確認できました．</p>
<p>またランダムフォレストでは，条件分岐させる数である <code class="docutils literal"><span class="pre">max_depth</span></code>
をハイパーパラメータとして調整することが多いため，今回はこちらを調整していきましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.model_selection import GridSearchCV
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [224]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 調整を行うハイパーパラメータの値の候補
param_grid = [
    {&#39;max_depth&#39;: [1, 2, 3, 4, 5, 6]}
]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [225]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 交差検証法を使用したハイパーパラメータの各組み合わせでの学習
model_cv = GridSearchCV(RandomForestRegressor(), param_grid, cv=3, scoring=&#39;neg_mean_squared_error&#39;)
model_cv.fit(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[225]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>GridSearchCV(cv=3, error_score=&#39;raise&#39;,
       estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None,
           max_features=&#39;auto&#39;, max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
           oob_score=False, random_state=None, verbose=0, warm_start=False),
       fit_params=None, iid=True, n_jobs=1,
       param_grid=[{&#39;max_depth&#39;: [1, 2, 3, 4, 5, 6]}],
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,
       scoring=&#39;neg_mean_squared_error&#39;, verbose=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [226]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 結果の確認 (valの対する結果)
model_cv.grid_scores_
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
C:\Users\ryosu\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[226]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>[mean: -41.89307, std: 9.40057, params: {&#39;max_depth&#39;: 1},
 mean: -25.95305, std: 8.05238, params: {&#39;max_depth&#39;: 2},
 mean: -23.11041, std: 4.68079, params: {&#39;max_depth&#39;: 3},
 mean: -17.92487, std: 4.42161, params: {&#39;max_depth&#39;: 4},
 mean: -19.30415, std: 7.51230, params: {&#39;max_depth&#39;: 5},
 mean: -17.16534, std: 8.32303, params: {&#39;max_depth&#39;: 6}]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [227]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 最も結果が良かったハイパーパラメータ
model_cv.best_params_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[227]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>{&#39;max_depth&#39;: 6}
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [228]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 最も結果が良かったハイパーパラメータの値を設定したモデル
model = model_cv.best_estimator_
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [229]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test_s, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[229]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.8065343207878718
</pre></div>
</div>
</div>
<p>今回はもともとオーバーフィッティングしていなかったため，ハイパーパラメータの調整によって改善することはありませんでしたが，オーバーフィッティングしている場合には有効な施策となります．</p>
<p>またランダムフォレストの大きなメリットとして，各入力変数がどの程度重要であるかを定量評価した値が得られます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [230]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 各入力変数の重要度
model.feature_importances_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[230]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>array([0.03498355, 0.00122218, 0.00721576, 0.00092089, 0.00898805,
       0.39898526, 0.01894469, 0.04598164, 0.00571163, 0.01607173,
       0.01889314, 0.00701622, 0.43506524])
</pre></div>
</div>
</div>
<p>総和が1になっており，この値を使って各入力変数がどれだけ重要か判断することができます．</p>
</div>
<div class="section" id="分類-(Classification)">
<h4>2.5.2.2. 分類 (Classification)<a class="headerlink" href="#分類-(Classification)" title="このヘッドラインへのパーマリンク">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [231]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># データの準備
from sklearn.datasets import load_boston

breast_cancer = load_breast_cancer()
X = breast_cancer[&#39;data&#39;]
t = breast_cancer[&#39;target&#39;]
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [232]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 訓練と検証データの分割
from sklearn.model_selection import train_test_split

X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [233]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># モデルのインスタンス化，学習
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[233]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [234]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[234]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9949748743718593
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [235]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[235]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9415204678362573
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [236]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

X_train_s = scaler.transform(X_train)
X_test_s = scaler.transform(X_test)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [237]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># スケーリング後のデータを使って学習
model.fit(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[237]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [238]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[238]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>1.0
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [239]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test_s, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[239]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9473684210526315
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [240]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.model_selection import GridSearchCV
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [241]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 調整を行うハイパーパラメータの値の候補
param_grid = [
    {&#39;max_depth&#39;: [1, 2, 3, 4, 5, 6]}
]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [242]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 交差検証法を使用したハイパーパラメータの各組合せでの学習
model_cv = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)
model_cv.fit(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[242]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>GridSearchCV(cv=3, error_score=&#39;raise&#39;,
       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False),
       fit_params=None, iid=True, n_jobs=1,
       param_grid=[{&#39;max_depth&#39;: [1, 2, 3, 4, 5, 6]}],
       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,
       scoring=None, verbose=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [243]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 結果の確認 (valの対する結果)
model_cv.grid_scores_
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
C:\Users\ryosu\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[243]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>[mean: 0.91206, std: 0.01259, params: {&#39;max_depth&#39;: 1},
 mean: 0.93467, std: 0.01763, params: {&#39;max_depth&#39;: 2},
 mean: 0.93467, std: 0.01265, params: {&#39;max_depth&#39;: 3},
 mean: 0.95226, std: 0.01543, params: {&#39;max_depth&#39;: 4},
 mean: 0.93970, std: 0.02119, params: {&#39;max_depth&#39;: 5},
 mean: 0.93970, std: 0.01632, params: {&#39;max_depth&#39;: 6}]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [244]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 最も結果が良かったハイパーパラメータ
model_cv.best_params_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[244]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>{&#39;max_depth&#39;: 4}
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [245]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 最も結果が良かったハイパーパラメータの値を設定したモデル
model = model_cv.best_estimator_
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [246]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test_s, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[246]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9590643274853801
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [247]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 各入力変数の重要度
model.feature_importances_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[247]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>array([4.55728165e-03, 4.80674523e-03, 7.04751333e-02, 6.67175039e-02,
       0.00000000e+00, 6.38391868e-03, 1.44783793e-01, 2.56851226e-02,
       1.01592678e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
       7.17633429e-03, 3.57428435e-02, 5.48903143e-05, 7.63635594e-03,
       8.65354259e-03, 3.50041088e-03, 4.29368206e-03, 4.42019691e-03,
       8.32478506e-02, 1.93259510e-02, 1.79759936e-01, 8.96864960e-02,
       4.76915506e-03, 2.20744844e-02, 6.78042092e-02, 1.24987366e-01,
       4.10480850e-03, 8.33606129e-03])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="ロジスティック回帰">
<h3>2.5.3. ロジスティック回帰<a class="headerlink" href="#ロジスティック回帰" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>シンプルですが，良く利用される手法のひとつです．回帰という名前がついていますが，問題設定としては分類に属する点に注意しましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [273]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># データの準備
from sklearn.datasets import load_boston

breast_cancer = load_breast_cancer()
X = breast_cancer[&#39;data&#39;]
t = breast_cancer[&#39;target&#39;]
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [274]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 訓練と検証データの分割
from sklearn.model_selection import train_test_split

X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [278]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># モデルのインスタンス化，学習
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[278]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1,
          penalty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001,
          verbose=0, warm_start=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [279]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_train, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[279]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.957286432160804
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [280]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[280]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9649122807017544
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [281]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

X_train_s = scaler.transform(X_train)
X_test_s = scaler.transform(X_test)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [282]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># スケーリング後のデータを使って学習
model.fit(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[282]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1,
          penalty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001,
          verbose=0, warm_start=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [283]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（訓練データ）
model.score(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[283]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9899497487437185
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [284]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test_s, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[284]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9766081871345029
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [285]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.model_selection import GridSearchCV
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [286]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 調整を行うハイパーパラメータの値の候補
param_grid = [
    {&#39;C&#39;: [0.01, 0.1, 1, 10]}
]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [287]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 交差検証法を使用したハイパーパラメータの各組合せでの学習
model_cv = GridSearchCV(LogisticRegression(), param_grid, cv=3)
model_cv.fit(X_train_s, t_train)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[287]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>GridSearchCV(cv=3, error_score=&#39;raise&#39;,
       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1,
          penalty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001,
          verbose=0, warm_start=False),
       fit_params=None, iid=True, n_jobs=1,
       param_grid=[{&#39;C&#39;: [0.01, 0.1, 1, 10]}], pre_dispatch=&#39;2*n_jobs&#39;,
       refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=0)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [288]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 結果の確認 (valの対する結果)
model_cv.grid_scores_
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
C:\Users\ryosu\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[288]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>[mean: 0.96985, std: 0.01223, params: {&#39;C&#39;: 0.01},
 mean: 0.97990, std: 0.00935, params: {&#39;C&#39;: 0.1},
 mean: 0.98492, std: 0.01624, params: {&#39;C&#39;: 1},
 mean: 0.97236, std: 0.02323, params: {&#39;C&#39;: 10}]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [289]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 最も結果が良かったハイパーパラメータ
model_cv.best_params_
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[289]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>{&#39;C&#39;: 1}
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [290]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 最も結果が良かったハイパーパラメータの値を設定したモデル
model = model_cv.best_estimator_
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [291]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 検証（検証データ）
model.score(X_test_s, t_test)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[291]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.9766081871345029
</pre></div>
</div>
</div>
<p>新しいサンプルに対する推論を行ってみましょう．Scikit-learnでは，推論には<code class="docutils literal"><span class="pre">predict</span></code>を使用します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [297]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 訓練データの一番最初のサンプルで試しに推論
x_pred = [X_train_s[0]]
y = model.predict(x_pred)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [298]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(y)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[1]
</pre></div></div>
</div>
<p>また，各カテゴリに属する確率まで求める際には， <code class="docutils literal"><span class="pre">predict_proba</span></code>
を使用します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [299]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>y = model.predict_proba(x_pred)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [300]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(y)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[0.00160119 0.99839881]]
</pre></div></div>
</div>
<p>結果からわかる通り，各カテゴリの確率の総和が1となっており，確率が大きいほうのカテゴリ1が選ばれたことがわかります．正常か異常かといった分類の場合，正常or異常だけでなく，どのくらい異常そうであるかの確率までわかることで，閾値を設けやすくなります．この特性は次の章で紹介するニューラルネットワークでも同じです．</p>
</div>
<div class="section" id="k-means">
<h3>2.5.4. k-means<a class="headerlink" href="#k-means" title="このヘッドラインへのパーマリンク">¶</a></h3>
<p>最後に，教師なし学習である<strong>クラスタリング</strong>手法として有名なk-meansを紹介します．分類では教師データとしてどのカテゴリに属しているかがわかっていましたが，クラスタリングではその教師データがない状況で学習を行います．距離的に近いものをまとめるといった特性を持っています．</p>
<p>例題では2つのクラスターからなるデータをあらかじめ用意しておき，正しく分けられるかを確認していきましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [336]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>np.random.seed(0)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [337]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>X1 = np.random.randn(50, 2) - 3
X2 = np.random.randn(50, 2) + 3
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [338]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span># 結合
X = np.r_[X1, X2]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [339]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>X.shape
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[339]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>(100, 2)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [340]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>%matplotlib inline
import matplotlib.pyplot as plt
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [341]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>plt.scatter(X[:, 0], X[:, 1])
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[341]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>&lt;matplotlib.collections.PathCollection at 0x28640957438&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_ML_libs_184_1.png" src="../_images/notebooks_Introduction_to_ML_libs_184_1.png" />
</div>
</div>
<p>それでは，k-meansを用いてクラスタリングを行いましょう．クラスタリングでは分けるクラスターの数がハイパーパラメータとして必要であることが一般的であり，<code class="docutils literal"><span class="pre">n_clusters</span></code>で指定します．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [344]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>from sklearn.cluster import KMeans
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [347]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>model = KMeans(n_clusters=2)
model.fit(X)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[347]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300,
    n_clusters=2, n_init=10, n_jobs=1, precompute_distances=&#39;auto&#39;,
    random_state=None, tol=0.0001, verbose=0)
</pre></div>
</div>
</div>
<p>学習したモデルをもとにクラスタリングを行いましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [350]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>y = model.predict(X)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [351]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>print(y)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
</pre></div></div>
</div>
<p>Numpyを使って条件に当てはまるサンプルだけを抽出し，結果を可視化してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [354]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>X0 = X[y==0]
X1 = X[y==1]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [356]:
</pre></div>
</div>
<div class="input_area highlight-none"><div class="highlight"><pre>
<span></span>plt.scatter(X0[:, 0], X0[:, 1], color=&#39;red&#39;)
plt.scatter(X1[:, 0], X1[:,1 ], color=&#39;blue&#39;)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[356]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>&lt;matplotlib.collections.PathCollection at 0x28640b6c8d0&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Introduction_to_ML_libs_193_1.png" src="../_images/notebooks_Introduction_to_ML_libs_193_1.png" />
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Introduction_to_Neural_Network.html" class="btn btn-neutral float-right" title="3. ニューラルネットワークの基礎" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Basic_Math_for_ML.html" class="btn btn-neutral" title="1. 機械学習に必要な数学の基礎" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Preferred Networks &amp; キカガク

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../',
              VERSION:'',
              LANGUAGE:'ja',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/translations.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>