
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="ja" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ja"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>2. 機械学習ライブラリの基礎 — メディカルAI専門コース オンライン講義資料  documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="03_Introduction_to_Neural_Network.html" rel="next" title="3. ニューラルネットワークの基礎"/>
<link href="01_Basic_Math_for_ML.html" rel="prev" title="1. 機械学習に必要な数学の基礎"/>
<script src="../_static/js/modernizr.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-797798-11"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-797798-11');
  </script>
<meta content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。" name="description"/>
<meta content="メディカルAI専門コース オンライン講義資料" property="og:title"/>
<meta content="メディカルAI学会公認資格向けオンライン講義資料。機械学習に必要な数学の基礎の解説から深層学習（ディープラーニング）を用いた実践的な内容までGoogle Colaboratory上でGPUを用いて実際にコードを実行可能な形式にしオンライン資料として無料公開。" property="og:description"/>
<meta content="website" property="og:type"/>
<meta content="https://japan-medical-ai.github.io/medical-ai-course-materials/" property="og:url"/>
<meta content="https://raw.githubusercontent.com/japan-medical-ai/medical-ai-course-materials/master/notebooks/images/medical_ai.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="@PreferredNetJP" name="twitter:site"/>
<meta content="@PreferredNetJP" name="twitter:creator"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../index.html"> メディカルAI専門コース オンライン講義資料
          

          
          </a>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="01_Basic_Math_for_ML.html">1. 機械学習に必要な数学の基礎</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2. 機械学習ライブラリの基礎</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#単回帰分析">2.1. 単回帰分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#問題設定（単回帰分析）">2.1.1. 問題設定（単回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step1.-モデルを決める（単回帰分析）">2.1.2. Step1. モデルを決める（単回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step2.-目的関数を決める（単回帰分析）">2.1.3. Step2. 目的関数を決める（単回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step3.-最適なパラメータを求める（単回帰分析）">2.1.4. Step3. 最適なパラメータを求める（単回帰分析）</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#重回帰分析">2.2. 重回帰分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#問題設定（重回帰分析）">2.2.1. 問題設定（重回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step1.-モデルを決める（重回帰分析）">2.2.2. Step1. モデルを決める（重回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step2.-目的関数を決める（重回帰分析）">2.2.3. Step2. 目的関数を決める（重回帰分析）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step3.-パラメータを最適化する（重回帰分析）">2.2.4. Step3. パラメータを最適化する（重回帰分析）</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#NumPyによる実装">2.3. NumPyによる実装</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Scikit-learnによる機械学習アルゴリズムの実行">2.4. Scikit-learnによる機械学習アルゴリズムの実行</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Scikit-learn-基礎編">2.4.1. Scikit-learn 基礎編</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scikit-learn-応用編">2.4.2. Scikit-learn 応用編</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#サンプルデータセットの使用">2.4.2.1. サンプルデータセットの使用</a></li>
<li class="toctree-l4"><a class="reference internal" href="#データセットの分割">2.4.2.2. データセットの分割</a></li>
<li class="toctree-l4"><a class="reference internal" href="#アンダーフィッティングとオーバーフィッティング">2.4.2.3. アンダーフィッティングとオーバーフィッティング</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Scikit-learnを使った前処理">2.4.2.4. Scikit-learnを使った前処理</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="03_Introduction_to_Neural_Network.html">3. ニューラルネットワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_Introduction_to_Chainer.html">4. Deep Learningフレームワークの基礎</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_Image_Segmentation.html">5. 実践編: MRI画像のセグメンテーション</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_Blood_Cell_Detection.html">6. 実践編: 血液の顕微鏡画像からの細胞検出</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_DNA_Sequence_Data_Analysis.html">7. 実践編: ディープラーニングを使った配列解析</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Sequential_Data_Analysis_with_Deep_Learning.html">8. 実践編: ディープラーニングを使ったモニタリングデータの時系列解析</a></li>
</ul>
<div style="padding-right:20px; bottom:10px;">
<a href="https://short-term.kikagaku.co.jp/dnn-seminar/">
<img src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/img_handson.png"/>
<p style="padding:5px; font-size:small; line-height: 150%">ディープラーニングの詳しい解説や画像・自然言語の取り扱い、クラウド上のGPUを使った実践的な演習をご希望の方はこちらがおすすめです</p>
</a>
</div>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../index.html">メディカルAI専門コース オンライン講義資料</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a href="../index.html">Docs</a> »</li>
<li>2. 機械学習ライブラリの基礎</li>
<li class="wy-breadcrumbs-aside">
<a href="../_sources/notebooks/02_Introduction_to_ML_libs.ipynb.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="機械学習ライブラリの基礎">
<h1>2. 機械学習ライブラリの基礎<a class="headerlink" href="#機械学習ライブラリの基礎" title="Permalink to this headline">¶</a></h1><p><a class="reference external" href="https://colab.research.google.com/github/japan-medical-ai/medical-ai-course-materials/blob/master/notebooks/02_Introduction_to_ML_libs.ipynb"><img alt="colab-logo" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<p>本章では，基礎的な機械学習手法として代表的な<strong>単回帰分析</strong>と<strong>重回帰分析</strong>の仕組みを、数式を用いて説明します． ここで単回帰分析と重回帰分析を紹介することには 2 つの理由があります． 1 つ目は，回帰分析と重回帰分析の数学がニューラルネットワーク含めたディープラーニングの数学の基礎となるためです． 2 つ目は，単回帰分析のアルゴリズムを通して微分，重回帰分析のアルゴリズムを通して線形代数に関する理解を深めることができるためです．</p>
<div class="section" id="単回帰分析">
<h2>2.1. 単回帰分析<a class="headerlink" href="#単回帰分析" title="Permalink to this headline">¶</a></h2>
<p>まずはじめに，単回帰分析について説明します． 機械学習手法は，<strong>教師あり学習 (supervised learning)</strong>，<strong>教師なし学習 (unsupervised learning)</strong>，<strong>強化学習 (reinforcement learning)</strong>に大別され，単回帰分析は教師あり学習に含まれます．</p>
<p>教師あり学習の中でも典型的な問題設定は 2 つに大別されます． 与えられた入力変数から，<span class="math">\(10\)</span> や <span class="math">\(0.1\)</span> といった実数値を予測する<strong>回帰 (regression)</strong>と、「赤ワイン」，「白ワイン」といったカテゴリを予測する<strong>分類 (classification)</strong>の 2 つです．</p>
<p>単回帰分析は回帰を行うための手法であり，1 つの入力変数から 1 つの出力変数を予測します． それに対し，重回帰分析は，複数の入力変数から 1 つの出力変数を予測します． この両手法は教師あり学習であるため，訓練の際には、入力変数 <span class="math">\(x\)</span> と目的変数 <span class="math">\(t\)</span> がペアで準備されている必要があります．</p>
<div class="section" id="問題設定（単回帰分析）">
<h3>2.1.1. 問題設定（単回帰分析）<a class="headerlink" href="#問題設定（単回帰分析）" title="Permalink to this headline">¶</a></h3>
<p>まず，データに含まれる情報の中から何を利用し，何を予測させるかを決めます．</p>
<p>ここでは例として，家賃を予測する問題を考えることにします． 従って，家賃が <strong>出力変数</strong> <span class="math">\(y\)</span> となります．</p>
<p>次に， <strong>入力変数</strong> として何を採用するかを考えます． 家賃の予測には，部屋の広さ，駅からの距離，犯罪発生率などを考慮する必要があると思われます． ここでは部屋の広さを入力変数 <span class="math">\(x\)</span> として採用することにします． 複数の入力変数の候補がある場合に，それらを同時に扱う方法は，次の重回帰分析の説明の際に紹介します．</p>
<p>多くの機械学習手法は，大きく分けて次の3ステップで構成されています．</p>
<ul class="simple">
<li>Step1: モデルを決める</li>
<li>Step2: 目的関数を決める</li>
<li>Step3: 最適なパラメータを求める</li>
</ul>
<p>上記の3ステップについて，順に説明していきます．</p>
</div>
<div class="section" id="Step1.-モデルを決める（単回帰分析）">
<h3>2.1.2. Step1. モデルを決める（単回帰分析）<a class="headerlink" href="#Step1.-モデルを決める（単回帰分析）" title="Permalink to this headline">¶</a></h3>
<p>まずはじめに、入力変数 x と出力変数 y との関係をどのように定式化するかを決定します． この定式化したものを<strong>モデル</strong>もしくは<strong>数理モデル</strong>と呼びます．</p>
<p>どのように定式化すれば，家賃をうまく予測することができるのでしょうか． このモデル設計は現在は人手で行うのが一般的であり，機械が自動的に決めてくれるわけではありません（ただし最近ではAutoMLなどモデルを自動決定するための研究も進展してきています）．</p>
<p>例えば，家賃と部屋の広さの組で表されるデータを 3 つ集め，「家賃」を y 軸に，「部屋の広さ」を x 軸にとってそれらをプロットしたとき，次のようになっていたとします．</p>
<p><img alt="家賃と部屋の広さの関係" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/01.png"/></p>
<p>この場合，部屋が広くなるほど，家賃が高くなるという関係が予想されます． また，この 2 変数間の関係性は直線によって表現を行うことができそうだと考えられます． そこで、2 つのパラメータ <span class="math">\(w\)</span> と <span class="math">\(b\)</span> によって特徴づけられる直線の方程式</p>
<div class="math">
\[f(x; w, b) = wx + b\]</div>
<p>によって，部屋の広さと家賃の関係を表すことを考えます． ここで，<span class="math">\(w\)</span> は<strong>重み (weight)</strong>，<span class="math">\(b\)</span> は<strong>バイアス (bias)</strong>の頭文字を採用しています．</p>
<p><img alt="直線式によるモデル化" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/02.png"/></p>
<p>単回帰分析では，このようにモデルとして直線 <span class="math">\(f(x; w, b) = wx + b\)</span> を用います． そして，2 つのパラメータ <span class="math">\(w\)</span> と <span class="math">\(b\)</span> を，直線がデータによくフィットするように調整します．</p>
<p>パラメータで特徴づけられたモデルを用いる場合，与えられた<strong>データセット</strong>に適合するように最適なパラメータを求めることが目標となります． 今回はデータセットとして部屋の広さ <span class="math">\(x\)</span> と家賃 <span class="math">\(t\)</span> の組からなるデータの集合を用います． 全部で <span class="math">\(N\)</span> 個のデータがあり，<span class="math">\(n\)</span> 番目のデータが <span class="math">\((x^{(n)}, t^{(n)})\)</span> と表されるとき，データセットは</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{D} &amp;= \{(x^{(1)}, t^{(1)}), (x^{(2)}, t^{(2)}), \dots, (x^{(N)}, t^{(N)})\} \\
&amp;= \{(x^{(n)}, t^{(n)}\}_{n=1}^N
\end{aligned}\end{split}\]</div>
<p>と表すことができます． これを用いて、新しい <span class="math">\(x\)</span> を入力すると，それに対応する <span class="math">\(t\)</span> を予測するモデルを訓練します．</p>
<p>ここで，データセット中の入力値とのことを<strong>データ点（datum）</strong>ということがあることに注意してください． データ点とは，具体的には上の説明で登場した <span class="math">\(\mathcal{D}\)</span> 中の各 <span class="math">\((x^{(1)},t^{(1)})\)</span> などのことです．</p>
<p>ここで，この後の計算を楽に進めるために，<strong>データの中心化</strong>というテクニックを紹介します． 部屋の広さと家賃は両方とも正の値であるため，各データ点をいくつかプロットすると，下図の左のグラフのようになります． 中心化では，各次元の<strong>平均が</strong> <span class="math">\(\boldsymbol{0}\)</span> となるよう全てのデータを同量平行移動します． 中心化はしばしば前処理として採用されます． 厳密には前章で紹介したスケーリング方法の一つである標準化（正規化）がよく用いられます．</p>
<p><img alt="中心化処理" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/03.png"/></p>
<p>この処理を行うと，下図のように，バイアス <span class="math">\(b\)</span> を <span class="math">\(0\)</span> とおけるため，<span class="math">\(f_c(x; w) = wx_{c}\)</span> のように，モデルをバイアス成分なしで表現することができるようになります． これによって，調整すべきパラメータを減らすことができます．</p>
<p><img alt="中心化後の直線式" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/04.png"/></p>
<p>データの中心化は入出力の平均をデータの全体から引くことで実現されます． つまり，</p>
<div class="math">
\[\begin{split}\begin{aligned}
x^{(n)}_{c} &amp;= x^{(n)} - \bar{x} \\
t^{(n)}_{c} &amp;= t^{(n)} - \bar{t}
\end{aligned}\end{split}\]</div>
<p>という変換を全ての <span class="math">\(n\)</span> について行います．</p>
<p>例えば，具体的な数値で見ると，下の表のようになります．</p>
<table border="1" class="docutils">
<colgroup>
<col width="8%"/>
<col width="30%"/>
<col width="30%"/>
<col width="33%"/>
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">n</th>
<th class="head"><span class="math">\(x^{(n)}\)</span></th>
<th class="head"><span class="math">\(\bar{x}\)</span></th>
<th class="head"><span class="math">\(x^{(n)}_c\)</span></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>1</td>
<td>1</td>
<td>3</td>
<td>-2</td>
</tr>
<tr class="row-odd"><td>2</td>
<td>3</td>
<td>3</td>
<td>0</td>
</tr>
<tr class="row-even"><td>3</td>
<td>5</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>中心化後を示す添え字の <span class="math">\(c\)</span> に関しては表現が冗長となるため，今後はこの添え字を省略し，データの中心化を事前に行っていることを前提とします． この時，モデルは</p>
<div class="math">
\[f(x; w) = wx\]</div>
<p>となり，単回帰分析の目標は，データセット <span class="math">\(\mathcal{D} = \{ x^{(n)}, t^{(n)} \}_{n=1}^{N}\)</span> に基づいて，パラメータ <span class="math">\(w\)</span> を<strong>適切</strong>に調整することになります．</p>
</div>
<div class="section" id="Step2.-目的関数を決める（単回帰分析）">
<h3>2.1.3. Step2. 目的関数を決める（単回帰分析）<a class="headerlink" href="#Step2.-目的関数を決める（単回帰分析）" title="Permalink to this headline">¶</a></h3>
<p>1章で説明したように，教師あり学習では多くの場合，目的関数を設計し，その目的関数を最小化（または最大化）することでモデルの訓練を行います．</p>
<p>今回は教師データと予測値が一致することが目標であり，乖離度を表す最小化すべき目的関数として教師データと予測値の二乗誤差を使います． ここで，モデルの出力が予測値となります．すなわち，<span class="math">\(y = f(x; w)\)</span> です． 二乗誤差が <span class="math">\(0\)</span> であれば <span class="math">\(t = y\)</span> となり予測が完全に教師データと一致していることを意味します． <span class="math">\(n\)</span> 番目の物件に対する教師データ <span class="math">\(t^{(n)}\)</span> と予測値 <span class="math">\(y^{(n)} = f(x^{(n)}; w)\)</span> の二乗誤差は</p>
<div class="math">
\[(t^{(n)} - y^{(n)})^2\]</div>
<p>です． 特定の物件についてだけ考慮するのではなく，データセット中の全ての物件の情報を考慮してモデルの訓練を行うために，上式で計算される各データ点における二乗誤差を全物件に対してそれぞれ計算して和をとったものを目的関数とします． すなわち，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L} &amp;=
\left( t^{(1)} - y^{(1)} \right)^2 +
\left( t^{(2)} - y^{(2)} \right)^2 +
\dots +
\left( t^{(N)} - y^{(N)} \right)^2 \\
&amp;=
\sum^{N}_{n=1} \left( t^{(n)} - y^{(n)} \right)^2 \\
\end{aligned}\end{split}\]</div>
<p>です． また，Step1で決めたモデル</p>
<div class="math">
\[y^{(n)} = f(x^{(n)}; w) = wx^{(n)}\]</div>
<p>をこれに代入すると，目的関数は</p>
<div class="math">
\[\mathcal{L} = \sum^{N}_{n=1}\left( t^{(n)} - wx^{(n)} \right)^2\]</div>
<p>とパラメータを含んだ形式で表現することができます．このような関数を損失関数とよぶことを思い出してください．</p>
</div>
<div class="section" id="Step3.-最適なパラメータを求める（単回帰分析）">
<h3>2.1.4. Step3. 最適なパラメータを求める（単回帰分析）<a class="headerlink" href="#Step3.-最適なパラメータを求める（単回帰分析）" title="Permalink to this headline">¶</a></h3>
<p>次に目的関数を最小にするようなパラメータを求めます． 今回用いた目的関数は二次関数であるため，微分して求まる導関数を 0 とおいてそのときの x を求めれば，最小値を取る時の x が分かります． すなわち，目的関数の「傾きが0」となる点が値 <span class="math">\(0\)</span> をとる点です．</p>
<p>それでは，目的関数を微分しましょう．</p>
<div class="math">
\[\begin{aligned}
\dfrac{\partial }{\partial w} \mathcal{L} &amp;=
\dfrac{\partial}{\partial w} { \sum^{N}_{n=1} ( t^{(n)}-wx^{(n)})^{2} }
\end{aligned}\]</div>
<p>微分という操作は<strong>線形性</strong>を持っているため，和の微分は，微分の和と等しくなります． これを利用して次を得ます．</p>
<div class="math">
\[\dfrac{\partial}{\partial w} \mathcal{L} =
\sum^{N}_{n=1} \dfrac {\partial }{\partial w} \left( t^{(n)}-wx^{(n)} \right)^2\]</div>
<p>パラメータ <span class="math">\(w\)</span> による微分を表す <span class="math">\(\dfrac {\partial }{\partial w}\)</span> と総和を表す <span class="math">\(\sum\)</span> が入れ替わっています． 次に和の各項を見ます．</p>
<div class="math">
\[\dfrac {\partial }{\partial w}\left( t^{(n)}-wx^{(n)} \right)^2\]</div>
<p><span class="math">\(t^{(n)} - wx^{(n)}\)</span> と <span class="math">\((\cdot)^2\)</span> の<strong>合成関数</strong>になっています．</p>
<p><span class="math">\(u^{(n)} = t^{(n)} - wx^{(n)}\)</span>, <span class="math">\(g(u^{(n)}; w) = (u^{(n)})^2\)</span> とおくと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial w}
\left(
    t^{(n)} - wx^{(n)}
\right)^2
&amp;= \dfrac {\partial }{\partial w} g(u^{(n)}) \\
&amp;= \dfrac {\partial u^{(n)}}{\partial w}
\dfrac{\partial g(u^{(n)})}{\partial u^{(n)}} \\
&amp;= -x^{(n)} \cdot (2 u^{(n)}) \\
&amp;= -2x^{(n)}( t^{(n)} - wx^{(n)} )
\end{aligned}\end{split}\]</div>
<p>が得られます．これより，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\dfrac{\partial }{\partial w} \mathcal{L}
&amp;= \sum^{N}_{n=1}
\dfrac {\partial }{\partial w}
\left( t^{(n)} - wx^{(n)} \right)^2 \\
&amp;= -\sum^{N}_{n=1} 2x^{(n)} \left( t^{(n)} - wx^{(n)} \right)
\end{aligned}\end{split}\]</div>
<p>となります．この微分の値を 0 とおいて <span class="math">\(w\)</span> について解くと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial w} \mathcal{L} &amp;= 0 \\
-2 \sum^{N}_{n=1} x^{(n)} \left( t^{(n)} - wx^{(n)} \right) &amp;= 0 \\
-2 \sum^{N}_{n=1} x^{(n)} t^{(n)} + 2 \sum^{N}_{n=1} w(x^{(n)})^2 &amp;= 0 \\
-2 \sum^{N}_{n=1} x^{(n)} t^{(n)} + 2w \sum^{N}_{n=1} (x^{(n)})^2 &amp;= 0 \\
w \sum^{N}_{n=1} (x^{(n)})^2 &amp;= \sum^{N}_{n=1} x^{(n)} t^{(n)}
\end{aligned}\end{split}\]</div>
<p>より，</p>
<div class="math">
\[\begin{aligned}
w &amp;= \dfrac
{\displaystyle \sum^{N}_{n=1} x^{(n)} t^{(n)}}
{\displaystyle \sum^{N}_{n=1} (x^{(n)})^2}
\end{aligned}\]</div>
<p>と求まります． この右辺に着目すると，与えられたデータセット <span class="math">\(\mathcal{D} = \{x^{(n)}, t^{(n)}\}_{n=1}^{N}\)</span> のみから決定されていることがわかります．</p>
<p>それでは，以下の数値例を使って実際にパラメータ <span class="math">\(w\)</span> を求めてみましょう．</p>
<table border="1" class="docutils">
<colgroup>
<col width="12%"/>
<col width="44%"/>
<col width="44%"/>
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">n</th>
<th class="head"><span class="math">\(x^{(n)}\)</span></th>
<th class="head"><span class="math">\(t^{(n)}\)</span></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>1</td>
<td>1</td>
<td>2</td>
</tr>
<tr class="row-odd"><td>2</td>
<td>2</td>
<td>3.9</td>
</tr>
<tr class="row-even"><td>3</td>
<td>3</td>
<td>6.1</td>
</tr>
</tbody>
</table>
<p>まずは，データの中心化を行うために，平均を求めます．</p>
<div class="math">
\[\begin{split}\begin{aligned}
\bar{x} &amp;= \dfrac{1}{3} (1 + 2 + 3) = 2 \\
\bar{t} &amp;= \dfrac{1}{3}(2 + 3.9 + 6.1) = 4
\end{aligned}\end{split}\]</div>
<p>各データ点からそれぞれの値を引きます．</p>
<div class="math">
\[\begin{split}\begin{aligned}
x_{1} &amp;= 1 - 2 = -1 \\
x_{2} &amp;= 2 -2 = 0 \\
x_{3} &amp;= 3- 2 = 1\\
t_{1} &amp;= 2 - 4 = -2\\
t_{2} &amp;= 3.9 - 4 = -0.1\\
t_{3} &amp;= 6.1 - 4 = 2.1
\end{aligned}\end{split}\]</div>
<p>それでは，中心化後の値を用いて，最適なパラメータ<span class="math">\(w\)</span> を計算します．</p>
<div class="math">
\[\begin{split}\begin{aligned}
w &amp;= \dfrac
{\displaystyle \sum_{n=1}^{N} x^{(n)} t^{(n)}}
{\displaystyle  \sum_{n=1}^{N} (x^{(n)})^2} \\
&amp;= \dfrac
{ x^{(1)} t^{(1)} + x^{(2)} t^{(2)} + x^{(3)} t^{(3)} }
{ (x^{(1)})^2 + (x^{(2)})^2 + (x^{(3)})^2 } \\
&amp;= \dfrac
{ -1 \times (-2) + 0 \times 0.1 + 1 \times 2.1 }{ (-1)^2 + 0^2 + 1^2 } \\
&amp;= 2.05
\end{aligned}\end{split}\]</div>
<p>これで単回帰分析の学習が完了しました． この求まったパラメータを <span class="math">\(\hat{w}\)</span> とすると，これを使用したモデル <span class="math">\(f(x; \hat{w})\)</span> が<strong>学習済みモデル</strong>となります．</p>
<p>続いて，このモデルを使って新しいサンプルに対する予測をしてみましょう． 学習したモデルを使って新たな入力データについて予測値を計算する処理を<strong>推論 (inference)</strong>とよびます． 例えば，新しいサンプル <span class="math">\(x^{(q)}=1.5\)</span> に対する予測値は次のように求まります，</p>
<div class="math">
\[\begin{split}\begin{aligned}
y^{(q)} - \bar{t} &amp;= \hat{w}(x^{(q)}-\bar{x}) \\
\Rightarrow y^{(q)} &amp;= \hat{w}(x^{(q)}-\bar{x}) + \bar{t} \\
&amp;= 2.05 \times (1.5 - 2) + 4 \\
&amp;= 2.975
\end{aligned}\end{split}\]</div>
<p>モデルは中心化データを用いて学習を行ったので，推論の際にも入力値・予測値それぞれに同様の操作を行う必要があることに注意しましょう．</p>
<p>以上が，単回帰分析の一連の手順となります．</p>
</div>
</div>
<div class="section" id="重回帰分析">
<h2>2.2. 重回帰分析<a class="headerlink" href="#重回帰分析" title="Permalink to this headline">¶</a></h2>
<p>次に，多変数の入力変数を扱う重回帰分析を扱います． この重回帰分析を学ぶことで線形代数に関する知識が深まります．</p>
<p>重回帰分析は単回帰分析と同様に教師あり学習の一種であり，回帰を行う手法です． 問題設定はほとんど単回帰分析と同じですが，重回帰分析では入力変数の数が複数となります． つまり，複数の入力変数から出力変数を予測する機械学習手法の一つです．</p>
<div class="section" id="問題設定（重回帰分析）">
<h3>2.2.1. 問題設定（重回帰分析）<a class="headerlink" href="#問題設定（重回帰分析）" title="Permalink to this headline">¶</a></h3>
<p>ここでは単回帰分析の場合と同様に家賃を予測する問題を考え，家賃を出力変数 <span class="math">\(y\)</span> とします． 入力変数としては，単回帰分析では考慮しきれていなかった駅からの距離や犯罪発生率なども同時に考慮します． 例えば，部屋の広さ <span class="math">\(x_{1}\)</span>, 駅からの距離 <span class="math">\(x_{2}\)</span>, ..., 犯罪発生率 <span class="math">\(x_{M}\)</span> のように <span class="math">\(M\)</span> 個の入力変数があるとします（<span class="math">\(M=1\)</span>の場合，単回帰分析の問題に帰着されます）．</p>
<p>単回帰分析と同様，以下の3ステップに従います．</p>
<ul class="simple">
<li>Step1: モデルを決める</li>
<li>Step2: 目的関数を決める</li>
<li>Step3: 最適なパラメータを求める</li>
</ul>
</div>
<div class="section" id="Step1.-モデルを決める（重回帰分析）">
<h3>2.2.2. Step1. モデルを決める（重回帰分析）<a class="headerlink" href="#Step1.-モデルを決める（重回帰分析）" title="Permalink to this headline">¶</a></h3>
<p>単回帰分析のモデルは，</p>
<div class="math">
\[f(x; w, b) = wx + b\]</div>
<p>であり，<span class="math">\(w\)</span> を重み（weight），<span class="math">\(b\)</span> をバイアス (bias) とよびました． 重回帰分析では，この式を複数の入力変数へと拡張し，</p>
<div class="math">
\[f({\bf x}; {\bf w}, b) = w_{1}x_{1} + w_{2}x_{2} + \dots + w_{M}x_{M} + b\]</div>
<p>のような<strong>線形結合</strong>の形で表します． ここで，太字の <span class="math">\({\bf x}, {\bf w}\)</span> はそれぞれベクトルを表し，それぞれの <span class="math">\(m\)</span> 番目の要素が <span class="math">\(w_m, x_m\)</span> で表されています．</p>
<p>ここでは，各入力変数は出力変数に線形に影響を与えることが仮定されています． 一方，もし入力変数間に非線形な依存関係が想定される場合には，そのことを考慮したモデル化を行う必要があります． それについては後述します．</p>
<p>重回帰分析のモデルは総和の記号を使って整理すると，</p>
<div class="math">
\[f({\bf x}; {\bf w}, b) = \sum_{m=1}^{M} w_{m} x_{m} + b\]</div>
<p>のように書くことができます．さらにここで，<span class="math">\(x_0 = 1\)</span>，<span class="math">\(w_0 = b\)</span>とおくと，</p>
<div class="math">
\[\begin{split}\begin{aligned}
f({\bf x}; {\bf w}, b)
&amp;= w_{1}x_{1} + w_{2}x_{2} + \dots + w_{M}x_{M} + b\\
&amp;= w_{1}x_{1} + w_{2}x_{2} + \dots + w_{M}x_{M} + w_{0}x_{0}\\
&amp;= w_{0}x_{0} + w_{1}x_{1} + \dots + w_{M}x_{M} \\
&amp;= \sum_{m=0}^M w_m x_m
\end{aligned}\end{split}\]</div>
<p>のようにバイアス <span class="math">\(b\)</span> を総和記号の中に含めることができます． <span class="math">\(w_0\)</span> を含む <span class="math">\({\bf w}\)</span> を改めて <span class="math">\({\bf w}\)</span>，最初の要素 <span class="math">\(x_0\)</span> として <span class="math">\(1\)</span> を付け加えた入力ベクトルを改めて <span class="math">\({\bf x}\)</span> と定義しなおすと，このモデルはベクトルを用いて</p>
<div class="math">
\[\begin{split}\begin{aligned}
f({\bf x}; {\bf w})
&amp;= \begin{bmatrix}
w_{0} &amp; w_{1} &amp; \dots  &amp; w_{M}
\end{bmatrix}
\begin{bmatrix}
x_{0} \\
x_{1} \\
\vdots  \\
x_{M}
\end{bmatrix} \\
&amp;= {\bf w}^{T} {\bf x} = {\bf x}^T {\bf w}
\end{aligned}\end{split}\]</div>
<p>と書けます． これで，ベクトルの内積を用いて表現することができました．</p>
<p>これで重回帰分析を行うためのモデルが決定できました． このモデルはパラメータとして <span class="math">\(M+1\)</span> 個の重み <span class="math">\({\bf w}\)</span> を持っています．</p>
</div>
<div class="section" id="Step2.-目的関数を決める（重回帰分析）">
<h3>2.2.3. Step2. 目的関数を決める（重回帰分析）<a class="headerlink" href="#Step2.-目的関数を決める（重回帰分析）" title="Permalink to this headline">¶</a></h3>
<p>単回帰分析では，目標値 <span class="math">\(t\)</span> と予測値 <span class="math">\(y\)</span> の二乗誤差が小さいほど良い予測であるとし，その総和を目的関数にしました． 重回帰分析でも，そのような予測値 <span class="math">\(y\)</span> を求めるというのは同じであるため，同じ目的関数を使います．</p>
<p>ここで，<span class="math">\(n\)</span> 個目の入力値 <span class="math">\({\bf x}^{(n)}\)</span> に対する予測値 <span class="math">\(f({\bf x}^{(n)}; {\bf w})\)</span> を <span class="math">\(y^{(n)}\)</span>，目標値を <span class="math">\(t^{(n)}\)</span> とおくと，目的関数は</p>
<div class="math">
\[\begin{aligned}
\mathcal{L}
&amp;= \left( t^{(1)} - y^{(1)} \right)^2
+ \left( t^{(2)} - y^{(2)} \right)^2
+ \dots
+ \left( t^{(N)} - y^{(N)} \right)^2
\end{aligned}\]</div>
<p>となります． 単回帰分析では，これを総和記号 <span class="math">\(\sum\)</span> を用いて表すことができましたが，これは以下のようにベクトル同士の内積を用いて表すこともできます．</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}
&amp;= \left( t^{(1)} - y^{(1)} \right)^2
+ \left( t^{(2)} - y^{(2)} \right)^2
+ \dots
+ \left( t^{(N)} - y^{(N)} \right)^2 \\
&amp;= \begin{bmatrix}
t^{(1)} - y^{(1)} &amp; t^{(2)} - y^{(2)} &amp; \dots &amp; t^{(N)} - y^{(N)}
\end{bmatrix}
\begin{bmatrix}
t^{(1)} - y^{(1)} \\
t^{(2)} - y^{(2)} \\
\vdots \\
t^{(N)} - y^{(N)}
\end{bmatrix} \\
&amp;= \left( {\bf t} - {\bf y} \right)^{\rm T}
\left( {\bf t} - {\bf y} \right)
\end{aligned}\end{split}\]</div>
<p>また，<span class="math">\({\bf y}\)</span> は</p>
<div class="math">
\[\begin{split}\begin{aligned}
{\bf y}
=
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(N)}
\end{bmatrix}
=
\begin{bmatrix}
({\bf x}^{(1)})^{\rm T} {\bf w} \\
({\bf x}^{(2)})^{\rm T} {\bf w} \\
\vdots \\
({\bf x}^{(N)})^{\rm T} {\bf w}
\end{bmatrix}
=
\begin{bmatrix}
({\bf x}^{(1)})^{\rm T} \\
({\bf x}^{(2)})^{\rm T} \\
\vdots \\
({\bf x}^{(N)})^{\rm T}
\end{bmatrix}
\boldsymbol{w}
\end{aligned}\end{split}\]</div>
<p>のように書くことができます． ここで，<span class="math">\({\bf x}^{(n)}\)</span> の <span class="math">\(m\)</span> 番目の要素を <span class="math">\(x_{nm}\)</span> と書くことにすると，上式はさらに</p>
<div class="math">
\[\begin{split}\begin{aligned}
{\bf y}
&amp;=
\begin{bmatrix}
x_{10} &amp; x_{11} &amp; x_{12} &amp; \dots  &amp; x_{1M} \\
x_{20} &amp; x_{21} &amp; x_{22} &amp; \dots  &amp; x_{2M} \\
\vdots  &amp; \vdots  &amp; \vdots  &amp; \ddots  \\
x_{N0} &amp; x_{N1} &amp; x_{N{2}} &amp; \dots  &amp; x_{NM}
\end{bmatrix}
\begin{bmatrix}
w_{0} \\
w_{1} \\
w_{2} \\
\vdots \\
w_{M}
\end{bmatrix} \\
&amp;=
{\bf X}{\bf w}
\end{aligned}\end{split}\]</div>
<p>と表せます． <span class="math">\({\bf X}\)</span> の各行が各サンプルを表しており，各列が入力変数を表しています． つまり，各サンプルごとに <span class="math">\(M\)</span> 個の入力変数（<span class="math">\(x_{n0}\)</span> は常に <span class="math">\(1\)</span>）を持ちます．</p>
<p>具体例を挙げてみます． 例えば，<span class="math">\(m=1\)</span> が部屋の広さ，<span class="math">\(m=2\)</span> が駅からの距離，<span class="math">\(m=3\)</span> が犯罪発生率に対応する入力変数だとして，<span class="math">\(n\)</span> 個目のデータ点が部屋の広さ <span class="math">\(= 50m^{2}\)</span> ，駅からの距離 <span class="math">\(= 600 m\)</span> ，犯罪発生率 <span class="math">\(= 2\)</span>% であるような物件を表している場合，これは</p>
<div class="math">
\[{\bf x}_{n}^{T} =
\begin{bmatrix}
1 &amp; 50 &amp; 600 &amp; 0.02
\end{bmatrix}\]</div>
<p>というベクトルになります． 今，3 つの入力変数を考えているので，<span class="math">\(M=3\)</span> ですが，入力ベクトル <span class="math">\({\bf x}_n\)</span> の１つ目の要素 <span class="math">\(x_{n1}\)</span> はバイアス <span class="math">\(w_0\)</span> に対応する要素で常に値は <span class="math">\(1\)</span> となることに注意してください．</p>
</div>
<div class="section" id="Step3.-パラメータを最適化する（重回帰分析）">
<h3>2.2.4. Step3. パラメータを最適化する（重回帰分析）<a class="headerlink" href="#Step3.-パラメータを最適化する（重回帰分析）" title="Permalink to this headline">¶</a></h3>
<p>それでは，目的関数を最小化するパラメータ <span class="math">\({\bf w}\)</span>を求めてみましょう．</p>
<p><strong>※ここでは，式変形を駆使しながら最適パラメータの解析的な解を求めていきますが，導出過程が少々複雑になります．導出結果は次節(§2.3)で示されているので，導出に興味のある方以外は本節はスキップしていただいて構いません．</strong></p>
<p>まずは目的関数をパラメータ <span class="math">\({\bf w}\)</span> を用いて表し直すと</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}
&amp;=
\left( {\bf t} - {\bf y} \right)^{\rm T}
\left( {\bf t} - {\bf y} \right) \\
&amp;=
\left( {\bf t} - {\bf X}{\bf w} \right)^{\rm T}
\left( {\bf t} - {\bf X}{\bf w} \right) \\
&amp;=
\left\{ {\bf t}^{\rm T} - ({\bf X}{\bf w})^{\rm T} \right\}
\left( {\bf t} - {\bf X}{\bf w} \right) \\
&amp;=
\left( {\bf t}^{\rm T} - {\bf w}^{\rm T}{\bf X}^{\rm T} \right)
\left( {\bf t} - {\bf X}{\bf w} \right)
\end{aligned}\end{split}\]</div>
<p>となります． ここで，転置の公式 <span class="math">\(({\bf A}{\bf B})^{\rm T} = {\bf B}^{\rm T}{\bf A}^{\rm T}\)</span> を使っていることに注意しましょう． さらに分配法則を使って展開すると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}
&amp;=
{\bf t}^{\rm T}{\bf t}
- {\bf t}^{\rm T}{\bf X}{\bf w}
- {\bf w}^{\rm T}{\bf X}^{\rm T}{\bf t}
+ {\bf w}^{\rm T}{\bf X}^{\rm T}{\bf X}{\bf w} \\
\end{aligned}\end{split}\]</div>
<p>となります． この目的関数に対しパラメータの <span class="math">\(w\)</span> についての偏微分を計算します． その前にこの式はもう少し整理することができます． はじめに，</p>
<div class="math">
\[(1)^T = 1\]</div>
<p>のようにスカラは転置しても変化しません． 上式の中で出てくる <span class="math">\({\bf t}^{\rm T}{\bf X}{\bf w}\)</span> はスカラなので，</p>
<div class="math">
\[({\bf t}^{\rm T}{\bf X}{\bf w})^{\rm T} =
{\bf t}^{\rm T}{\bf X}{\bf w}\]</div>
<p>が成り立ちます． さらに，転置の公式 <span class="math">\(({\bf A}{\bf B}{\bf C})^{\rm T} = {\bf C}^{\rm T}{\bf B}^{\rm T}{\bf A}^{\rm T}\)</span> より，</p>
<div class="math">
\[({\bf t}^{\rm T}{\bf X}{\bf w})^{\rm T}
= {\bf w}^{\rm T}{\bf X}^{\rm T}{\bf t}\]</div>
<p>も成り立ちます．これより，</p>
<div class="math">
\[({\bf t}^{\rm T}{\bf X}{\bf w})^{\rm T}
= {\bf t}^{\rm T}{\bf X}{\bf w}
= {\bf w}^{\rm T}{\bf X}^{\rm T}{\bf t}\]</div>
<p>を導くことができます． 目的関数を <span class="math">\(\mathcal{L}\)</span> とおくと，上の式を利用して，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}
= {\bf t}^{\rm T}{\bf t}
- 2 {\bf t}^{\rm T}{\bf X}{\bf w}
+ {\bf w}^{\rm T}{\bf X}^{\rm T}{\bf X}{\bf w} \\
\end{aligned}\end{split}\]</div>
<p>とまとめることができます．ここで， <span class="math">\({\bf w}\)</span> について偏微分を行いやすくするため， <span class="math">\({\bf w}\)</span> 以外の定数項をまとめると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}
&amp;= {\bf t}^{\rm T}{\bf t}
- 2 {\bf t}^{\rm T}{\bf X}{\bf w}
+ {\bf w}^{\rm T}{\bf X}^{\rm T}{\bf X}{\bf w} \\
&amp;= {\bf t}^{\rm T}{\bf t}
- 2 \left( {\bf X}^{\rm T}{\bf t} \right)^{\rm T}{\bf w}
+ {\bf w}^{\rm T}{\bf X}^{\rm T}{\bf X}{\bf w} \\
&amp;= \gamma + \boldsymbol{\beta}^{\rm T}{\bf w} + {\bf w}^{\rm T}{\bf A}{\bf w}
\end{aligned}\end{split}\]</div>
<p>となります． 線形代数の章で学んだような <span class="math">\({\bf w}\)</span> に関する二次形式（二次関数）で表現することができました． ここで，<span class="math">\({\bf A}= {\bf X}^{\rm T}{\bf X}, \ \boldsymbol{\beta} =-2 {\bf X}^{\rm T}{\bf t}, \ \gamma = {\bf t}^{\rm T}{\bf t}\)</span> です． また，<span class="math">\(\boldsymbol{\beta}\)</span> を転置の形式にした理由は，線形代数の章で学んだベクトルで微分するための公式集にある形式に合わせるためです．</p>
<p>それでは，この目的関数を最小化するパラメータ <span class="math">\({\bf w}\)</span> の求め方を考えましょう． 前述の通り，目的関数はパラメータ <span class="math">\({\bf w}\)</span> に関しては二次関数になっています． 例えば，</p>
<div class="math">
\[\begin{split}\begin{aligned}
{\bf w} = \begin{bmatrix}
w_{1} \\ w_{2}
\end{bmatrix},
{\bf A} = \begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix},
\boldsymbol{\beta} = \begin{bmatrix}
1 \\
2
\end{bmatrix},
\gamma = 1
\end{aligned}\end{split}\]</div>
<p>のとき，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L} &amp; =
{\bf w}^{\rm T}{\bf A}{\bf w}
+ \boldsymbol{\beta}^{\rm T}{\bf w}
+ \gamma \\
&amp;=
\begin{bmatrix}
w_{1} &amp; w_{2}
\end{bmatrix}
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}
\begin{bmatrix}
w_{1} \\
w_{2}
\end{bmatrix}
+
\begin{bmatrix}
1 &amp; 2
\end{bmatrix}
\begin{bmatrix}
w_{1} \\
w_{2}
\end{bmatrix}
+ 1 \\
&amp;=
\begin{bmatrix}
w_{1} &amp; w_{2}
\end{bmatrix}
\begin{bmatrix}
w_{1} + 2w_{2} \\
3w_{1} + 4w_{2}
\end{bmatrix}
+ w_{1}
+ 2 w_{2}
+ 1 \\
&amp;= w_{1} \left( w_{1} + 2w_{2} \right)
+ w_{2} \left( 3w_{1} + 4w_{2} \right)
+ w_{1}
+ 2w_{2}
+ 1 \\
&amp;= w^{2}_{1}
+ 5w_{1}w_{2}
+ 4w^{2}_{2}
+ w_{1}
+ 2w_{2}
+ 1 \\
\end{aligned}\end{split}\]</div>
<p>となります． さらに <span class="math">\(w_{1}, w_{2}\)</span> に関してそれぞれまとめると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathcal{L}
&amp;= w^{2}_{1}
+ \left( 5w_{2} + 1\right) w_{1}
+ \left( 4w^{2}_{2} + 2w_{2} + 1 \right) \\
&amp;= 4w^{2}_{2}
+ \left( 5w_{1} + 2 \right) w_{2}
+ \left( w^{2}_{1} + w_{1} + 1 \right)
\end{aligned}\end{split}\]</div>
<p>となり，それぞれの二次関数であることがわかります．</p>
<p>そして，二次関数であれば，下図のような形となります．</p>
<p><img alt="パラメータと目的関数の関係（2次元）" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/06.png"/></p>
<p>これを3次元でイメージすると，下図のようになります．</p>
<p><img alt="パラメータと目的関数の関係（3次元）" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/08.png"/></p>
<p>そして，目的関数である二乗誤差の総和が最小となる点では各変数で微分した時の傾きが0となります．</p>
<p><img alt="目的関数が最小となる点" src="https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/07.png"/></p>
<p>この例では，<span class="math">\(w_{1}\)</span> と <span class="math">\(w_{2}\)</span> の２つのパラメータの場合で考えましたが，これは <span class="math">\(w_{0}\)</span>, <span class="math">\(w_{1}\)</span>, <span class="math">\(w_{2}\)</span>, <span class="math">\(\ldots\)</span>, <span class="math">\(w_{M}\)</span> の場合でも同様に考えることができ，目的関数が最小となる点は</p>
<div class="math">
\[\begin{split}\begin{cases}
\dfrac {\partial }{\partial w_{0}}\mathcal{L} = 0\\
\dfrac {\partial }{\partial w_{1}}\mathcal{L} = 0\\
\ \ \ \ \ \vdots \\
\dfrac {\partial }{\partial w_{M}}\mathcal{L} = 0\\
\end{cases}\end{split}\]</div>
<p>となり，これをまとめると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\begin{bmatrix}
\dfrac {\partial}{\partial w_{0}} \mathcal{L} \\
\dfrac {\partial}{\partial w_{1}} \mathcal{L} \\
\vdots  \\
\dfrac {\partial}{\partial w_{M}} \mathcal{L} \\
\end{bmatrix}&amp;=\begin{bmatrix}
0 \\
0 \\
\vdots  \\
0 \\
\end{bmatrix} \\
\Rightarrow \dfrac {\partial}{\partial {\bf w}} \mathcal{L} &amp;= \boldsymbol{0} \\
\end{aligned}\end{split}\]</div>
<p>のようにベクトルでの微分として表されます． あとは，上式を満たす <span class="math">\({\bf w}\)</span> を決めます． まずは <span class="math">\({\bf w}\)</span> が求めやすくなるように，代入と式変形を行います． （下記の計算ではベクトルでの微分をはじめとして線形代数の章で学んだ内容を活用しているため，計算途中がわからなくなった場合には，線形代数の章を再度確認しながら進めてください．）</p>
<div class="math">
\[\begin{split}\begin{aligned}
\dfrac {\partial }{\partial {\bf w}}
\mathcal{L} =
\dfrac {\partial }{\partial {\bf w}}
\left( \gamma + \boldsymbol{\beta}^{\rm T}{\bf w} + {\bf w}^{\rm T}{\bf A}{\bf w} \right)
= {\bf 0} \\
\dfrac {\partial }{\partial {\bf w}}
\left( \gamma \right)
+ \dfrac {\partial }{\partial {\bf w}} \left( \boldsymbol{\beta}^{\rm T}{\bf w} \right)
+ \dfrac {\partial }{\partial {\bf w}} \left( {\bf w}^{\rm T}{\bf A}{\bf w} \right)
= {\bf 0} \\
{\bf 0}
+ \boldsymbol{\beta}
+ \left( {\bf A}+{\bf A}^{\rm T} \right) {\bf w}
= {\bf 0} \\
-2{\bf X}^{\rm T}{\bf t}
+ \left\{ {\bf X}^{\rm T}{\bf X} + \left( {\bf X}^{\rm T}{\bf X} \right)^{\rm T} \right\} {\bf w}
= {\bf 0} \\
-2{\bf X}^{\rm T}{\bf t}+2{\bf X}^{\rm T}{\bf X}{\bf w}={\bf 0}\\
{\bf X}^{\rm T}{\bf X}{\bf w}={\bf X}^{\rm T}{\bf t}\\
\end{aligned}\end{split}\]</div>
<p>ここで，<span class="math">\({\bf X}^{\rm T} {\bf X}\)</span>に逆行列が存在すると仮定して，両辺に左側から <span class="math">\(\left( {\bf X}^{\rm T}{\bf X}\right)^{-1}\)</span> をかけると，</p>
<div class="math">
\[\begin{split}\begin{aligned}
\left( {\bf X}^{\rm T}{\bf X}\right)^{-1}{\bf X}^{\rm T}{\bf X} {\bf w} =\left( {\bf X}^{\rm T}{\bf X}\right)^{-1}{\bf X}^{\rm T}{\bf t} \\
{\bf I}{\bf w}=\left( {\bf X}^{\rm T}{\bf X}\right)^{-1}{\bf X}^{\rm T}{\bf t} \\
{\bf w}=\left( {\bf X}^{\rm T}{\bf X}\right)^{-1}{\bf X}^{\rm T}{\bf t}
\end{aligned}\end{split}\]</div>
<p>となり，与えられたデータセット <span class="math">\({\bf X}, {\bf t}\)</span> から，最適なパラメータ <span class="math">\({\bf w}\)</span> が求まりました．ここで，<span class="math">\({\bf I}\)</span> は単位行列です．また，式変形の際には，</p>
<div class="math">
\[{\bf w} = \dfrac{{\bf X}^{\rm T}{\bf t}}{{\bf X}^{\rm T}{\bf X}}\]</div>
<p>のような分数が表れないように注意してください．これは行列の計算には割り算が定義されていないためです． そのため，逆行列を使って行列積のみで計算しています．</p>
<p>また，もうひとつよくある間違いとして，<span class="math">\({\bf w}\)</span> を求めるために以下のような式変形をしてしまう例が挙げられます．</p>
<div class="math">
\[\begin{split}\begin{aligned}
{\bf X}^{\rm T}{\bf X}{\bf w}
&amp;= {\bf X}^{\rm T}{\bf t} \\
\left( {\bf X}^{\rm T} \right)^{-1} {\bf X}^{\rm T}{\bf X}{\bf w}
&amp;= \left( {\bf X}^{\rm T} \right)^{-1}{\bf X}^{\rm T}{\bf t} \\
{\bf X}{\bf w}
&amp;= {\bf t} \\
{\bf X}^{-1}{\bf X}{\bf w}
&amp;= {\bf X}^{-1}{\bf t} \\
{\bf w}
&amp;= {\bf X}^{-1}{\bf t}
\end{aligned}\end{split}\]</div>
<p>しかし，これは一般には成立しません． その理由は，逆行列を持つための条件の一つである<strong>正方行列であること</strong>を <span class="math">\({\bf X}\)</span> が常に満たすとは限らないためです． サンプル数 <span class="math">\(N\)</span> と入力変数の数 <span class="math">\(M+1\)</span> が等しくない場合，<span class="math">\({\bf X} \in \mathcal{R}^{N \times (M+1)}\)</span> は正方行列ではなく，逆行列をもちません． 一方，<span class="math">\({\bf X}^{\rm T} {\bf X}\)</span> は <span class="math">\({\bf X}^{\rm T}{\bf X} \in \mathcal{R}^{(M+1) \times (M+1)}\)</span> であり，サンプル数 <span class="math">\(N\)</span>
に依存することなく，常に正方行列となることに注目してください．（逆行列が求まるためにはもう少し厳密な条件がありますが，ここでは説明しません．）</p>
<p>推論の際は学習で得られたパラメータを <span class="math">\(\hat{\bf w}\)</span> として</p>
<div class="math">
\[y_{q} = \hat{\bf w}^{\rm T}{\bf x}_{q}\]</div>
<p>のように計算することで予測値が得られます．</p>
<p>で表されることが分かりました．この最適なパラメータを計算するために，以下の5つを扱います．</p>
<ul class="simple">
<li>ベクトルの定義</li>
<li>行列の定義</li>
<li>転置</li>
<li>行列積</li>
<li>逆行列</li>
</ul>
<p>具体的に，以下のようなデータセットが与えられているケースを想定してみましょう．この例では，データのサンプル数<span class="math">\(N\)</span>は<span class="math">\(4\)</span>であり，入力データ<span class="math">\(X\)</span>の変数の数は<span class="math">\(2\)</span>です．そして<span class="math">\(t\)</span>は教師データとなります．</p>
<div class="math">
\[\begin{split}\boldsymbol{X} =
\begin{bmatrix}
1 &amp; 2 &amp; 3 \\
1 &amp; 2 &amp; 5 \\
1 &amp; 3 &amp; 4 \\
1 &amp; 5 &amp; 9
\end{bmatrix}, \
\boldsymbol{t} =
\begin{bmatrix}
1 \\ 5 \\ 6 \\ 8
\end{bmatrix}\end{split}\]</div>
<p>ここで<span class="math">\(\boldsymbol{X}\)</span>は <strong>パラメータ</strong> <span class="math">\(\boldsymbol{w}\)</span> <strong>がバイアス</strong> <span class="math">\(\boldsymbol{b}\)</span> <strong>を包含する</strong> 形式を想定しており，従って入力データ<span class="math">\(\boldsymbol{X}\)</span>の1列目には<span class="math">\(1\)</span>が格納されています．</p>
<p>それでは実装方法について見ていきましょう．まずは，NumPyの読み込みから始めます．</p>
</div>
</div>
<div class="section" id="NumPyによる実装">
<h2>2.3. NumPyによる実装<a class="headerlink" href="#NumPyによる実装" title="Permalink to this headline">¶</a></h2>
<p>それでは重回帰分析で行われる計算をPythonを用いてコンピュータに実行させてみましょう． Pythonには<strong>NumPy</strong>とよばれる線形代数を簡単に扱えるライブラリが存在し，広く利用されています． 次の章で紹介するChainerの中でもNumPyは用いられており，様々なデータ解析・科学計算のライブラリで広く採用されているため，NumPyの使い方にある程度慣れ親しんでおくことは後々役立ちます．</p>
<p>以下では，Pythonの基本的な文法はすでに理解していることを前提としています． 例えば，変数（数値・文字列，リスト，タプル，辞書），制御構文（for，if），関数，クラスを理解している必要があります．</p>
<p>重回帰分析では，最終的に最適なパラメータ <span class="math">\({\bf w}\)</span> が</p>
<div class="math">
\[{\bf w} = \left( {\bf X}^{\rm T}{\bf X}\right)^{-1}{\bf X}^{\rm T}{\bf t}\]</div>
<p>と閉じた形で求まりました． この計算を行うために，これから以下の5つをNumPyを使って行います．</p>
<ul class="simple">
<li>ベクトルの定義</li>
<li>行列の定義</li>
<li>転置</li>
<li>行列積</li>
<li>逆行列</li>
</ul>
<p>以下のようなデータセットが与えられているとします．</p>
<div class="math">
\[\begin{split}{\bf X} = \left[ \begin{matrix}
1 &amp; 2 &amp; 3 \\
1 &amp; 2 &amp; 5 \\
1 &amp; 3 &amp; 4 \\
1 &amp; 5 &amp; 9
\end{matrix} \right], \
{\bf t} = \left[ \begin{matrix}
1 \\ 5 \\ 6 \\ 8
\end{matrix} \right]\end{split}\]</div>
<p>データ数 <span class="math">\(N = 4\)</span> であり，入力変数の数 <span class="math">\(M = 2\)</span> です． それではこのデータセットにフィットするモデル <span class="math">\(f({\bf x}; {\bf w}) = {\bf x}^{\rm T}{\bf w}\)</span> のパラメータ <span class="math">\({\bf w}\)</span> をNumPyを使って計算してみましょう．</p>
<p>まずNumPyを読み込みます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
<p>ベクトルの定義は以下のように行います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>ベクトルを表示してみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[1 5 6 8]
</pre></div></div>
</div>
<p>行列の定義も行い，表示してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1 2 3]
 [1 2 5]
 [1 3 4]
 [1 5 9]]
</pre></div></div>
</div>
<p>ここでは<code class="docutils literal"><span class="pre">np.array</span></code>という関数を用いて，PythonのリストからNumPyの多次元配列の形式(<code class="docutils literal"><span class="pre">np.ndarray</span></code>)への変換を行っています．</p>
<p>次に，Xの転置を行ってみましょう．<code class="docutils literal"><span class="pre">np.ndarray</span></code>で定義されている場合，<code class="docutils literal"><span class="pre">.T</span></code>をつけるだけで転置することができます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[1 1 1 1]
 [2 2 3 5]
 [3 5 4 9]]
</pre></div></div>
</div>
<p>縦と横が入れ替わっていることを確認できます．</p>
<p>行列積は以下のように <code class="docutils literal"><span class="pre">np.dot</span></code> によって実現できます．行列積を行う際には，一番目の行列の列数と，二番目の行列の行数が同じであることに注意して下さい．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">XX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">XX</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[  4  12  21]
 [ 12  42  73]
 [ 21  73 131]]
</pre></div></div>
</div>
<p>ここからさらに，<span class="math">\({\bf X}^{\rm T}{\bf X}\)</span> に対する逆行列，<span class="math">\(\left( {\bf X}^{\rm T}{\bf X} \right)^{-1}\)</span> を計算します．逆行列を求めるには，<code class="docutils literal"><span class="pre">np.linalg.inv</span></code> を用います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">XX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">XX</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">XX_inv</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[ 1.76530612 -0.39795918 -0.06122449]
 [-0.39795918  0.84693878 -0.40816327]
 [-0.06122449 -0.40816327  0.24489796]]
</pre></div></div>
</div>
<p>これで重回帰分析のために必要な演算が揃いました．</p>
<p>最適なパラメータ <span class="math">\(\left({\bf X}^{\rm T}{\bf X} \right)^{-1} {\bf X}^{\rm T}{\bf t}\)</span> を求めると，</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">Xt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[ 20  70 124]
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XX_inv</span><span class="p">,</span> <span class="n">Xt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[-0.14285714  0.71428571  0.57142857]
</pre></div></div>
</div>
<p>このように求まりました． NumPyを使って数式と同じ計算をプログラムに書き下せば，コンピュータに具体的な数値を使った計算を高速に行わせることができます．</p>
</div>
<div class="section" id="Scikit-learnによる機械学習アルゴリズムの実行">
<h2>2.4. Scikit-learnによる機械学習アルゴリズムの実行<a class="headerlink" href="#Scikit-learnによる機械学習アルゴリズムの実行" title="Permalink to this headline">¶</a></h2>
<p>重回帰分析であればNumPyで比較的容易に実装することができましたが，実践的に使用する機械学習手法のアルゴリズムの多くは複雑であり，初学者が一から書くのは難しい場合も少なくありません． Pythonには<strong>Scikit-learn</strong>と呼ばれる様々な機械学習手法の実装が含められたライブラリが公開されており，初学者でも簡単に扱うことができます．</p>
<p>ここでは重回帰分析を<strong>Scikit-learnを用いて行う方法</strong>を紹介します． データセットは先程と同様に <span class="math">\({\bf X}\)</span> と <span class="math">\({\bf t}\)</span> を使用しますが，Scikit-learnにおいては，<strong>パラメータ</strong> <span class="math">\({\bf w}\)</span> <strong>がバイアス</strong> <span class="math">\({\bf b}\)</span> <strong>を包含しない</strong> 形式を想定しており，入力データ <span class="math">\({\bf X}\)</span> の1列目から <span class="math">\(1\)</span> を取り除く必要があることに注意してください． 従って，</p>
<div class="math">
\[\begin{split}{\bf X} =
\begin{bmatrix}
2 &amp; 3 \\
2 &amp; 5 \\
3 &amp; 4 \\
5 &amp; 9
\end{bmatrix}, \
{\bf t} =
\begin{bmatrix}
1 \\ 5 \\ 6 \\ 8
\end{bmatrix}\end{split}\]</div>
<p>をデータセットとして準備します．</p>
<div class="section" id="Scikit-learn-基礎編">
<h3>2.4.1. Scikit-learn 基礎編<a class="headerlink" href="#Scikit-learn-基礎編" title="Permalink to this headline">¶</a></h3>
<p>Scikit-learnは<code class="docutils literal"><span class="pre">sklearn</span></code>という名前で読み込みます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">sklearn</span>
</pre></div>
</div>
</div>
<p>重回帰分析を使用する場合は以下の <code class="docutils literal"><span class="pre">LinearRegression</span></code> というクラスを読み込みます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>
</pre></div>
</div>
</div>
<p>なお，使い方を調べる際には，<a class="reference external" href="http://scikit-learn.org/">公式のリファレンス</a>に加えて，使用例を見るのも有用です（例えば検索エンジンで「重回帰分析 Scikit-learn」のようなキーワードで検索すればたくさんの使用例が見つかります）．</p>
<p><code class="docutils literal"><span class="pre">LinearRegression</span></code> クラスを利用するために，インスタンス化を行い，<code class="docutils literal"><span class="pre">model</span></code> と名付けます．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>これで重回帰分析を行う準備が完了しました． この <code class="docutils literal"><span class="pre">model</span></code> を使って，最適なパラメータを求めるには以下のようにします．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># データセットの定義</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>

<span class="c1"># 最適なパラメータの計算</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
</pre></div>
</div>
</div>
<p>求まったパラメータの検証結果を以下のようにして確認することができます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.6923076923076923
</pre></div>
</div>
</div>
<p><code class="docutils literal"><span class="pre">LinearRegression</span></code> クラスのオブジェクトが持つ <code class="docutils literal"><span class="pre">score()</span></code> メソッドを呼ぶと，以下の式で示される，<strong>決定係数</strong>とよばれる指標が計算され，結果が返されます．</p>
<div class="math">
\[R^{2} = 1 - \dfrac{\sum_{i}\left( t_{i} - y_{i} \right)^{2}}{\sum_{i}\left( t_{i} - \bar{t} \right)^{2}}\]</div>
<p>Scikit-learnは，よく使う機械学習手法を簡単に切り替えて使えるよう，統一されたインターフェースで様々な機械学習手法が実装されています． Scikit-learnが好まれる理由の一つには，様々なアルゴリズムが「<code class="docutils literal"><span class="pre">.fit()</span></code>で学習，<code class="docutils literal"><span class="pre">.score()</span></code>で検証」という同じインターフェースから利用できることが挙げられるでしょう．</p>
<p>また，アルゴリズムによって内容は多少異なりますが，パラメータも <code class="docutils literal"><span class="pre">model</span></code> オブジェクトに属性として格納されているため，学習後に確認することができます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># パラメータw</span>
<span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>array([0.71428571, 0.57142857])
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># バイアスb</span>
<span class="n">model</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>-0.14285714285714235
</pre></div>
</div>
</div>
</div>
<div class="section" id="Scikit-learn-応用編">
<h3>2.4.2. Scikit-learn 応用編<a class="headerlink" href="#Scikit-learn-応用編" title="Permalink to this headline">¶</a></h3>
<p>Scikit-learnは機械学習の実装を支援する多くの機能を兼ね備えています．本節では，サンプルデータセットの使用方法，及びデータセットの分割方法について紹介していきます．</p>
<div class="section" id="サンプルデータセットの使用">
<h4>2.4.2.1. サンプルデータセットの使用<a class="headerlink" href="#サンプルデータセットの使用" title="Permalink to this headline">¶</a></h4>
<p>まずはじめにサンプルデータセットの取り扱いを紹介します． Scikit-learnでは，幾つかのデータセットが提供されています． 今回はその中から，米国ボストン市郊外における地域別の物件価格のデータセットを使用することにします．</p>
<p>このデータセットには 506 件のデータが登録されており，各サンプルには対象地域の平均物件価格と，それに紐づく情報として対象地域の平均的な物件情報（一戸あたりの部屋数，築年数，雇用施設からの距離など），人口統計情報（低所得者の割合，教師あたりの生徒数など），生活環境に関する情報（犯罪発生率など）などが含まれています． このデータセットを利用して，物件や人口統計などの情報から，平均物件価格を予測するモデルを構築してみましょう．</p>
<p>入力変数は全部で13種類あり，詳細は以下の通りです．</p>
<ul class="simple">
<li>CRIM : 人口<span class="math">\(1\)</span>人あたりの犯罪発生率</li>
<li>ZN : <span class="math">\(25,000\)</span>平方フィート以上の住宅区画が占める割合</li>
<li>INDUS : 非小売業が占める面積の割合</li>
<li>CHAS : チャールズ川に関するダミー変数 (1 : 川沿い，0 : それ以外)</li>
<li>NOX : 窒素酸化物の濃度</li>
<li>RM : 住居あたりの平均部屋数</li>
<li>AGE : 1940年以前に建てられた物件の割合</li>
<li>DIS : 5つのボストン雇用施設からの重み付き距離</li>
<li>RAD : 都心部の幹線道路へのアクセス指数</li>
<li>TAX : $ <span class="math">\(10,000\)</span>あたりの固定資産税の割合</li>
<li>PTRATIO : 教師1人あたりの生徒数</li>
<li>B : 黒人の比率を表す指数</li>
<li>LSTAT : 低所得者の割合</li>
</ul>
<p>それでは， <code class="docutils literal"><span class="pre">load_boston()</span></code> 関数を実行して，データセットを読み込みましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_boston</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p><code class="docutils literal"><span class="pre">boston</span></code> はPythonの辞書と同じインターフェースが備わっており，<code class="docutils literal"><span class="pre">'data'</span></code> キーに入力値が，<code class="docutils literal"><span class="pre">'target'</span></code> キーに目標値が格納されています．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">X</span> <span class="o">=</span> <span class="n">boston</span><span class="p">[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">boston</span><span class="p">[</span><span class="s1">'target'</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]
 [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]
 [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]
 ...
 [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]
 [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]
 [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4
 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8
 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6
 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4
 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9
 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9
 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7
 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8
 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4
 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8
 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4
 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8
 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2
 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.
 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.
 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1
 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5
 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8
 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8
 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1
 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9
 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2
 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1
 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1
 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6
 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8
 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3
 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2
  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.
 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4
 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3
 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6
 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7
 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3
 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.
  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9
 22.  11.9]
</pre></div></div>
</div>
<p>NumPyの形式で入力データと教師データが格納されているため，<code class="docutils literal"><span class="pre">.shape</span></code> を使うことで行と列の数を確認できます．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>(506, 13)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">t</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>(506,)
</pre></div>
</div>
</div>
<p>入力値 <code class="docutils literal"><span class="pre">X</span></code> には，506 件分のデータが格納されていることが分かりました． 各サンプルは 13 次元のベクトルとなっており，これはそれぞれが 13 種類の入力変数を持っていることを表しています． 目標値 <code class="docutils literal"><span class="pre">t</span></code> には，入力変数に対応する出力変数の値として，平均物件価格のスカラー値が 506 件分格納されています．</p>
</div>
<div class="section" id="データセットの分割">
<h4>2.4.2.2. データセットの分割<a class="headerlink" href="#データセットの分割" title="Permalink to this headline">¶</a></h4>
<p>つぎに，このデータセットを <strong>訓練データセット</strong> と <strong>テストデータセット</strong> に分割する方法をご紹介します． モデルの性能の評価を訓練の時に使ったデータを使って行っても，<strong>訓練中に見たことが無い未知のデータに対してもうまく働くかどうか</strong>が分かりません．
多くの場合機械学習手法の目標は既知のデータから一般的な法則やパターンを見つけ出し，未知のデータに対しても正しく予測等が行えるようになることであるので，訓練済みモデルの評価は訓練には用いていない（モデルにとって）未知のデータ，<strong>テストデータ</strong>を使って行う必要があります． 訓練データとテストデータは，用意したデータセットを分割して作成することができます． この方法を <strong>ホールドアウト法</strong> と呼びます．</p>
<p>Scikit-learnではデータセットを訓練用とテスト用に分割するための機能が用意されています． これを用いて上で読み込んだデータセットを２つに分割してみましょう．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">t_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>(354, 13)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>(152, 13)
</pre></div>
</div>
</div>
<p><code class="docutils literal"><span class="pre">train_test_split()</span></code> 関数の引数 <code class="docutils literal"><span class="pre">test_size</span></code> にはテスト用に使うデータ数のデータセット全体のデータ数に対する比率を指定します． <span class="math">\(0.3\)</span> と指定すると全体の <span class="math">\(30\)</span>% がテストデータセットが持つデータの数となります． また，<code class="docutils literal"><span class="pre">random_state</span></code> は乱数のシードであり，これに与える値を固定すると，データセット中からどのデータを訓練用に割り振り，どのデータをテスト用に割り振るか，という選択が固定されます．
この選択はランダムに行われますが，乱数シードを固定すればそのランダムな選択を再現することができるためです．</p>
<p>それでは，訓練データを用いて学習を行います．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
</pre></div>
</div>
</div>
<p>訓練が終了しました． <code class="docutils literal"><span class="pre">score()</span></code> を用いたモデルの検証を訓練データとテストデータの両方に対して行ってみましょう．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[35]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># 訓練データ</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[35]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.7645451026942549
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[36]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># テストデータ</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[36]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.6733825506400171
</pre></div>
</div>
</div>
<p>テストデータだけでなく，訓練データでもモデルの検証を行うことで良いモデルの獲得に失敗している場合はその原因について考察するのに役立つ情報を得ることができることがあります．</p>
</div>
<div class="section" id="アンダーフィッティングとオーバーフィッティング">
<h4>2.4.2.3. アンダーフィッティングとオーバーフィッティング<a class="headerlink" href="#アンダーフィッティングとオーバーフィッティング" title="Permalink to this headline">¶</a></h4>
<p>モデルが訓練データに対してすら良い精度で予測できないという状態は，<strong>アンダーフィッティング</strong>と呼ばれます． アンダーフィッティングが起きている場合，現状の機械学習手法もしくはモデルではうまくデータの特徴を捉えられていないと考えられ，アルゴリズムを変更したり，入力データの特徴をより適切に表現できるような変換をモデルの訓練の前に施すことを考えるなど，新たな工夫を加える必要があります．
一方，訓練データに対しては良い精度で予測が行えるモデルになっているにも関わらず，テストデータに対してのパフォーマンスが良くない，という場合は，モデルが訓練データセットに<strong>オーバーフィッティング</strong>していると言います． この場合，訓練データにあまりにも特化したモデルになってしまっている可能性があります． この問題を避ける方法が現在も活発に研究されていますが，よく行われる対策としては正則化（パラメータが取りうる値に制約をもたせるなど）やモデルの自由度（パラメータ数など）の調整があります．
また，<strong>ハイパーパラメータ</strong>とよばれる各アルゴリズムの動作を制御するのに使われるパラメータ値を調整していくことも行われます． このように，一口に望ましい結果が得られないといっても，原因としては様々なものがあり得るため，訓練データセットとテストデータセットの両方に対して訓練済みモデルの性能評価を行っておくことは，その原因を探るにあたって重要になります．</p>
</div>
<div class="section" id="Scikit-learnを使った前処理">
<h4>2.4.2.4. Scikit-learnを使った前処理<a class="headerlink" href="#Scikit-learnを使った前処理" title="Permalink to this headline">¶</a></h4>
<p>Scikit-learnを用いると，データに対する前処理も簡単に行うことができます． 例えば，訓練データセットの入力値の集合が，平均0，標準偏差1となるように変換を行う場合は，以下のようにします．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[37]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">StandardScaler</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[38]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># インスタンス化</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>訓練データを用いて，平均と分散を計算します．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># 平均と分散を計算</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>StandardScaler(copy=True, with_mean=True, with_std=True)
</pre></div>
</div>
</div>
<p>計算された平均，分散を用いて，訓練データ及びテストデータをスケーリングします．</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[40]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># 変換</span>
<span class="n">X_train_s</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_s</span>  <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>テストデータをスケーリングする際にも，訓練データの平均・分散を利用していることに注意しましょう． テストデータはモデルにとっては未知のデータセットでなければならないにも関わらず，訓練データとテストデータを合わせた全データの平均・分散を利用して訓練データのスケーリングを行ってしまうと，本来知りえないはずのテストデータの情報をモデルに間接的に与えてしまうことになります． それを避けるために，訓練データのみを用いて統計値の計算を行い，これを用いてスケーリングを行っています．</p>
<p>訓練データとテストデータでは平均・分散が異なるため，訓練データの平均・分散でスケーリングされたテストデータについては，その平均が<span class="math">\(0\)</span>，分散が<span class="math">\(1\)</span>になるとは限らないことに注意してください．</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[41]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">X_train_s</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[-0.20735619 -0.49997924  1.54801583 ...  1.2272573   0.42454294
   3.10807269]
 [-0.38886492  0.34677427 -0.58974728 ...  0.05696346  0.40185312
  -0.66643035]
 [-0.33573486 -0.49997924  1.54801583 ...  1.2272573   0.39846135
   0.63936662]
 ...
 [-0.38450355 -0.49997924 -0.15303077 ... -0.30312696  0.39659002
  -0.30284441]
 [-0.37511786 -0.49997924 -0.59690657 ... -0.25811566  0.37588849
   0.89967717]
 [-0.38592298 -0.49997924 -1.00641779 ... -0.84326258  0.42454294
   0.31822262]]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">X_test_s</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[-0.39454262 -0.49997924 -1.12239824 ... -0.70822867  0.17086147
  -0.72160487]
 [ 0.70419882 -0.49997924  1.00534187 ...  0.77714428  0.0648977
  -0.41177872]
 [-0.38890688 -0.49997924  0.4025299  ... -0.93328518  0.38758427
  -0.27454978]
 ...
 [ 1.61285743 -0.49997924  1.00534187 ...  0.77714428  0.42454294
   2.59876943]
 [-0.34350332 -0.49997924 -0.1687812  ... -0.03305915  0.42454294
  -1.11772962]
 [-0.39902507 -0.49997924 -1.27417512 ...  0.10197476  0.39202867
  -1.02294263]]
</pre></div></div>
</div>
<p>この他，Scikit-learnはロジスティック回帰やサポートベクターマシン，ランダムフォレストなど様々な機械学習手法をサポートしています．</p>
<p>それらについても，前節までで紹介した重回帰分析を行う場合と同様に，モデルをインスタンス化し，学習データを引数に与えて <code class="docutils literal"><span class="pre">.fit()</span></code> 関数で訓練を行い，<code class="docutils literal"><span class="pre">.score()</span></code> 関数で結果を評価できるようになっています．</p>
<p>より詳しく知りたい方は<a class="reference external" href="https://scikit-learn.org/">Scikit-learn</a>サイトや解説サイトなどを参照してください．</p>
</div>
</div>
</div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="03_Introduction_to_Neural_Network.html" rel="next" title="3. ニューラルネットワークの基礎">Next <span class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral" href="01_Basic_Math_for_ML.html" rel="prev" title="1. 機械学習に必要な数学の基礎"><span class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        © Copyright 2018, Preferred Networks &amp; キカガク

    </p>
</div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../',
              VERSION:'',
              LANGUAGE:'ja',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
<script src="../_static/jquery.js" type="text/javascript"></script>
<script src="../_static/underscore.js" type="text/javascript"></script>
<script src="../_static/doctools.js" type="text/javascript"></script>
<script src="../_static/translations.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
</body>
</html>