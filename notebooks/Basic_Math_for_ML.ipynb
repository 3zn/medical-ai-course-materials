{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Math for ML",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hillbig/medical-ai-course-materials/blob/master/notebooks/Basic_Math_for_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_atjBBsxuKH7"
      },
      "cell_type": "markdown",
      "source": [
        "# 機械学習に必要な数学の基礎\n",
        "\n",
        "本章では，ディープラーニングを含めた機械学習に必要な数学の基礎である微分，線形代数，統計の3つについて，簡潔に紹介していきます．\n",
        "\n",
        "\n",
        "\n",
        "## 機械学習とは\n",
        "\n",
        "機械学習は，コンピュータがデータから学習することで，望むような挙動をする関数を獲得する手法です．これら関数の多くは**パラメータ**によって特徴づけられており，パラメータを決めればその関数の挙動が決まります．例えば，直線の関数は傾き$a$と切点$b$の２つのパラメータで特徴づけられ，$f(x; a, b) = ax + b$のように表されます．関数の表記中の$；$の後ろにある変数は引数ではなく，パラメータを表します．機械学習の目標は，学習データを使って，これらのパラメータを決定することです．\n",
        "\n",
        "機械学習は，**目的関数**を最小化することで学習，つまり望ましい挙動をするようなパラメータを決定します．この目的関数は関数が望ましい値をとる時に小さく，望ましくない値をとる時に大きな値をとるように設計された関数です．\n",
        "\n",
        "例えば，学習データとして入力と出力のペアからなるデータセット$D={(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)}$が与えられ，これらの点の近くをできる限り通るような直線$f(x; a, b) = ax + b$を学習したい場合，\n",
        "\n",
        "$L( \\theta) = \\sum_{i=1}^n (y_i - f(x_i; \\theta))^2$\n",
        "\n",
        "を目的関数とし，これを最小化するようにします．上式は，それぞれのデータ点で現在のモデルの予測値$f(x_i; \\theta)$と，正解$y_i$との二乗誤差の合計であり，予測と正解が一致する時だけ$0$，それ以外は，大きく間違えるほど大きな正の値をとるような関数です．目的関数では入出力$x_i, y_i$が引数ではなく，パラメータ$\\theta$を引数としていることに注意してください．このような間違えた度合いを測る関数を**損失関数**と呼びます．\n",
        "\n",
        "目的関数の最小化問題を解くためには微分と線形代数の知識が必要になります．ただし，全ての微分と線形代数の知識は必要ありません．以降，機械学習の理解に必要な最低限の知識をみていきます．\n",
        "\n",
        "## 微分\n",
        "\n",
        "微分は関数の**接線の傾き**を表します．接線の定義は後で説明しますが，意味としては下の図のように関数に接するような直線です．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/01.png)\n",
        "\n",
        "例えば上図の関数においては，$a$ の点における接線は赤い直線であり，その傾きは$+3$となっています．傾きが$+3$ということは，入力の値が$1$増えると，関数の値が$3$増えるという意味です．右肩上がりな直線の傾きは正の値になります．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/02.png)\n",
        "\n",
        "それに対し，次の図の $b$ の点においては，傾きは-1であり，接線は右肩下がりの直線となっています．このように，微分値を計算することで，与えられた点における接線の傾きを求めることができます．\n",
        "\n",
        "一般に目的関数がどのような形をとっているかはわかりません．もしかしたら山あり谷ありで複雑な形をしているかもしれません．もし，目的関数の（パラメータについての）微分を計算可能であれば，目的関数の全体の形がわからなくても，現在のパラメータにおいてパラメータを変化させた時に目的関数がどう変化するのかが分かります．この情報に基づいて，目的関数を小さくするようにパラメータを更新することができます．\n",
        "\n",
        "再度，微分の説明に戻り，その定義や多変数入力，多変数出力の場合について詳しくみていきます．\n",
        "\n",
        "### 2点間を通る直線の傾き\n",
        "\n",
        "はじめに，微分の原理を理解していくために，下図に示す2点間を通る直線の傾き $a$ を求めてみましょう．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/04.png)\n",
        "\n",
        "この時，傾き$a$ = $f(x)$の変化量 / $x$の変化量　より，\n",
        "$$\n",
        "a = \\dfrac{f(x_{2}) - f(x_{1})}{x_{2}-x_{1}}\n",
        "$$\n",
        "と求まります．このように二点間の直線の傾きは，その直線の範囲で入力の単位量の変化あたり関数の値がどれだけ変わるかを表します．\n",
        "\n",
        "### 1点での接線の傾き\n",
        "\n",
        "次に，与えられた関数の接線の傾きを求めていきます．そのためには，**極限**の考えが必要になります．極限では，変数がある値に限りなく近づくとき，その変数によって記述される関数がどのような振る舞いをするか考えます．極限を表すために，$\\lim$ という記号が一般的に用いられます．例えば，\n",
        "$$\n",
        "\\displaystyle \\lim _{x\\rightarrow 0}3x=3\\times 0=0\n",
        "$$\n",
        "は，$x$という変数を$0$に近づけていったときに式の値がどのような値になるかを与えます．\n",
        "\n",
        "それでは，下図のある点 $x$ における接線の傾き$a$を求めていきましょう．先程は関数の2点を通る直線でしたが，今回は1点での接線の傾きを求めていきます．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/05.png)\n",
        "\n",
        "さきほど考えた2点を通る直線と極限を組み合わせて，接線を求めることができます．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/06.png)\n",
        "\n",
        "はじめに，$x$ から $h$ だけ離れた点 $x+h$ を考え，2点を通る直線の傾きを求めてみます．次に$h$を$h \\rightarrow 0$ のように小さくしていけば，直線の開始点と終了点の2点が1点に収束し，1点での接線として考えることができます．これを式でみると\n",
        "$$\n",
        "\\begin{aligned}\n",
        "a &=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{\\left( x+h\\right) -x}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{h}\\\\\n",
        " &=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{h}\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "となります．上の式は**導関数**と呼び，$f'(x)$ で表されます．また導関数を求めることを**微分する**といいます．また，記号の使い方として，\n",
        "$$\n",
        "(\\cdot)' = \\dfrac{d}{dx}(\\cdot)\n",
        "$$\n",
        "のように表しても構いません．この$d$という記号は微分（differentiation）を表しており，$d(\\cdot)$が対象の値の変化量，$dx$が$x$の変化量を表し，それらを小さくしていった時の極限を表します．この記法は煩雑ですが，どの変数を対象に微分しているかが明確になるため，正確な表現をすることができます．\n",
        "\n",
        "### 微分の公式\n",
        "\n",
        "微分には，覚えておくと便利な公式が幾つかあります．\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\left( c\\right) ^{'}&=0 \\\\\n",
        "\\left( x\\right)^{'}&=1\\\\\n",
        "\\left( cf(x) \\right)^{'} &= c f'(x) \\\\\n",
        "\\left( x^{n} \\right)^{'} &=nx^{n-1} \\\\\n",
        "\\left( f(x) + g(x) \\right) ^{'} &=f^{'}(x)+g^{'}(x) \\\\\n",
        "\\left( f(x) g(x) \\right) ^{'} &= f^{'}(x)g(x) + f(x)g^{'}(x) \\\\\n",
        "\\left( f(g(x)) \\right) ^{'} &= \\frac{df(u)}{du}\\frac{du}{dx}, u = g(x)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "例えば，以下の微分を考えてみましょう．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "( 3x^{2})'&=3\\times (x^{2})'\\\\\n",
        "&=3\\times 2x\\\\\n",
        "&=6x\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "この $(3x^{2})' = 3 \\times (x^{2})'$ の部分に注目してください．微分では，このように定数の係数（変数にかかる数）は微分の演算の外側に出すことができます．また，次の例題でも新しい特性があります．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left( 3x^{2}+4\\right)'&=\\left( 3x^{2}\\right)'+\\left( 4\\right)'\\\\\n",
        "&=3\\times \\left( x^{2}\\right)'+4\\times \\left( 1\\right)'\\\\\n",
        "&=3\\times 2x+4\\times 0\\\\\n",
        "&=6x\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "上記の例題では，$\\left( 3x^{2}+4\\right)'=\\left( 3x^{2}\\right)'+\\left( 4\\right)'$ のように，全体の和をとった後に微分の演算を行っても，それぞれで微分の演算を行った後に和の計算をしても同じとなっています．これは微分の**線形性**と呼ばれる性質であり，この性質を使うことで，微分を簡単に計算できるようになります．\n",
        "\n",
        "### 合成関数の微分\n",
        "\n",
        "機械学習では関数は複雑な形をとり，それらの微分の計算も複雑になりがちです．例えば，\n",
        "\n",
        "$$\n",
        "\\left\\{ (3x + 4)^{2} \\right\\}'\n",
        "$$\n",
        "\n",
        "の場合，$3x+4$ という内側の部分と $(\\cdot)^{2}$ という外側の部分で構成されています．この式を$(9x^2 + 24x + 16)'$のように展開してから微分を計算してもよいのですが，これが3乗や4乗となってくると展開するのも大変になります．この時に役に立つ考え方が**合成関数の微分**の公式です．先程の微分の便利な公式の最後にでていた式です．合成関数の微分は，内側の微分と外側の微分をそれぞれ行い，その結果をかけ合わせます．外側の微分の際には関数の引数を入力とみなし，その入力について微分をとります．\n",
        "\n",
        "それでは先程の$(3x+4)^2$という関数の微分を考えてみます．\n",
        "\n",
        "まず内側の関数を $u = (3x+4)$ とおいて，\n",
        "\n",
        "$$\n",
        "\\left\\{ (3x + 4)^{2} \\right\\}' = (u^{2})'\n",
        "$$\n",
        "\n",
        "とします．ここで，$(\\cdot)'$ をもう少し厳密に考える必要が出てきます．今は，$x$ と $u$ の2つの変数が登場しており，$(\\cdot)'$ では，$x$ で微分しているのか $u$ で微分しているのかの区別がつきません．そこで，多少複雑に見えますが，先程紹介した$d$を使った記法で微分する変数を厳密に記述すると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left\\{ (3x + 4)^{2} \\right\\}' &= \\dfrac{d}{dx} \\left\\{ (3x + 4)^{2} \\right\\} \\\\\n",
        "&= \\dfrac{d}{dx} (u^2) \\\\\n",
        "&=  \\dfrac{d}{dx} f(u) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "となり，$u$ の関数 $f(u) = u^{2}$ に対して，$x$ で微分していることがわかります．\n",
        "\n",
        "この合成関数の微分の公式を覚える際には，微分をする際に何か良い中間変数$u$（先程の例では$u=3x+4$）をおき，$du$を分母と分子においた式を考えます．$du$は分子と分母に出現しており，（実際には極限の操作であるため約分はできませんが，）約分できるとして消去してしまえば元の式と同じになります．\n",
        "\n",
        "なお，今回は分数の約分という形で大雑把に説明しましたが，実際には極限を使って合成関数の微分が成り立つことを証明できます．ニューラルネットワークの学習では合成関数の微分を使用する場面が何度も登場するため，この計算方法をしっかりと覚えておきましょう．\n",
        "\n",
        "### 偏微分\n",
        "\n",
        "微分の最後のトピックとして，**偏微分**を紹介します．機械学習では，1つの入力変数 $x$ から出力変数 $y$ を予測するケースは稀であり，基本的には，複数の入力変数 $x_{1}$, $x_{2}$, $\\ldots$, $x_{M}$ を用いて出力変数 $y$ を予測する多変数関数を扱います．例えば，家賃を予測する場合，部屋の広さだけではなく，駅からの距離や犯罪発生率なども考慮した方がより正確に予測できると期待されます．複数の入力$x_1, x_2, \\ldots, x_M$を考慮した関数$f(x_1, x_2, \\ldots, x_M)$を多変数関数と呼びます．この多変数関数において，ある入力$x_m$にのみ注目して微分することを偏微分と呼び，\n",
        "\n",
        "$$\n",
        "\\dfrac{\\partial}{\\partial x_{m}} f(x_{1}, x_{2}, \\ldots, x_{M})\n",
        "$$\n",
        "\n",
        "のように表します．微分を表す記号が，$d$ から $\\partial$ に変わり，計算としては $\\dfrac{\\partial}{\\partial x_{m}}$ の場合は $x_{m}$ 以外は定数と考え，$x_{m}$ にのみ着目して微分を行います．（ただし，入力変数が他の入力変数と独立ではない場合は定数と考えることはできません．本講義ではそのようなケースは出てきません）．\n",
        "\n",
        "例題で具体的な計算の流れを確認していきましょう．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}+4x_{2}\\right) &=\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}\\right) +\\dfrac {\\partial }{\\partial x_{1}}\\left( 4x_{2}\\right) \\\\\n",
        "&=3\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( x_{1}\\right) +4x_{2}\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( 1\\right) \\\\\n",
        "&=3\\times 1+4x_{2}\\times 0\\\\\n",
        "&= 3\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "偏微分でも微分と同じ公式を適用できます．今回のケースでは，$x_{1}$ にだけ着目するため，$x_{2}$ は定数として扱うことを把握しておけば上記の計算の流れが理解できるはずです．\n",
        "\n",
        "## 線形代数\n",
        "\n",
        "### 線形代数とは\n",
        "\n",
        "次に，**線形代数** について解説します．ベクトル，行列，ランク，逆行列などが登場します．\n",
        "\n",
        "線形代数を導入することで，複数の変数間の関係をシンプルに記述可能となるため，機械学習の中でも度々登場してきます．大変重要な概念ですので，ぜひ身に着けていきましょう．\n",
        "\n",
        "### スカラー，ベクトル，行列，テンソル\n",
        "\n",
        "最初に線形代数で使われるスカラー，ベクトル，行列，テンソルの4つを解説します．\n",
        "\n",
        "**スカラー**は，1つの値もしくは変数のことです．例えば，\n",
        "\n",
        "$$\n",
        "x, \\ y,\\  M,\\  N\n",
        "$$\n",
        "\n",
        "のように表します．スカラーは例えば温度や身長といった単一の量を表すことに使われます．\n",
        "\n",
        "**ベクトル**は，複数のスカラーを縦方向（もしくは横方向）に集めて並べたものであり，\n",
        "\n",
        "$$\n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2} \\\\\n",
        "x_{3}\n",
        "\\end{bmatrix}, \\\n",
        "\\boldsymbol{y}=\\begin{bmatrix}\n",
        "y_{1} \\\\\n",
        "y_{2} \\\\\n",
        "\\vdots \\\\\n",
        "y_{N}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "のように表します．ベクトルの表記は太文字とする場合が多く，スカラーかベクトルかを区別できるようにしています．ベクトルを縦方向に定義するか横方向に定義するかは分野によって異なりますが，数学や機械学習では縦方向で定義している論文や参考書が多いため，本講義では**ベクトルは縦方向**で統一します．このような縦方向のベクトルを列ベクトルと呼びます．また横方向のベクトルを行ベクトルと呼びます．\n",
        "\n",
        "**行列**は複数の同じサイズのベクトルを並べたものであり，\n",
        "\n",
        "$$\n",
        "\\boldsymbol{X}=\\begin{bmatrix}\n",
        "x_{11} & x_{12} \\\\\n",
        "x_{21} & x_{22} \\\\\n",
        "x_{31} & x_{32}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "のように表します．行列のサイズは行と列で表現します．例えば，この $\\boldsymbol{X}$ は3行2列であり，サイズが(3, 2)の行列と言います．多くの場合，行列は大文字，または大文字の太文字で表記されます．\n",
        "\n",
        "**テンソル**はベクトルや行列を一般化した概念であり，ベクトルは1階のテンソル，行列は2階のテンソルと表現することができます．また，図のように行列を奥行き方向にさらに並べたものは3階のテンソルとなります．例えば，RGB (Red Green Blue) などの色空間で表現するカラー画像を表現する場合，（行番号，列番号，色）の3つの軸で一つの値を指定することができ，3階のテンソルで表現可能です．本講座を含めて，単にテンソルと表現されている場合には，3階以上のテンソルを指してることが多いので，注意してください．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/07.png)\n",
        "\n",
        "\n",
        "\n",
        "線形代数では $\\boldsymbol{y}$ や $\\boldsymbol{X}$ といった文字だけで式変形をしていくため，どのような形の数値が取り扱われているかわかりくいのですが，これはベクトルなどと常に意識しておくことでその形を見失わないように注意しましょう．\n",
        "\n",
        "|        | 小文字         | 大文字         |\n",
        "| ------ | -------------- | -------------- |\n",
        "| 細文字 | スカラーの変数 | スカラーの定数 |\n",
        "| 太文字 | ベクトル       | 行列，テンソル |\n",
        "\n",
        "### 足し算・引き算\n",
        "\n",
        "行列やベクトルの演算について覚えていきましょう．足し算は同じサイズの行列，ベクトル間だけで成立し，次のように定義されます．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}&\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "3\n",
        "\\end{bmatrix}+\\begin{bmatrix}\n",
        "4 \\\\\n",
        "5 \\\\\n",
        "6\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "1+4 \\\\\n",
        "2+5 \\\\\n",
        "3+6\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "7 \\\\\n",
        "8 \\\\\n",
        "9\n",
        "\\end{bmatrix}\\\\\n",
        "&\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6\n",
        "\\end{bmatrix}+\\begin{bmatrix}\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12 \n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "1+7 & 2+8 & 3+9 \\\\\n",
        "4+10 & 5+11 & 6+12\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "8 & 10 & 12 \\\\\n",
        "14 & 16 & 18\n",
        "\\end{bmatrix}\\end{aligned}\n",
        "$$\n",
        "\n",
        "このように行列やベクトルの中の**要素**で対応する場所を足し合わせます．引き算も同様です．**同じサイズでないと計算が成立しない**ということを覚えておきましょう．\n",
        "\n",
        "### 内積\n",
        "\n",
        "同じサイズのベクトル間では内積が定義できます．内積は同じ位置の対応する値同士を掛けていき，それらを足し合わせたものです．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}& \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\cdot \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} = 1 \\cdot 4 + 2 \\cdot 5  + 3 \\cdot 6 = 36 \\end{aligned}\n",
        "$$\n",
        "\n",
        "### かけ算（行列積）\n",
        "\n",
        "行列の掛け算には，行列積，外積，要素積（アダマール積）など複数種あります．ここではそのうち最もよく使われる**行列積**について説明します．以降では明示しない限り行列の掛け算は行列積を指すこととします．\n",
        "\n",
        "行列$A$と行列$B$の行列積は$A$の各行と$B$の各列の内積を並べたものとして定義されます．例えば行列Aの2行目の行ベクトルと行列Bの3列目の列ベクトルの内積は結果の行列Cの2行3列目に対応します．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/08.png)\n",
        "\n",
        "そして，内積が定義される条件はベクトルのサイズが等しいということでしたが，ここでもそれが成り立つために，Aの行のサイズ（=Aの列数）とBの列のサイズ（=Bの行数）が一致する必要があります．また，結果はAの行数とBの列数からなる行列となります．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/09.png)\n",
        "\n",
        "行列積は線形代数や機械学習の多くの問題で使われます．また，行列では割り算に相当する演算はありませんが，後述する逆行列を使って$4 / 2 = 4 \\times \\dfrac{1}{2}$ のように割り算を逆数（逆行列）の掛け算として記述します．\n",
        "\n",
        "それでは，計算条件の確認も踏まえて，下記の３つを練習問題として解いてください．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\left( 1\\right) \n",
        "\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "3 \\\\ \n",
        "4\n",
        "\\end{bmatrix}\\\\ \n",
        "&\\left( 2\\right) \n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\ \n",
        "3 & 4 \n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "5 \\\\ \n",
        "6 \n",
        "\\end{bmatrix}\\\\ \n",
        "&\\left( 3\\right) \n",
        "\\begin{bmatrix} \n",
        "1 & 2 \n",
        "\\end{bmatrix}\\begin{bmatrix} \n",
        "3 & 4 \\\\ \n",
        "5 & 6 \n",
        "\\end{bmatrix}\\begin{bmatrix} \n",
        "3 \\\\ \n",
        "1\n",
        "\\end{bmatrix} \n",
        "\\end{aligned} \n",
        "$$ \n",
        "\n",
        "こちらが解答です．\n",
        "\n",
        "$$\n",
        "\\begin{aligned} \n",
        "&\\left( 1\\right) \n",
        "\\begin{bmatrix} \n",
        "1 & 2 \n",
        "\\end{bmatrix}\\begin{bmatrix} \n",
        "3 \\\\ \n",
        "4 \n",
        "\\end{bmatrix} = 1\\times 3 + 2 \\times 4 = 11\\\\ \n",
        "&\\left( 2\\right) \n",
        "\\begin{bmatrix} \n",
        "1 & 2 \\\\ \n",
        "3 & 4\n",
        "\\end{bmatrix}\\begin{bmatrix} \n",
        "5 \\\\ \n",
        "6\n",
        "\\end{bmatrix} = \\begin{bmatrix} \n",
        "1 \\times 5 + 2 \\times 6 \\\\ \n",
        "3 \\times 5 + 4 \\times 6 \n",
        "\\end{bmatrix} = \\begin{bmatrix} \n",
        "17 \\\\ \n",
        "39 \n",
        "\\end{bmatrix}\\\\ \n",
        "&\\left( 3\\right) \n",
        "\\begin{bmatrix} \n",
        "1 & 2 \n",
        "\\end{bmatrix}\\begin{bmatrix} \n",
        "3 & 4 \\\\ \n",
        "5 & 6 \n",
        "\\end{bmatrix}\\begin{bmatrix} \n",
        "3 \\\\ \n",
        "1 \n",
        "\\end{bmatrix} \n",
        "=\\begin{bmatrix} \n",
        "1 & 2 \n",
        "\\end{bmatrix}\\begin{bmatrix} \n",
        "3 \\times 3 + 4 \\times 1 \\\\ \n",
        "5 \\times 3 + 6 \\times 1 \n",
        "\\end{bmatrix} = \\begin{bmatrix} \n",
        "1 & 2 \n",
        "\\end{bmatrix}\\begin{bmatrix} \n",
        "13 \\\\ \n",
        "21 \n",
        "\\end{bmatrix}\n",
        "= 1 \\times 13 + 2 \\times 21 \n",
        "=55\n",
        "\\end{aligned} \n",
        "$$\n",
        "\n",
        "この形の計算は，機械学習においてよく登場してきます．行列積は，演算後に形が変わることを覚えておきましょう．\n",
        "\n",
        "\n",
        "\n",
        "### 転置\n",
        "\n",
        "ベクトルは縦向きの列ベクトルを基本としていましたが，横向きのベクトルを使いたい場合もあります．そこで縦向きのベクトルを横向きのベクトルに，横向きのベクトルを縦向きのベクトルに入れ替える演算を**転置**（Transpose）と呼び，$T$で表記します．例えば，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "3\n",
        "\\end{bmatrix}, \\ \n",
        "\\boldsymbol{x}^{T}=\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\\\\n",
        "\\boldsymbol{X}&=\\begin{bmatrix}\n",
        "1 & 4 \\\\\n",
        "2 & 5 \\\\\n",
        "3 & 6\n",
        "\\end{bmatrix}, \\\n",
        "\\boldsymbol{X}^{T}=\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6\n",
        "\\end{bmatrix}\\end{aligned}\n",
        "$$\n",
        "\n",
        "のようになります．行列に対する転置では，サイズが$(N, M)$から$(M, N)$になり，$i$行$j$列目の値が転置後には$j$行$i$列目の値になります．転置の公式として次を覚えておくとよいでしょう．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}&\\left( 1\\right) \\ \\left( \\boldsymbol{A}^{T}\\right)^{T}=\\boldsymbol{A}\\\\\n",
        "&\\left( 2\\right) \\ \\left( \\boldsymbol{A}\\boldsymbol{B}\\right) ^{T}=\\boldsymbol{B}^{T}\\boldsymbol{A}^{T}\\\\\n",
        "&\\left( 3\\right) \\ \\left( \\boldsymbol{A}\\boldsymbol{B}\\boldsymbol{C}\\right) ^{T}=\\boldsymbol{C}^{T}\\boldsymbol{B}^{T}\\boldsymbol{A}^{T}\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "### ベクトル，行列のサイズ\n",
        "\n",
        "行列積を行った後は行列サイズが変わります．サイズが$(L, M)$の行列と$(M ,N)$の行列の行列積の結果は$(L, N)$となります．例えば先ほどの３つの練習問題のサイズがどのように変化したかをまとめると，\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/10.png)\n",
        "\n",
        "となります．(3)は最初のベクトルと行列の結果が横方向のベクトルであり(1)に帰着することに注意してください．また，ある次元のサイズが1となった場合，その次元を削除しベクトルがスカラーに，行列がベクトルになる場合があります．\n",
        "\n",
        "### 単位行列\n",
        "\n",
        "単位行列 $\\boldsymbol{I}$ は任意の行列$\\boldsymbol{A}$に対し\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{A}\\boldsymbol{I}&=\\boldsymbol{A}\\\\\n",
        "\\boldsymbol{I}\\boldsymbol{A}&=\\boldsymbol{A}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "が成り立ちます．\n",
        "\n",
        "スカラー値の$1$は，$10\\times1 = 10$ といったように，その数を任意の数に乗じても変わらないという性質を持ちます．行列の演算において，これと同様の働きをする行列が**単位行列**であり，\n",
        "\n",
        "$$\n",
        "\\boldsymbol{I}=\\begin{bmatrix}\n",
        "1 & 0 & \\ldots  & 0 \\\\\n",
        "0 & 1 & \\ldots  & 0 \\\\\n",
        "\\vdots & \\vdots  & \\ddots  & \\vdots  \\\\\n",
        "0 & 0 & \\ldots  & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "という形をしています．行列の斜めの要素を**対角要素**と呼び，それ以外の要素を非対角要素と呼びます．単位行列は，対角要素が1で，非対角要素が0であるような行列です．例えば， 2x2行列の場合は，\n",
        "\n",
        "$$\n",
        "\\boldsymbol{I} =\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "であり，3x3行列の場合は，\n",
        "\n",
        "$$\n",
        "\\boldsymbol{I}=\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "となります．\n",
        "\n",
        "実際に計算して，元の行列と値が変わらないかを確認してみると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "&=\\begin{bmatrix}\n",
        "1\\times 1+2\\times 0 & 1\\times 0+2\\times 1 \\\\\n",
        "3\\times 1+4\\times 0 & 3\\times 0+4\\times 1\n",
        "\\end{bmatrix}\\\\\n",
        "&=\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "のように，確かに元の値と一致することがわかりました．\n",
        "\n",
        "### 逆行列\n",
        "\n",
        "**逆行列**とは，元の行列にかけると単位行列になるような行列であり，スカラーにおける逆数($2 \\times 2^{-1} = 1$)に対応するような行列です．数式で定義すると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{A}\\boldsymbol{A}^{-1}=\\boldsymbol{I}\\\\\n",
        "\\boldsymbol{A}^{-1}\\boldsymbol{A}=\\boldsymbol{I}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "となります．ここで，$\\boldsymbol{I}$ は先程の単位行列です．サイズが$2 \\times 2$ や $3 \\times 3$ といった小さな行列の場合には，逆行列計算に公式がありますが，機械学習ではより大きなサイズの行列( $1000 \\times 1000$ など)を扱う必要が出てくるため，逆行列を効率的に求める計算手法が提案されています．\n",
        "\n",
        "逆行列は常に存在するとは限りません．必要条件として逆行列は**正方行列**と呼ばれる行と列のサイズが同じであるような行列である必要があります（他の必要条件については今回は詳説しません）．\n",
        "\n",
        "### 線形結合と二次形式\n",
        "\n",
        "機械学習の式によく出てくる形式として， $\\boldsymbol{b}^{T}\\boldsymbol{x}$ と $\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}$ の2つの形式があります．前者は**線形結合**もしくは**一次結合**，後者は**二次形式**と呼ばれています．スカラーの場合，一次式（$ax+b$）や二次式（$ax^2+bx+c$）がありますが，それをベクトルに拡張したものです．\n",
        "\n",
        "線形結合の計算の中身を見てみると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{b}&=\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{bmatrix},\\ \n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\boldsymbol{b}^{T}\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}=x_{1}+2x_{2}\\end{aligned}\n",
        "$$\n",
        "\n",
        "のように $\\boldsymbol{x}$ の要素である $x_{1}$ もしくは $x_{2}$ に関して，一次式となっていることがわかります．\n",
        "\n",
        "また，二次形式も同様に計算の中身を確認してみると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{A}&=\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix},\\ \n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}\n",
        "&=\\begin{bmatrix} x_{1} & x_{2}\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "&=\\begin{bmatrix}x_{1} & x_{2}\\end{bmatrix} \\begin{bmatrix}\n",
        "x_{1}+2x_{2} \\\\\n",
        "3x_{1}+4x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "&=x_{1}\\left( x_{1}+2x_{2}\\right) +x_{2}\\left( 3x_{1}+4x_{2}\\right) \\\\\n",
        "&=x^{2}_{1}+5x_{1}x_{2}+4x_{2}^{2}\\end{aligned}\n",
        "$$\n",
        "\n",
        "となり，各要素において二次式となっていることがわかります．\n",
        "\n",
        "そして，一般にこれらを足し合わせることでベクトルの二次関数を\n",
        "\n",
        "$$\n",
        "\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{b}^{T}\\boldsymbol{x} + c\n",
        "$$\n",
        "\n",
        "のように表現します．ここで，$c$ はスカラーの定数項です．\n",
        "\n",
        "### ベクトルによる微分と勾配\n",
        "\n",
        "微分は入力を変えた場合の関数値の変化量と説明しました．同様に関数の入力がベクトルである場合，ベクトルによる微分を考えることができます．関数のそれぞれのベクトルの成分毎に偏微分を計算し，それらを並べてベクトルにしたものを**勾配**と呼びます．\n",
        "\n",
        "勾配の計算を紹介する前に，下記の例題を計算しましょう．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{b}&=\\begin{bmatrix}\n",
        "3 \\\\\n",
        "4\n",
        "\\end{bmatrix}, \\ \n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\boldsymbol{b}^{T}\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "3 & 4\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\n",
        "=3x_{1}+4x_{2}\\end{aligned}\n",
        "$$\n",
        "\n",
        "この $\\boldsymbol{b}^{T}\\boldsymbol{x}$ を ベクトル $\\boldsymbol{x}$ で微分したものを，\n",
        "\n",
        "$$\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right)\n",
        "$$\n",
        "\n",
        "と表し．これを**ベクトルで微分**すると言います．今回の例では，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right) &=\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( 3x_{1}+4x_{2}\\right) \\\\\n",
        "&=\\begin{bmatrix}\n",
        "\\dfrac {\\partial }{\\partial x_{1}} \\left( 3x_{1}+4x_{2}\\right)  \\\\\n",
        "\\dfrac {\\partial }{\\partial x_{2}} \\left( 3x_{1}+4x_{2}\\right) \n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "のようになり，計算を進めると\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}+4x_{2}\\right) &=\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}\\right) +\\dfrac {\\partial }{\\partial x_{1}}\\left( 4x_{2}\\right) \\\\\n",
        "&=3\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( x_{1}\\right) +4x_{2}\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( 1\\right) \\\\\n",
        "&=3\\times 1+4x_{2}\\times 0\\\\\n",
        "&=3\\end{aligned}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\\dfrac {\\partial }{\\partial x_{2}}\\left( 3x_{1}+4x_{2}\\right)&=\\dfrac {\\partial }{\\partial x_{2}}\\left( 3x_{1}\\right) +\\dfrac {\\partial }{\\partial x_{2}}\\left( 4x_{2}\\right) \\\\\n",
        "&=3x_{1}\\times \\dfrac {\\partial }{\\partial x_{2}}\\left( 1\\right) +4\\times \\dfrac {\\partial }{ax_{2}}\\left( x_{2}\\right) \\\\\n",
        "&=3x_{1} \\times 0 + 4 \\times 1 \\\\\n",
        "&= 4\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "となり，下記の計算結果が得られます．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right) \n",
        "&=\\begin{bmatrix}\n",
        "\\dfrac {\\partial }{\\partial x_{1}} \\left( 3x_{1}+4x_{2}\\right)  \\\\\n",
        "\\dfrac {\\partial }{\\partial x_{2}} \\left( 3x_{1}+4x_{2}\\right) \n",
        "\\end{bmatrix} =\\begin{bmatrix}\n",
        "3  \\\\\n",
        "4\n",
        "\\end{bmatrix} = \\boldsymbol{b}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "この結果は，よく使われる公式(2)として後ほど取り上げます．\n",
        "\n",
        "もう一問，以下の例題を考えましょう．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{6}\\right) &=\\begin{bmatrix}\n",
        "\\dfrac {\\partial }{\\partial x_{1}}\\left( 6\\right)  \\\\\n",
        "\\dfrac {\\partial }{\\partial x_{2}}\\left( 6\\right) \n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "0 \\\\\n",
        "0\n",
        "\\end{bmatrix}=\\boldsymbol{0}\\end{aligned}\n",
        "$$\n",
        "\n",
        "偏微分を行う対象の変数が含まれていない場合，その偏微分は 0 となります．要素が 0 のみで構成されたベクトルを**零（ゼロ）ベクトル**と言います．\n",
        "\n",
        "これらを踏まえて，公式としてまとめておきましょう．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\left( 1\\right) \\ \\dfrac {\\partial}{\\partial \\boldsymbol{x}}\\left( c\\right) = \\boldsymbol{0}\\\\\n",
        "&\\left( 2\\right) \\ \\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right) = \\boldsymbol{b}\\\\\n",
        "&\\left( 3\\right) \\ \\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}\\right) =\\left( \\boldsymbol{A}+\\boldsymbol{A}^{T}\\right) \\boldsymbol{x}\\end{aligned}\n",
        "$$\n",
        "\n",
        "(1)と(2) はすでに導出済みです．(3) は導出が少し複雑なので省略しますが，数値を代入して確認してみてください．この3つの公式は機械学習を学んでいく上で非常に重要な公式となりますので，必ず覚えておきましょう．\n",
        "\n",
        "こういった行列などにおける公式は他にもたくさんあり，論文などを読む際にはどういった公式があるのかを知っておくことも重要です．例えば，[The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)などを参考にしてみてください．\n",
        "\n",
        "また，今回は多入力単出力関数の微分として勾配まで紹介しましたが，多入力多出力関数の微分であるヤコビ行列（ヤコビアン）もニューラルネットワークの誤差逆伝搬法を理解するために必要となります（ただし殆どの場合，行列を掛けた場合のヤコビ行列は，その転置行列だと覚えるだけで十分です）．さらに詳しく知りたい方は，例えば[The Matrix Calculus You Need For Deep Learning](https://arxiv.org/abs/1802.01528)などを参考にしてみてください．\n",
        "\n",
        "\n",
        "\n",
        "## 統計\n",
        "\n",
        "### 確率や統計は何に使えるのか\n",
        "\n",
        "機械学習といえば確率や統計といったイメージで勉強する人も多いと思いますが，簡単なアルゴリズムであれば，微分と線形代数を理解しておくだけで説明することができ，確率や統計が出てくることはありません．それでは，確率，統計はなぜ必要なのでしょうか？\n",
        "\n",
        "機械学習はデータを扱う手法です．データは個別の事象の集まりですが，学習の目的はそのデータの背後にある普遍性，法則を捉えることです．確率はこうしたデータの分布や不確実性といった概念を数式化することができます．また，統計はある集団に対する様々な特性値を与え，それらを使ってデータを学習しやすいように正規化したり，モデルが妥当なのか，各データが外れ値ではないのかといった判断をすることができます．\n",
        "\n",
        "### 確率\n",
        "\n",
        "確率は様々な可能性がある事象に対し，その事象が起こることが期待される度合いを表します．さらにパラメータ推定の文脈では確率はどれくらい起きそうだという信念を表す場合もあります．確率は$p(X)$のような関数の形で表し，$X$を**確率変数**と呼びます．確率変数は起きうる事象のいずれかの値をとるような変数です．さらに$p(X=u)$を確率変数$X$の値$u$だった時の確率を表すものとします．これを省略した$p(u)$の形で表します．確率は，「全ての事象の確率の和が$1$になる」，「全ての事象の確率は$0$以上である」という2つの制約を満たします．これを式で書くと，\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\sum_{x } p(x) &= 1 \\\\\n",
        "p(x) & \\geq  0\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "となります．\n",
        "\n",
        "（パラメトリックな）確率モデル$p(x; \\theta)$はパラメータ$\\theta$で特徴付けられたような関数です．確率モデル$p(x)$上で事象$u$が観測される確率$p(x=u; \\theta)$を事象$u$の**尤度**と呼びます．尤度の尤は「尤（もっと）もらしい」という意味であり，その事象の起きやすさを表します．\n",
        "\n",
        "2つの事象が同時に起きる確率を**同時確率**と呼び$p(x, y)$のように表します．例えばサイコロを2回振り，1回目の目がx，2回目の目がyとなる確率は同時確率で表すことができます．\n",
        "\n",
        "同時確率のうち，特定の確率変数のみに注目し，それ以外の確率変数について和を取って消去する操作を**周辺化**と呼びます（周辺化という言葉は，行を１つ目の確率変数，列を二つ目の確率変数に対応させて同時確率を表で書いた場合，その行の合計，列の合計を表の周辺に書いたことからそう呼ばれています）．周辺化の結果は，注目した確率変数の確率に一致します．\n",
        "\n",
        "$$\n",
        "p(x) = \\sum_y p(x, y) \\\\\n",
        "p(y) = \\sum_x p(x, y)\n",
        "$$\n",
        "\n",
        "片方の確率変数がある値で固定されている条件下で，もう片方の確率がどうなるのかを表した確率分布を**条件付き確率**と呼び，$p(y|x)$のように表します．例えば$y$を外で雨が降っているかを表す確率変数，$x$を部屋に入ってきた人が傘を持っていたかを表す確率変数とします（$y=1$を雨が振っている，$y=0$を雨が振っていない，のように割り当てます）．この時$p(y|x)$は，部屋に入ってきた人が傘をもっていた場合に外で雨が振っている条件付き確率を表します．\n",
        "\n",
        "条件付き確率は，同時確率を条件の確率で割った値と一致します．\n",
        "\n",
        "$$\n",
        "p(y|x) = \\frac{p(x, y)}{p(x)}\n",
        "$$\n",
        "\n",
        "ここで，条件付確率の式を変形させた$p(y|x)p(x) = p(x, y)$に注意すると，\n",
        "\n",
        "$$\n",
        "p(x|y) = \\frac{p(x, y)}{p(y)} = \\frac{p(y | x)p(x)}{p(y)}\n",
        "$$\n",
        "\n",
        "が得られます．これを**ベイズの定理**と呼びます．重要な定理なので，ぜひ覚えておきましょう．\n",
        "\n",
        "例えば，ベイズの定理の応用事例として，スパムメールフィルターがあります．メールに単語$i$が含むか否かを表す確率変数を $x_{i}$ ，メールがスパムであるか否かを表す確率変数を $y$ とおくと， $p(x_{i}=1)$ は「メールが単語$i$を含む確率」， $p(y=1)$ は「メールがスパムである確率」， $p(x_{i}=1|y=1)$ は「メールがスパムであった場合に，その中に単語$i$が含まれる確率」となります．受信済みの大量のメールからそれぞれの割合を求めて，ベイズの定理を適用することで， $p(y=1|x_{i}=1)$ として，「メールに単語$i$が出現した場合に，そのメールがスパムである確率」を求めることができます．\n",
        "\n",
        "### 最尤推定，事後確率最大化推定，ベイズ推定\n",
        "\n",
        "$N$個のデータ$X = \\{x^{(1)}, x^{(2)}, \\ldots, x^{(N)}\\}$が与えられ，そのデータ$X$を生成するような確率分布を推定する問題を考えます．この場合，**最尤（さいゆう）推定**と呼ばれる手法がよく使われます．最尤推定は観測データ$X$を最も生成しそうなパラメータを推定する手法です．観測するデータがそれぞれ独立に生成されている場合，その尤度は\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\prod_{i=1}^N p(x^{(i)}; \\theta)\n",
        "$$\n",
        "\n",
        "のように表されます．この$\\prod$という記号は$\\sum$の掛け算版で全ての値を掛け合わせるという意味です．複数データに対する尤度は，$1$より小さな値の積となるため非常に小さな数になりコンピュータ上で扱うことが困難になります．また尤度を最大化したい場合，積の形の式の最大化は難しいことが知られています．そこで尤度の代わりにその対数をとった対数尤度($L_{ll}$，llはlog-likelihoodの略）を考えます．\n",
        "\n",
        "$$\n",
        "L_{ll}(\\theta) = \\sum_{i=1}^N \\log p(x^{(i)}; \\theta)\n",
        "$$\n",
        "\n",
        "この対数尤度をパラメータについて最大化することでデータを最も生成しそうな確率モデルのパラメータが得られます．\n",
        "\n",
        "回帰モデルの目的関数として真値と予測値の二乗誤差の和を使う場合（**最小二乗法**と呼ばれています），モデルの出力値に正規分布（後述）の誤差を仮定した最尤推定を行っているのと等価であることが知られています．\n",
        "\n",
        "最尤推定は多くの場合有効ですが，データ数に比べて求めるパラメータ数が多過ぎると，推定がうまくいかない場合があります．また求めるパラメータに何らかの事前情報がある場合にも最尤推定はうまくいきません．\n",
        "\n",
        "たとえば$100$面からなるサイコロを$300$回ころがし，$1$の目が出た回数が$0$回だったとします．このとき，$1$の目が出る確率は最尤推定では$0$と推定してしまいます．しかし明らかに$1$の目が出る確率は$0$より大きいはずという事前情報はあるので，よりよい推定ができそうです．\n",
        "\n",
        "\n",
        "\n",
        "ここでベイズの定理を思い出しましょう．ベイズの定理は\n",
        "\n",
        "$$\n",
        "p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\n",
        "$$\n",
        "\n",
        "という形をしていました．ここで$x$にパラメータ$\\theta$，$y$に観測データ$X$を割り当てると，\n",
        "\n",
        "$$\n",
        "p(\\theta|X) = \\frac{p(X|\\theta)p(\\theta)}{p(X)}\n",
        "$$\n",
        "\n",
        "となります．これは$X$を観測したという条件でパラメータが$\\theta$であった条件付き確率です．この条件付き確率をパラメータについて最大化することを考えると，$P(X)$はパラメータとは関係が無いので無視することができ，\n",
        "\n",
        "$$\n",
        "p(X|\\theta) p(\\theta)\n",
        "$$\n",
        "\n",
        "を最大化するようなパラメータを求めることになります．これを事後確率最大化推定またはMAP（Maximum A Posterior）推定と呼びます．MAP推定では，最尤推定にさらにパラメータの事前確率を掛けた確率を最大化することになります．\n",
        "\n",
        "機械学習においてパラメータを最適化する際，正則化と呼ばれる，パラメータの値が大きいことに対する罰則項を設けたりしますが，これはパラメータの事前確率（の対数）とみなすことができ，パラメータをMAP推定していると解釈できます．\n",
        "\n",
        "### 統計量\n",
        "\n",
        "ここでは，代表的な統計量である平均，分散，標準偏差について紹介していきます．\n",
        "\n",
        "最初は，**平均**を紹介します．たとえば，300円, 400円, 500円の平均は，\n",
        "\n",
        "$$\n",
        "\\dfrac{300 + 400 + 500}{3} = 400\n",
        "$$\n",
        "\n",
        "となり，すべてを足し合わせて対象の数で割ります．これを定式化すると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\\overline {x}=\\dfrac {x_{1}+x_{2}+\\ldots +x_{N}}{N}\n",
        "=\\dfrac {1}{N}\\sum ^{N}_{n=1}x_{n}\\end{aligned}\n",
        "$$\n",
        "\n",
        "のようになります．$N$ は**サンプルの数**を表します．平均は， $\\bar{x}$ や $\\mu$ といった記号で表わされるのが一般的です．データ分布において，平均はその重心に相当する値です．\n",
        "\n",
        "次に，**分散**を紹介します．分散の定義は\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\\sigma ^{2}=\\dfrac {1}{N}\\sum ^{N}_{n=1}\\left( x_{n}-\\overline {x}\\right) ^{2}\\end{aligned}\n",
        "$$\n",
        "\n",
        "となります．各サンプルの平均 $\\bar{x}$ からの差分 $x- \\bar{x}$ を計算し，それらの二乗誤差の平均の値を計算します．分散にはもう一つ定義があり，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\sigma ^{2}=\\dfrac {1}{N-1}\\sum ^{N}_{n=1}\\left( x_{n}-\\overline {x}\\right) ^{2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "と表す場合もありあります．前者は**母分散**といい，後者は**不偏分散**といいます．これらの式の導出は他書に譲るとして，ここではその使い分けについて説明します．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/12.png)\n",
        "\n",
        "データ解析を行う際に，**母集団**に対する解析か**標本集団**に対する解析かを意識することが重要です．母集団とは解析を行いたい想定の範囲に対して，すべてのデータが揃っている場合であり，標本集団はそのうちの一部を抽出する場合です．例えば，全国の小学生の身長と体重を集計する際，全国の小学生を一人の抜け漏れもなく集められれば母集団ですが，各都道府県で100人ずつ抜き出して集計すると，標本集団となります．母集団のデータを集めることは現実的に難しいことが多く，標本集団のデータから母集団の分布を推定することが一般的です．そうなると，基本的には標本集団向けである不偏分散を使用することになります．サンプル数$N$が多い場合には，母分散と不偏分散の違いは殆どありませんが，サンプル数が小さい場合は大きな差となるので注意しましょう．\n",
        "\n",
        "分散を利用すると，データのばらつきを定量評価することができるようになります．例えば，実験を行った際に，結果にばらつきが多ければ，各実験で再現性が確保できていない可能性が考えられます．このように，多数の試行の結果がある値に集まっていることが望ましいような状況において，ばらつきの度合いを定量し評価することが重要となってきます．他に，データのばらつき具合にもよりますが，分散を使えばスケールの違いも評価することができます．\n",
        "\n",
        "最後に**標準偏差**を紹介します．分散では各サンプルの平均からの差の二乗の合計のため，単位は元の単位の二乗となっています．例えば元の単位がkgであれば，分散はkgの二乗という単位になります．そこで，\n",
        "\n",
        "$$\n",
        "\\sigma = \\sqrt{\\sigma^{2}}\n",
        "$$\n",
        "\n",
        "のように分散の平方根をとることで，元の単位と等しくなり，解釈が容易になります．これを標準偏差と呼びます．\n",
        "\n",
        "練習問題で具体的な計算手順の確認を行いましょう．以下の①と②のデータに対して，平均，分散，標準偏差を求めてください．ただし，今回は母分散を使用することとします．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/13.png)\n",
        "\n",
        "①の解答は以下の通りです．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\bar{x}&=\\dfrac {1}{5}\\left( -2-1+0+1+2\\right) =0\\\\\n",
        "\\sigma ^{2}&=\\dfrac {1}{5}\\left\\{ \\left( -2-0\\right) ^{2}+\\left( -1-0\\right) ^{2}+(0-0)^{2}+(1-0)^{2}+(2-0)^{2}\\right\\} \\\\\n",
        "&=\\dfrac {1}{5}\\times 10=2\\\\\n",
        "\\sigma &=\\sqrt {2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "また，②の解答は以下の通りです．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\overline {x}&=\\dfrac {1}{5}\\left( -4-2+0+2+4\\right) =0\\\\\n",
        "\\sigma ^{2}&=\\dfrac {1}{5}\\left\\{ \\left( -4-0\\right) ^{2}+\\left( -2-0\\right) ^{2}+\\left( 0-0\\right) ^{2}+\\left( 2-0\\right) ^{2}+\\left( 4-0\\right) ^{2}\\right\\} \\\\\n",
        "&=\\dfrac {1}{5}\\times 40=8\\\\\n",
        "\\sigma &=\\sqrt {8}=2\\sqrt {2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "これより，②のケースの方が分散が大きく，データのばらつきが大きいことがわかります．\n",
        "\n",
        "### 正規分布と正規化\n",
        "\n",
        "ここでは，確率で度々登場する**正規分布**について紹介します．**ガウス分布**とも呼ばれています．平均$\\mu$，標準偏差$\\sigma$を持つ正規分布は以下のような形をしています．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/14.png)\n",
        "\n",
        "なぜこの正規分布がよく登場するのでしょうか．その理由として，以下のような物理的・数学的背景があります．\n",
        "\n",
        "- 独立で多数の因子の和で表される確率変数は正規分布に従う\n",
        "- 数式が扱いやすい\n",
        "\n",
        "世の中でみられる多くのデータが正規分布に従うことが知られています．一方で必ずしもデータは正規分布に従うとは限りません．正規分布ではないような分布に対し正規分布にあてはめて考えてしまい誤った結論を導く場合も多々あります．データの分布は図示化するなどして正規分布として扱ってよいかは常に考えましょう．\n",
        "\n",
        "正規分布では平均 $\\mu$ と標準偏差 $\\sigma$ に対して，何%がその分布に入っているかといった議論を良く行います．例えば，$\\mu \\pm 3\\sigma$ の範囲内に，データの全体の99.7%が入るため，この $\\mu \\pm 3 \\sigma$ に入らない領域を外れ値として定義するといった使い方ができます．\n",
        "\n",
        "### 標準偏差を利用したスケーリング\n",
        "\n",
        "スケーリングは，大抵の機械学習アルゴリズムにおける前処理として重要です，\n",
        "\n",
        "なぜスケーリングが重要かを説明するために，2点間の距離を計算する例題を取りあげます．スケールが異なる変数 $x_{1}$ と $x_{2}$ があった場合に，下記の図のような状況になります．ここで，縦軸と横軸のスケールが大きく異なっていることに注意してください．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/15.png)\n",
        "\n",
        "\n",
        "\n",
        "この２点間の距離 $d$ を求めると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "d&=\\sqrt {\\left( 100-1000\\right) ^{2}+\\left( 0.1-1\\right) ^{2}}\\\\\n",
        "&= \\sqrt {900^{2}+0.9^{2}}\\\\\n",
        "&= \\sqrt {810000+0.81} \\\\\n",
        "&= \\sqrt {810000.81}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "のようになります．距離 $d$ の中で$x_{1}$の影響量が大きく$x_{2}$ に関しては，スケールが小さいが故にほとんど影響を与えていません．これでは $x_{2}$ がデータの意味として重要な場合においても考慮できません．こうした問題を解決する方法の一つが，ここで紹介する**スケーリング**です．代表的なスケーリングの方法としては２つあります．\n",
        "\n",
        "１つ目が，サンプル集合を**最小値0**，**最大値1**にスケーリングする方法です．これを**Min-Max スケーリング**と呼びます．この方法では，各変数ごとに最小値 $x_{\\min}$ と最大値 $x_{\\max}$ を求めておき，すべてのデータに対して，\n",
        "\n",
        "$$\n",
        "\\widetilde{x} = \\dfrac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
        "$$\n",
        "\n",
        "の計算を行います．Min-Maxスケーリングには計算が単純というメリットがある反面，下図の例ように$x_1$で外れ値を持つデータ点が存在するような場合，$x_{\\max}$ が外れ値に大きく引っ張られてしまうという弱点があります．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/16.png)\n",
        "\n",
        "もう１つのスケーリングの方法として，**平均0**，**標準偏差1** にスケーリングする方法があります．全てのデータから平均を引くと平均$0$になり，標準偏差で割ると標準偏差は$1$になります．\n",
        "\n",
        "$$\n",
        "\\widetilde{x}  = \\dfrac{x - \\bar{x}}{\\sigma}\n",
        "$$\n",
        "\n",
        "．分散を計算した例題の①に対して，このスケーリングを適用してみると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "x_{1}&=\\dfrac {-2-0}{\\sqrt {2}}=-\\dfrac {2}{\\sqrt {2}}\\\\\n",
        "x_{2}&=\\dfrac {-1-0}{\\sqrt {2}}=-\\dfrac {1}{\\sqrt {2}}\\\\\n",
        "x_{3}&=\\dfrac {0-0}{\\sqrt {2}}=0\\\\\n",
        "x_{4}&=\\dfrac {1-0}{\\sqrt {2}}=\\dfrac {1}{\\sqrt {2}}\\\\\n",
        "x_{5}&=\\dfrac {2-0}{\\sqrt {2}}=\\dfrac {2}{\\sqrt {2}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "のように，データが変換されます．この時の平均と標準偏差を求めてみると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\overline {x}&=\\dfrac {1}{5}\\left( -\\dfrac {2}{\\sqrt {2}}-\\dfrac {1}{\\sqrt {2}}+0+\\dfrac {1}{\\sqrt {2}}+\\dfrac {2}{\\sqrt {2}}\\right) =0\\\\\n",
        "\\sigma ^{2}&=\\dfrac {1}{5}\\left\\{ \\left( -\\dfrac {2}{\\sqrt {2}}-0\\right) ^{2}+\\left( -\\dfrac {1}{\\sqrt {2}}-0\\right) ^{2}+\\left( 0-0\\right) ^{2}\n",
        " +\\left( \\dfrac {1}{\\sqrt {2}}-0\\right) ^{2}+\\left( \\dfrac {2}{\\sqrt {2}}-0\\right) ^{2}\\right\\} =1\\\\\n",
        "\\sigma &=\\sqrt {\\sigma ^{2}}=1\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "のように，平均0，標準偏差1にスケーリングできていることがわかります．この方法であれば，Min-Maxスケーリングと比較して，少数の外れ値には強いスケーリングが実現できます．\n",
        "\n",
        "### 外れ値除去\n",
        "\n",
        "以下のように時間によって変動するようなデータを扱うとしましょう．例えば，横軸が時刻，縦軸が温度だとしましょう．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/17.png)\n",
        "\n",
        "このデータに対して，温度計の異常や不具合による温度の異常（外れ値）を検出したい場合，どのようにこの外れ値を定義して検出すれば良いでしょうか．一つの方法として，値の**頻度**に着目する方法があります．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/18.png)\n",
        "\n",
        "上図のように，平均に対して線を引き，それぞれの値において頻度を算出してヒストグラムを描いてみると正規分布が現れます．物理現象として正規分布に従うものが多いと説明しましたが，このように中心付近の値の頻度は多く，離れるほど頻度が少なくなっていく現象に対しては，正規分布をあてはめることができます．そして，外れ値を定義するために，データの平均 $\\mu$ と標準偏差 $\\sigma$ を計算し，$\\mu \\pm 3\\sigma$の値に線を引けば，外れ値除去を行うことができます．これを**3σ法**と呼びます．ただし，外れ値の回数が多かったり，外れ値が極端な値を持つ場合には平均や標準偏差がその外れ値に引っ張られ，3σ法ではうまく対処できないことがあります．\n",
        "\n",
        "その場合には，データを大きい順に並べて上位5%，下位5%を取り除くといった処理をすることもできます．"
      ]
    }
  ]
}